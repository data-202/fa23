[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nHWs\nLabs\nPapers\nOther\n\n\n\n\n\nI\n1\nTues, Aug 22\nIntroduction to the course\nHW 0\nLab 0\n\n\n\n\n\n\n2\nTues, Aug 29\nFoundations\n\n\nPaper 1\n\n\n\n\n\n3\nTues, Sept 5\nTheory construction\n\n\n(Paper 1 edits)\n\n\n\n\n\n4\nTues, Sept 12\nMore on theory construction\n\n\n(Paper 1 edits)\n\n\n\n\nII\n5\nTues, Sept 19\nUnivariate analysis\n\n\n(Paper 1 edits)\n\n\n\n\n\n6\nTues, Sept 26\nExploratory data analysis\nHW 1\nLab 1\n\n\n\n\n\n\n7\nTues, Oct 3\nBivariate analysis\n\n\n\nAnnotated bibliography\n\n\n\n\n8\nTues, Oct 10\nNotes on causal theories\n\n\nPaper 2\n\n\n\n\nIII\n9\nTues, Oct 17\nModeling social in/justice\nHW 2\nLab 2\n\n\n\n\n\n\n10\nTues, Oct 24\nMidterm examination week\n\n\n\nMidterm examination\n\n\n\n\n11\nTues, Oct 31\nBivariate regression analysis\nHW 3\nLab 3\n\n\n\n\n\n\n12\nTues, Nov 7\nMultivariate regression (part a)\n\n\nPaper 3\n\n\n\n\nIV\n13\nTues, Nov 14\nMultivariate regression (part b)\nHW 4\nLab 4\n\n\n\n\n\n\n14\nTues, Nov 21\nOverview of additional models\n\n\n\n\n\n\n\n\n15\nTues, Nov 28\nWrap-up\nHW 5\nLab 5\n\n\n\n\n\nWrap-up\n16\nTues, Dec 5\nOpen office hours\n\n\nPaper 4\n\n\n\n\n\n17\nTues, Dec 12\nNo class meeting\n\n\n\nFinal examination"
  },
  {
    "objectID": "week11.html",
    "href": "week11.html",
    "title": "DATA 202 - Week 11",
    "section": "",
    "text": "Over the next several weeks, our goals will relate to the various ways we can integrate social and theoretical concepts (the context) into decisions around measurement (the content) and analysis (the code).\nImportantly, we will examine various historical examples to make sense of what has been done. Our next example will be based on the work of Ida B. Wells-Barnett (1862-1931), a late 19th century and early 20th century activist and research journalist."
  },
  {
    "objectID": "week11.html#part-i-context",
    "href": "week11.html#part-i-context",
    "title": "DATA 202 - Week 11",
    "section": "Part I: Context",
    "text": "Part I: Context\n\n\n\nA Red Record. Tabulated Statistics and Alleged Causes of Lynchings in the US.\n\n\nThe below summary of The Red Record is from the New York Public Library:\n\nThe investigative journalist and activist Ida B. Wells, later Wells-Barnett, spearheaded the anti-lynching movement in the United States. Expanding on her groundbreaking exposé Southern Horrors: Lynch Law in All Its Phases (1892), A Red Record used mainstream white newspapers to document a resurgence of white mob violence, finding that more than 10,000 African Americans had been killed by lynching in the South between 1864 and 1894. Wells compiled statistics on alleged offenses and the geographic distribution and extent of lynching, and tied whites’ increased brutality and violence to their fear of African Americans’ increased political power. Her conclusion exhorts anti-lynching advocates to “[t]ell the world the facts,” for “When the Christian world knows the alarming growth and extent of outlawry in our land, some means will be found to stop it.”\n\nThe Red Record is an important historical text worth reading. Further investigation of the data and content found in Wells-Barnett’s The Red Record will reveal a relationship be individual incidents of lynching and broader systemic issues of racism.\n\nTo explore these connections between theory, method, and analysis, we will explore the use of a more recent data set at the individual level. The data we will use is the Fatal Force data set from the Washington Post.\nSince 2015, the Washington Post has collected a record of every fatal encounter with police in the United States.\nThis database is one resource that we’ll explore and connect to the Red Record.\nWhat are some theoretical conceptions between Ida B. Wells-Barnett’s Red Record and the data collected by the Washington Post? How might these relationships inform our anlaysis? What is a theoretical construction that can be developed in relation to these connections?\n\n\nFatal force\nThe Washington Post makes the fatal force data publicly accessible on their website and they provide a direct link to their GitHub repository (or repo). To access the data, we will pull it directly into R from Github.\n\nLoad the data\nWe will need to install packages and/or load libraries to import and explore the data.\n\n# install packages\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load libraries\nlibrary(tidyverse) # collection of essential packages for data science\nlibrary(dplyr) # the dplyr package makes data manipulation easier\n\nWe then load the data directly into R from GitHub.\n\nfatal &lt;- read.csv(\"https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v2/fatal-police-shootings-data.csv\")\n\n\n\n\nUnderstand the data\nWe will first turn the file into a tibble using the as_tibble function.\nFrom there, we will use the glimpse(), names(), and str() functions to understand the data.\n\n# examine fatal force data\nfatal &lt;- as_tibble(fatal) # turn data into a tibble\n\nLet us take note of the variables and variable types.\n\nGet a glimpse of the data.\n\n# examine fatal force data\nglimpse(fatal)\n\nRows: 8,811\nColumns: 19\n$ id                         &lt;int&gt; 3, 4, 5, 8, 9, 11, 13, 15, 16, 17, 19, 21, …\n$ date                       &lt;chr&gt; \"2015-01-02\", \"2015-01-02\", \"2015-01-03\", \"…\n$ threat_type                &lt;chr&gt; \"point\", \"point\", \"move\", \"point\", \"point\",…\n$ flee_status                &lt;chr&gt; \"not\", \"not\", \"not\", \"not\", \"not\", \"not\", \"…\n$ armed_with                 &lt;chr&gt; \"gun\", \"gun\", \"unarmed\", \"replica\", \"other\"…\n$ city                       &lt;chr&gt; \"Shelton\", \"Aloha\", \"Wichita\", \"San Francis…\n$ county                     &lt;chr&gt; \"Mason\", \"Washington\", \"Sedgwick\", \"San Fra…\n$ state                      &lt;chr&gt; \"WA\", \"OR\", \"KS\", \"CA\", \"CO\", \"OK\", \"AZ\", \"…\n$ latitude                   &lt;dbl&gt; 47.24683, 45.48742, 37.69477, 37.76291, 40.…\n$ longitude                  &lt;dbl&gt; -123.12159, -122.89170, -97.28055, -122.422…\n$ location_precision         &lt;chr&gt; \"not_available\", \"not_available\", \"not_avai…\n$ name                       &lt;chr&gt; \"Tim Elliot\", \"Lewis Lee Lembke\", \"John Pau…\n$ age                        &lt;int&gt; 53, 47, 23, 32, 39, 18, 22, 35, 34, 47, 25,…\n$ gender                     &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"ma…\n$ race                       &lt;chr&gt; \"A\", \"W\", \"H\", \"W\", \"H\", \"W\", \"H\", \"W\", \"W\"…\n$ race_source                &lt;chr&gt; \"not_available\", \"not_available\", \"not_avai…\n$ was_mental_illness_related &lt;chr&gt; \"True\", \"False\", \"False\", \"True\", \"False\", …\n$ body_camera                &lt;chr&gt; \"False\", \"False\", \"False\", \"False\", \"False\"…\n$ agency_ids                 &lt;chr&gt; \"73\", \"70\", \"238\", \"196\", \"473\", \"101\", \"19…\n\n\n\nView the variable names.\n\n# examine fatal force data\nnames(fatal)\n\n [1] \"id\"                         \"date\"                      \n [3] \"threat_type\"                \"flee_status\"               \n [5] \"armed_with\"                 \"city\"                      \n [7] \"county\"                     \"state\"                     \n [9] \"latitude\"                   \"longitude\"                 \n[11] \"location_precision\"         \"name\"                      \n[13] \"age\"                        \"gender\"                    \n[15] \"race\"                       \"race_source\"               \n[17] \"was_mental_illness_related\" \"body_camera\"               \n[19] \"agency_ids\"                \n\n\n\nUnderstand the structure of the tibble.\n\n# examine fatal force data\nstr(fatal)\n\ntibble [8,811 × 19] (S3: tbl_df/tbl/data.frame)\n $ id                        : int [1:8811] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : chr [1:8811] \"2015-01-02\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n $ threat_type               : chr [1:8811] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:8811] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:8811] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:8811] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:8811] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ state                     : chr [1:8811] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ latitude                  : num [1:8811] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:8811] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:8811] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : int [1:8811] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:8811] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:8811] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: chr [1:8811] \"True\" \"False\" \"False\" \"True\" ...\n $ body_camera               : chr [1:8811] \"False\" \"False\" \"False\" \"False\" ...\n $ agency_ids                : chr [1:8811] \"73\" \"70\" \"238\" \"196\" ...\n\n\n\nWe should also view the head() and tail() of the data.\n\nhead(fatal)\n\n# A tibble: 6 × 19\n     id date      threat_type flee_status armed_with city  county state latitude\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1     3 2015-01-… point       not         gun        Shel… Mason  WA        47.2\n2     4 2015-01-… point       not         gun        Aloha Washi… OR        45.5\n3     5 2015-01-… move        not         unarmed    Wich… Sedgw… KS        37.7\n4     8 2015-01-… point       not         replica    San … San F… CA        37.8\n5     9 2015-01-… point       not         other      Evans Weld   CO        40.4\n6    11 2015-01-… attack      not         gun        Guth… Logan  OK        35.9\n# ℹ 10 more variables: longitude &lt;dbl&gt;, location_precision &lt;chr&gt;, name &lt;chr&gt;,\n#   age &lt;int&gt;, gender &lt;chr&gt;, race &lt;chr&gt;, race_source &lt;chr&gt;,\n#   was_mental_illness_related &lt;chr&gt;, body_camera &lt;chr&gt;, agency_ids &lt;chr&gt;\n\ntail(fatal)\n\n# A tibble: 6 × 19\n     id date      threat_type flee_status armed_with city  county state latitude\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1  9549 2023-10-… threat      \"car\"       unknown    \"\"    Lanca… NE        NA  \n2  9550 2023-10-… move        \"\"          knife      \"Hen… Clark  NV        36.0\n3  9551 2023-10-… shoot       \"\"          gun        \"San… San B… CA        34.1\n4  9552 2023-10-… threat      \"\"          gun        \"Tok\" Fairb… AK        NA  \n5  9554 2023-11-… attack      \"\"          knife      \"Swa… Grant  IN        40.5\n6  9553 2023-11-… move        \"foot\"      blunt_obj… \"Cal… Los A… CA        34.2\n# ℹ 10 more variables: longitude &lt;dbl&gt;, location_precision &lt;chr&gt;, name &lt;chr&gt;,\n#   age &lt;int&gt;, gender &lt;chr&gt;, race &lt;chr&gt;, race_source &lt;chr&gt;,\n#   was_mental_illness_related &lt;chr&gt;, body_camera &lt;chr&gt;, agency_ids &lt;chr&gt;\n\n\nWhat do you notice? What do you wonder?\n\nThere are many ways to use R to understand and explore our data.\nWe will use a mixture of base R and tidyverse commands to clean up our data.\nWe will also use the R for Data Science, 2nd Edition text as a guide.\n\n\n\nR for Data Science, 2nd Edition\n\n\n\n\n\nCodebook\nPrior to beginning any cleaning and analysis, we need to know our data…\n…before loading our data, we should have become more familiar with the codebook.\nThe code book will allow us to understand how data was collected and input into the data set. This often includes the levels of measurement for each variable and other related details.\nThe codebook for the Fatal Force data can be found online. We will need to navigate the GitHub site.\n\nMain landing page: https://github.com/washingtonpost/data-police-shootings\nFatal Force Database (version 2): https://github.com/washingtonpost/data-police-shootings/tree/master/v2\n\nA codebook for the data we have uploaded can be found at the bottom of the database page.\n\nLet’s first remind ourselves of the variables and variable types.\n\nstr(fatal)\n\ntibble [8,811 × 19] (S3: tbl_df/tbl/data.frame)\n $ id                        : int [1:8811] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : chr [1:8811] \"2015-01-02\" \"2015-01-02\" \"2015-01-03\" \"2015-01-04\" ...\n $ threat_type               : chr [1:8811] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:8811] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:8811] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:8811] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:8811] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ state                     : chr [1:8811] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ latitude                  : num [1:8811] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:8811] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:8811] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : int [1:8811] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:8811] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:8811] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: chr [1:8811] \"True\" \"False\" \"False\" \"True\" ...\n $ body_camera               : chr [1:8811] \"False\" \"False\" \"False\" \"False\" ...\n $ agency_ids                : chr [1:8811] \"73\" \"70\" \"238\" \"196\" ...\n\n\nWhat do you notice? What do you wonder?\nThe use of # in line with your code is one valuable way to leave notes for yourself and any readers of your code. This also helps to support if others would like to reproduce your analysis.\nHere, you may want to take note of some of the issues you see that could cause potential issues later as you begin your analysis. For example, a few things noteworthy observations will inform some initial changes.\n\n\n\nClean up the data\nLet’s fix a few variables based on our previous observations and the codebook.\n\nThe date variable is listed as a character variable, however, we want to transform this into a more appropriate variable for the variable type. We can use the command as.date() to do so.\nThe age variable is listed as an integer, which is correct. However, we but want to change the variable type to numeric to conduct some specific analyses in the tidyverse.\nThe latitude and longitude variables are listed as numeric variables. However, they contain some different information where values such as the mean or other measures of center do not make sense.\nThe was_mental_illness_related variable is also listed as a character variable when, in fact, it should be a logical variable based on both the codebook and contents of the cells.\nThe body_camera variable is also listed as a character variable when it should be a logical variable given the contents of the cells.\n\n\n\n# fix vars\n\n  # change vars to appropriate formats\n  fatal$date &lt;- as.Date(fatal$date) # check/change to date format\n  fatal$age &lt;- as.numeric(fatal$age)\n  fatal$was_mental_illness_related &lt;- as.logical(fatal$was_mental_illness_related)\n  fatal$body_camera &lt;- as.logical(fatal$body_camera)\n  \n  # format to 20YY\n  fatal.year &lt;- format(fatal$date, format=\"20%y\") \n  fatal$year &lt;- fatal.year # add a year column to the df\n  fatal %&gt;% relocate(state, year) -&gt; fatal\n  fatal\n\n# A tibble: 8,811 × 20\n   state year     id date       threat_type flee_status armed_with city   county\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; \n 1 WA    2015      3 2015-01-02 point       not         gun        Shelt… Mason \n 2 OR    2015      4 2015-01-02 point       not         gun        Aloha  Washi…\n 3 KS    2015      5 2015-01-03 move        not         unarmed    Wichi… Sedgw…\n 4 CA    2015      8 2015-01-04 point       not         replica    San F… San F…\n 5 CO    2015      9 2015-01-04 point       not         other      Evans  Weld  \n 6 OK    2015     11 2015-01-04 attack      not         gun        Guthr… Logan \n 7 AZ    2015     13 2015-01-05 shoot       car         gun        Chand… Maric…\n 8 KS    2015     15 2015-01-06 point       not         gun        Assar… Saline\n 9 IA    2015     16 2015-01-06 accident    not         unarmed    Burli… Des M…\n10 PA    2015     17 2015-01-06 point       not         replica    Knoxv… Alleg…\n# ℹ 8,801 more rows\n# ℹ 11 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;,\n#   location_precision &lt;chr&gt;, name &lt;chr&gt;, age &lt;dbl&gt;, gender &lt;chr&gt;, race &lt;chr&gt;,\n#   race_source &lt;chr&gt;, was_mental_illness_related &lt;lgl&gt;, body_camera &lt;lgl&gt;,\n#   agency_ids &lt;chr&gt;\n\n\n\n\nAncillary explorations\nSometimes when analyzing data, you may want to gather quick information.\nFor example, let’s say I also want to get a count of fatal shootings by year.\n\n# get counts by year\nfatal %&gt;% count(year)\n\n# A tibble: 9 × 2\n  year      n\n  &lt;chr&gt; &lt;int&gt;\n1 2015    995\n2 2016    958\n3 2017    983\n4 2018    992\n5 2019    997\n6 2020   1019\n7 2021   1048\n8 2022   1096\n9 2023    723\n\n\nI was able to do this when I added the year variable to the dataframe.\nWhat do you notice? What do you wonder?\n\n\n\n\nTransform the data\nWe should check the structure of our data again.\n\nstr(fatal)\n\ntibble [8,811 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : chr [1:8811] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ year                      : chr [1:8811] \"2015\" \"2015\" \"2015\" \"2015\" ...\n $ id                        : int [1:8811] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:8811], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : chr [1:8811] \"point\" \"point\" \"move\" \"point\" ...\n $ flee_status               : chr [1:8811] \"not\" \"not\" \"not\" \"not\" ...\n $ armed_with                : chr [1:8811] \"gun\" \"gun\" \"unarmed\" \"replica\" ...\n $ city                      : chr [1:8811] \"Shelton\" \"Aloha\" \"Wichita\" \"San Francisco\" ...\n $ county                    : chr [1:8811] \"Mason\" \"Washington\" \"Sedgwick\" \"San Francisco\" ...\n $ latitude                  : num [1:8811] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:8811] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:8811] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : num [1:8811] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : chr [1:8811] \"male\" \"male\" \"male\" \"male\" ...\n $ race                      : chr [1:8811] \"A\" \"W\" \"H\" \"W\" ...\n $ race_source               : chr [1:8811] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: logi [1:8811] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:8811] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : chr [1:8811] \"73\" \"70\" \"238\" \"196\" ...\n\n\nWhile our initial changes are noted, observe the chr (character) variable type.\n\nWe may want to change these into fct (factor) variables types.\n\nfatal_factor &lt;- fatal %&gt;% \n  mutate_if(is.character, as.factor)\nstr(fatal_factor)\n\ntibble [8,811 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 48 38 17 5 6 37 4 17 13 39 ...\n $ year                      : Factor w/ 9 levels \"2015\",\"2016\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ id                        : int [1:8811] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:8811], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : Factor w/ 9 levels \"\",\"accident\",..: 6 6 5 6 6 3 7 6 2 6 ...\n $ flee_status               : Factor w/ 5 levels \"\",\"car\",\"foot\",..: 4 4 4 4 4 4 2 4 4 4 ...\n $ armed_with                : Factor w/ 22 levels \"\",\"blunt_object\",..: 5 5 17 14 11 5 5 5 17 14 ...\n $ city                      : Factor w/ 3384 levels \"\",\"Abbeville\",..: 2779 48 3302 2683 957 1233 507 125 393 1551 ...\n $ county                    : Factor w/ 846 levels \"\",\"Acadia\",\"Ada\",..: 473 807 694 674 815 444 466 669 210 16 ...\n $ latitude                  : num [1:8811] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:8811] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : Factor w/ 8 levels \"\",\"address\",\"block\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ name                      : Factor w/ 8263 levels \"\",\"A.B. Carr\",..: 7669 5106 4061 5523 5811 4757 4735 948 633 5094 ...\n $ age                       : num [1:8811] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : Factor w/ 4 levels \"\",\"female\",\"male\",..: 3 3 3 3 3 3 3 3 2 3 ...\n $ race                      : Factor w/ 8 levels \"\",\"A\",\"B\",\"B;H\",..: 2 8 5 8 5 8 5 8 8 3 ...\n $ race_source               : Factor w/ 7 levels \"\",\"clip\",\"not_available\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ was_mental_illness_related: logi [1:8811] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:8811] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : Factor w/ 3429 levels \"\",\"1\",\"100\",\"100;3031\",..: 3166 3134 1530 1054 2884 16 1045 2907 2067 1796 ...\n\n\nTake note of the changes. It seems that this method was not the best solution.\nWhich variables should be changed or reverted?\nThe answers can be found in the Fatal Force codebook.\n\n\nDrop missing values\nFor efficiency, we will make changes to variables of interest and drop all missing values.\n\nfatal %&gt;% \n  mutate_at(c('threat_type', 'flee_status', 'armed_with', 'city', 'county', 'gender', 'race'), as.factor) %&gt;%\n  drop_na() -&gt; fatal_clean # generate a new `clean` dataframe\nstr(fatal_clean)\n\ntibble [7,360 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                     : chr [1:7360] \"WA\" \"OR\" \"KS\" \"CA\" ...\n $ year                      : chr [1:7360] \"2015\" \"2015\" \"2015\" \"2015\" ...\n $ id                        : int [1:7360] 3 4 5 8 9 11 13 15 16 17 ...\n $ date                      : Date[1:7360], format: \"2015-01-02\" \"2015-01-02\" ...\n $ threat_type               : Factor w/ 9 levels \"\",\"accident\",..: 6 6 5 6 6 3 7 6 2 6 ...\n $ flee_status               : Factor w/ 5 levels \"\",\"car\",\"foot\",..: 4 4 4 4 4 4 2 4 4 4 ...\n $ armed_with                : Factor w/ 22 levels \"\",\"blunt_object\",..: 5 5 17 14 11 5 5 5 17 14 ...\n $ city                      : Factor w/ 3384 levels \"\",\"Abbeville\",..: 2779 48 3302 2683 957 1233 507 125 393 1551 ...\n $ county                    : Factor w/ 846 levels \"\",\"Acadia\",\"Ada\",..: 473 807 694 674 815 444 466 669 210 16 ...\n $ latitude                  : num [1:7360] 47.2 45.5 37.7 37.8 40.4 ...\n $ longitude                 : num [1:7360] -123.1 -122.9 -97.3 -122.4 -104.7 ...\n $ location_precision        : chr [1:7360] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ name                      : chr [1:7360] \"Tim Elliot\" \"Lewis Lee Lembke\" \"John Paul Quintero\" \"Matthew Hoffman\" ...\n $ age                       : num [1:7360] 53 47 23 32 39 18 22 35 34 47 ...\n $ gender                    : Factor w/ 4 levels \"\",\"female\",\"male\",..: 3 3 3 3 3 3 3 3 2 3 ...\n $ race                      : Factor w/ 8 levels \"\",\"A\",\"B\",\"B;H\",..: 2 8 5 8 5 8 5 8 8 3 ...\n $ race_source               : chr [1:7360] \"not_available\" \"not_available\" \"not_available\" \"not_available\" ...\n $ was_mental_illness_related: logi [1:7360] TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ body_camera               : logi [1:7360] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ agency_ids                : chr [1:7360] \"73\" \"70\" \"238\" \"196\" ...\n\n\nHowever, with more time, we should inspect the data further and change all variables.\n\n\n\n\nCheck the data\nOne more check to make sure that we have dropped any missing observations.\nI will use the sapply() function from Lab 1 to check for missing values.\nFirst, we’ll check our original data.\n\nsapply(fatal, function(x) sum(is.na(x)))\n\n                     state                       year \n                         0                          0 \n                        id                       date \n                         0                          0 \n               threat_type                flee_status \n                         0                          0 \n                armed_with                       city \n                         0                          0 \n                    county                   latitude \n                         0                        999 \n                 longitude         location_precision \n                       999                          0 \n                      name                        age \n                         0                        561 \n                    gender                       race \n                         0                          0 \n               race_source was_mental_illness_related \n                         0                          0 \n               body_camera                 agency_ids \n                         0                          0 \n\n\n\nThen we’ll examine our cleaned data.\n\nsapply(fatal_clean, function(x) sum(is.na(x)))\n\n                     state                       year \n                         0                          0 \n                        id                       date \n                         0                          0 \n               threat_type                flee_status \n                         0                          0 \n                armed_with                       city \n                         0                          0 \n                    county                   latitude \n                         0                          0 \n                 longitude         location_precision \n                         0                          0 \n                      name                        age \n                         0                          0 \n                    gender                       race \n                         0                          0 \n               race_source was_mental_illness_related \n                         0                          0 \n               body_camera                 agency_ids \n                         0                          0 \n\n\n\n\n\nExplore the data\nWe will begin to explore with a summary of our cleaned data.\n\nsummary(fatal_clean)\n\n    state               year                 id            date           \n Length:7360        Length:7360        Min.   :   3   Min.   :2015-01-02  \n Class :character   Class :character   1st Qu.:2126   1st Qu.:2016-12-06  \n Mode  :character   Mode  :character   Median :4366   Median :2019-01-03  \n                                       Mean   :4481   Mean   :2019-02-03  \n                                       3rd Qu.:6621   3rd Qu.:2021-02-06  \n                                       Max.   :9565   Max.   :2023-11-01  \n                                                                          \n       threat_type   flee_status         armed_with            city     \n shoot       :2037        : 869   gun         :4281   Los Angeles: 112  \n threat      :2001   car  :1143   knife       :1258   Phoenix    :  95  \n point       :1433   foot : 962   unarmed     : 463   Houston    :  80  \n attack      :1097   not  :4119   replica     : 261   Las Vegas  :  65  \n move        : 362   other: 267   undetermined: 234   San Antonio:  64  \n undetermined: 219                vehicle     : 229   Chicago    :  48  \n (Other)     : 211                (Other)     : 634   (Other)    :6896  \n         county        latitude       longitude          location_precision\n            :3694   Min.   :19.50   Min.   :-9.007e+15   Length:7360       \n Los Angeles: 152   1st Qu.:33.49   1st Qu.:-1.120e+02   Class :character  \n Maricopa   : 102   Median :36.15   Median :-9.400e+01   Mode  :character  \n Washington :  52   Mean   :36.71   Mean   :-1.224e+12                     \n Orange     :  51   3rd Qu.:40.05   3rd Qu.:-8.300e+01                     \n Jefferson  :  50   Max.   :71.30   Max.   :-6.800e+01                     \n (Other)    :3259                                                          \n     name                age               gender          race     \n Length:7360        Min.   : 2.00             :   9   W      :3358  \n Class :character   1st Qu.:27.00   female    : 334   B      :1780  \n Mode  :character   Median :35.00   male      :7016   H      :1188  \n                    Mean   :37.27   non-binary:   1          : 785  \n                    3rd Qu.:45.00                     A      : 129  \n                    Max.   :91.00                     N      :  98  \n                                                      (Other):  22  \n race_source        was_mental_illness_related body_camera    \n Length:7360        Mode :logical              Mode :logical  \n Class :character   FALSE:5776                 FALSE:6193     \n Mode  :character   TRUE :1584                 TRUE :1167     \n                                                              \n                                                              \n                                                              \n                                                              \n  agency_ids       \n Length:7360       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\n\nAncillary explorations.\nWith the clean data, I may be interested in getting counts by state and then year as we explore.\n\n# get counts by state then year\nfatal_clean %&gt;% \n  count(state, year)\n\n# A tibble: 446 × 3\n   state year      n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 AK    2015      4\n 2 AK    2016      7\n 3 AK    2017      6\n 4 AK    2018      5\n 5 AK    2019      6\n 6 AK    2020      6\n 7 AK    2021      4\n 8 AK    2022      5\n 9 AL    2015     17\n10 AL    2016     21\n# ℹ 436 more rows\n\n\nWhat might this output provide for us?\nWhat code chunk should we add in order to see the additional rows?\n\n\n\n\n\nTheory\nBased on our explorations up to this point, what relationship might you examine?\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nResearch questions\nWith our conceptual framing and backing theoretical work, we can proceed to examine exploratory analyses on the data. During the exploratory phase of analysis, a nice opportunity is presented that will allow you to examine the data is to generate a series of discrete research questions that help you make associations between various components of your data."
  },
  {
    "objectID": "week11.html#part-ii-content",
    "href": "week11.html#part-ii-content",
    "title": "DATA 202 - Week 11",
    "section": "Part II: Content",
    "text": "Part II: Content\nRegression analysis is a standard analysis in many statistical studies.\nThere are many different types of regression. We’ll continue with our exploration of bivariate regression analysis and focus on some of the base assumptions as it relates to study development.\nWe will begin by looking at the underlying assumptions of regression analysis. These assumptions are the technical (or structural) components of our analyses, and should be checked at the initiation of a research study, starting with data collection or understanding how data was collected if it is a secondary analysis.\n\n\nBivariate regression analysis\nLet us take two variables, \\(x\\) and \\(y\\) and assume there is an association.\n\n# Fit simple linear regression model\nmodel &lt;- lm(y ~ x)\n\n\n\n# Examine regression outputs\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9073 -0.6835 -0.0875  0.5806  3.2904 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.89720    0.09755   19.45   &lt;2e-16 ***\nx            2.94753    0.10688   27.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9707 on 98 degrees of freedom\nMultiple R-squared:  0.8859,    Adjusted R-squared:  0.8847 \nF-statistic: 760.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n# Check regression coefficients\ncoeff &lt;- coef(model)\ncoeff\n\n(Intercept)           x \n   1.897197    2.947528 \n\n\n\n\n# Check R-squared value\nrsq &lt;- summary(model)$r.squared\nrsq\n\n[1] 0.8858556\n\n\n\n\n# Generate predictions\npred &lt;- predict(model) # call this object to show the predicted values of the model\n\n\n\n# Plot the regression line\nplot(x, y)\nabline(model, col=\"blue\")\n\n\n\n\n\n\n# Check residuals\nresids &lt;- residuals(model)\nplot(x, resids)\n\n\n\n\n\n\n# Diagnostic plots\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\n\n# Check significance of predictor\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 716.67  716.67  760.56 &lt; 2.2e-16 ***\nResiduals 98  92.34    0.94                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week11.html#part-iii-code",
    "href": "week11.html#part-iii-code",
    "title": "DATA 202 - Week 11",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis is a reminder to deepen your understanding of the General Social Survey (GSS) using the notes provided below from week 9.\n\nGeneral Social Survey\nThis week we’ll return to our examination of the General Social Survey (GSS) data.\nAs a first task, please identify up to two or three variables that you can utilize to follow along with your analysis.\n\n\nLoad the data\nWe will begin by loading a host of libraries so that we can perform more complex analyses.\nThere will also be a need to install some additional packages that we have not used before.\nWe will begin by installing a few new packages.\n\n## install new packages\ninstall.packages(\"srvyr\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"statsr\", repos = \"http://cran.us.r-project.org\")\n\n\nThen we will load the libraries.\n\n## load libraries\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(descr)\nlibrary(foreign)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(statsr)\n\n\nTo access the GSS data, we will need to use a remote install command.\nThe data come from the GSSR package.\n\n# install GSSR data\nremotes::install_github(\"kjhealy/gssr\")\nlibrary(gssr)\n\n\nWe will now load the GSS documentation.\n\ndata(gss_doc)\n\n\nView the GSS documentation.\n\ndata(gss_doc)\n\n\n\nNext up: Week 12"
  },
  {
    "objectID": "week11-slides.html#part-i-context",
    "href": "week11-slides.html#part-i-context",
    "title": "DATA 202 - Week 11",
    "section": "Part I: Context",
    "text": "Part I: Context\n\nA Red Record. Tabulated Statistics and Alleged Causes of Lynchings in the US.The below summary of The Red Record is from the New York Public Library:\n\nThe investigative journalist and activist Ida B. Wells, later Wells-Barnett, spearheaded the anti-lynching movement in the United States. Expanding on her groundbreaking exposé Southern Horrors: Lynch Law in All Its Phases (1892), A Red Record used mainstream white newspapers to document a resurgence of white mob violence, finding that more than 10,000 African Americans had been killed by lynching in the South between 1864 and 1894. Wells compiled statistics on alleged offenses and the geographic distribution and extent of lynching, and tied whites’ increased brutality and violence to their fear of African Americans’ increased political power. Her conclusion exhorts anti-lynching advocates to “[t]ell the world the facts,” for “When the Christian world knows the alarming growth and extent of outlawry in our land, some means will be found to stop it.”\n\nThe Red Record is an important historical text worth reading. Further investigation of the data and content found in Wells-Barnett’s The Red Record will reveal a relationship be individual incidents of lynching and broader systemic issues of racism."
  },
  {
    "objectID": "week11-slides.html#part-ii-content",
    "href": "week11-slides.html#part-ii-content",
    "title": "DATA 202 - Week 11",
    "section": "Part II: Content",
    "text": "Part II: Content\nRegression analysis is a standard analysis in many statistical studies.\nThere are many different types of regression. We’ll continue with our exploration of bivariate regression analysis and focus on some of the base assumptions as it relates to study development.\nWe will begin by looking at the underlying assumptions of regression analysis. These assumptions are the technical (or structural) components of our analyses, and should be checked at the initiation of a research study, starting with data collection or understanding how data was collected if it is a secondary analysis."
  },
  {
    "objectID": "week11-slides.html#part-iii-code",
    "href": "week11-slides.html#part-iii-code",
    "title": "DATA 202 - Week 11",
    "section": "Part III: Code",
    "text": "Part III: Code\nThis is a reminder to deepen your understanding of the General Social Survey (GSS) using the notes provided below from week 9.\nGeneral Social Survey\nThis week we’ll return to our examination of the General Social Survey (GSS) data.\nAs a first task, please identify up to two or three variables that you can utilize to follow along with your analysis."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "The methods used to collect sample data for statistical analysis is extremely important.\nIf sample data are not collected appropriately, resulting statistical analyses will be futile.\nAs a result, planning a study by identifying research questions, the population and sample of interest, and selecting the appropriate research method(s) that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\n\nThere are many different types of research studies.\nSome studies use non-traditional methods (such as oral traditions) to collect data, while others focus on more traditional methods (such as surveys) to analyze data on a sample or a population.\nThese data collection methods produce a set of observations upon which statistical analyses can be applied. We consider two core study designs in statistical data analysis: experimental studies and observational studies.\n\n\nExperimental study: In an experimental study, a treatment is applied to a sample of interest to observe its effects. There is generally a control group and a treatment group used to understand the effects of the treatment. Individual observations are referred to as experimental units whereas studies involving humans are generally defined as study subjects.\nObservational study: In an observational study, specific characteristics of a sample or population are observed and measured but individual observations or subjects of study are not influenced or modified in any way.\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Types of studies\n\n\n\nRetrospective study: In a retrospective study, we go back in time to collect data over some past period.\nCross-sectional study: In a cross-sectional study, data are collected and measured at one point in time.\nProspective study: In a prospective study, we set up a study to go forward in time and observe groups sharing common factors.\n\n\n\n\n\n\nThere are two broad categories of selecting members of a population to generate sample data:\n\nProbability sampling\nNon-probability sampling\n\nWithin these two broad categories are other methods based on the needs of the study. Each methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\n\n\n\n\n\nDefinitions: Sampling methods\n\n\n\nProbability sampling: Involves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\nNon-probability sampling: Involves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu"
  },
  {
    "objectID": "week5.html#part-i-context",
    "href": "week5.html#part-i-context",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "The methods used to collect sample data for statistical analysis is extremely important.\nIf sample data are not collected appropriately, resulting statistical analyses will be futile.\nAs a result, planning a study by identifying research questions, the population and sample of interest, and selecting the appropriate research method(s) that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\n\nThere are many different types of research studies.\nSome studies use non-traditional methods (such as oral traditions) to collect data, while others focus on more traditional methods (such as surveys) to analyze data on a sample or a population.\nThese data collection methods produce a set of observations upon which statistical analyses can be applied. We consider two core study designs in statistical data analysis: experimental studies and observational studies.\n\n\nExperimental study: In an experimental study, a treatment is applied to a sample of interest to observe its effects. There is generally a control group and a treatment group used to understand the effects of the treatment. Individual observations are referred to as experimental units whereas studies involving humans are generally defined as study subjects.\nObservational study: In an observational study, specific characteristics of a sample or population are observed and measured but individual observations or subjects of study are not influenced or modified in any way.\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Types of studies\n\n\n\nRetrospective study: In a retrospective study, we go back in time to collect data over some past period.\nCross-sectional study: In a cross-sectional study, data are collected and measured at one point in time.\nProspective study: In a prospective study, we set up a study to go forward in time and observe groups sharing common factors.\n\n\n\n\n\n\nThere are two broad categories of selecting members of a population to generate sample data:\n\nProbability sampling\nNon-probability sampling\n\nWithin these two broad categories are other methods based on the needs of the study. Each methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\n\n\n\n\n\nDefinitions: Sampling methods\n\n\n\nProbability sampling: Involves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\nNon-probability sampling: Involves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\n\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu"
  },
  {
    "objectID": "week5.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "href": "week5.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Example 5.4: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.5: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.6: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.7: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.8: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.9: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\n\nExample 5.10: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu\n\n\n\n\nExample 5.11: The Black Census\n\n\n\nImage from The Black Futures Lab. Blackcensus.org\n\n\n\n\n\nExample 5.12: The Black Census reports\n\n\n\nImage from The Black Futures Lab. Blackcensus.org"
  },
  {
    "objectID": "week5.html#part-ii-content",
    "href": "week5.html#part-ii-content",
    "title": "DATA 202 - Week 5",
    "section": "Part II: Content",
    "text": "Part II: Content\n\n\nThe Big Picture\nPrior to jumping into more details about univariate data examples in the next section, it will be important to gather a “big picture” view of the field. This big picture view will provide us with a high-level description of the various topics we will cover in the course.\n\n\nPopulation parameter\nA number which summarizes the entire group.\n\n\nSample statistic\nA single number that summarizes a subset of data, or the sample.\n\n\n\nPopulation vs. Sample. Image from Scribbr.\n\n\n\n\n\nConfidence interval (CI)\n\nRange of likely or plausible values for a population parameter.\nBased on a sample and statistics from that sample.\nMargin of error represents the number of standard deviations on a statistic.\n\n\n\n\nPlausible values for a variable via the CI. Image from Psychologicalscience.org\n\n\n\n\n\nHypothesis test\nStatistical procedure used to test an existing claim about a population.\n\nTest is based on data; most ideally data collected via probability sampling.\n\\(H_0\\) - Null hypothesis: If data supports the claim, fail to reject \\(H_0\\)\n\\(H_a\\) - Alternative hypothesis: If data does not support claim, reject \\(H_0\\)\n\n\n\n\nHypothesis testing. Image from Towards Data Science.\n\n\n\n\n\nAnalysis of variance (ANOVA)\nComparing means of more than two populations.\nF-statistic is a ratio that is used to compare variability between sets.\n\n\n\nWithin and between group variation. Image from QCBS R Workshop Series.\n\n\n\nMultiple comparison procedures\nSet of statistical tests that compare means to each other.\n\nExamples include Turkey’s test, Least significant difference (LSD), pairwise t-test\nThese tests are only conducted if you analysis of variance identifies differences\n\n\n\n\n\nInteraction effects\nInteraction effects are relevant to statistical models that use two or more variables.\n\n\nCorrelation\nMeasures the strength and direction of a linear relationship between two variables.\n\n\nLinear regression\nHelps make predictions for one variable based on the values of another.\n\nThere are many types of regression:\n\nSimple linear regression\nMultiple linear regression\nLogistic regression\nNon-linear regression"
  },
  {
    "objectID": "week5.html#correlation-and-linear-regression.-image-from-lennys-newsletter.",
    "href": "week5.html#correlation-and-linear-regression.-image-from-lennys-newsletter.",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Chi-square test\nWhen using correlation and regression analyses, one core assumption is that the variables are quantitative in nature.\nWe use a chi-square test to study categorical variables.\n\n\n\nExample research question for a chi-square test. Image from Datatab.net.\n\n\nIn the example above, the null hypothesis is that there is no relationship between gender and highest level of education; the alternative hypothesis is that there is a relationship between gender and highest level of education."
  },
  {
    "objectID": "week5.html#part-iii-code",
    "href": "week5.html#part-iii-code",
    "title": "DATA 202 - Week 5",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will help prepare you to complete Lab 1.\nLab 1 focuses on univariate statistics.\nUnivariate statistics refer to analyses that describe a single variable or attribute.\n\nOpen RStudio.\n\n\n\nOpen RStudio.\n\n\n\n\nTask 0: Start a new project\nFirst, you will need to navigate to: File &gt; New Project\nSelect the first option: New Directory.\n\n\n\nSelect New Directory.\n\n\n\nThen you will want to select the New Project option.\n\n\n\nSelect New Project\n\n\n\nSelect the directory where you wish to store your project.\n\n\n\nSelect New Directory.\n\n\n\nThen, name your project to an appropriate title. Consider stats-pt21.\n\n\n\nName your project.\n\n\n\nBe sure to place the project in your preferred directory.\nOne good option is to make a sub-folder in your Documents.\n\n\n\nPlace your project in an appropriate directory.\n\n\n\nClick the box at the bottom right of the pop-up window.\nWe generally want to start a new project in a new RStudio session.\n\n\n\nCheck the box in the bottom right of the popup box.\n\n\nThen click Create Project.\n\n\n\nTask 1: Open a new RScript\nNow we will open a new RScript.\nNavigate to File &gt; New File &gt; RScript.\nThis RScript file is what you will use to outline your analysis.\n\n\n\nOpen a new RScript.\n\n\n\n\n\nTask 2: Write a preamble\nA preamble is similar to the heading of a paper.\nThe preamble contains information that will be useful for you and your collaborators.\n\n# Use the `#` symbol to tell R to ignore the text\n## Name: &lt;include your full name&gt;\n## Date: &lt;sometimes you may want to add a date&gt;\n## Purpose: &lt;insert the goals or purpose of the RScript&gt;\n\nRemember to use the # symbol to write your preamble.\n\n\n\nTask 3: Check your working directory\nPrior to inserting any code, it is generally helpful to check your working directory.\nThis will ensure that you are in the right location to call and save files.\n\n# get the working directory\ngetwd()\n\nIf, for any reason, the working directory is different from where you saved your R-project, check in the top right of your screen. Here, you should be able to select stats-pt2 or open the project using the menu options.\n\n\n\nTask 4: Install your packages\nWe will use the install.packages function to complete a few tasks.\nAny required packages are generally placed at the very top of our code.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"remotes\", repos = \"http://cran.us.r-project.org\")\n\n\n\n\nTask 5: Load your libraries\nAfter using the install.packages function, we need to load specific libraries.\nSimilar to installing packages, we generally load libraries at the very top of our code.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(remotes)\n\n\n\n\nTask 6: Call in the critstats data package\nWe will use the remotes::install_github() command to install packages to call in data that I have prepared in a package for our course. The remotes::install_github() command communicates with GitHub to access files.\n\nremotes::install_github(\"professornaite/critstats\", force=TRUE)\n\n\n\nTask 6-a: Load the critstats library\nNext, we load the critstats library which will give us access to the package’s contents.\n\nlibrary(critstats)\n\n\n\n\n\nTask 7: View datasets in the current R session\nRStudio has a lot of pre-loaded data sets that we can view or use to practice on different types of variables. We use the data() function to view all of the data sets loaded in R.\n\ndata()\n\nA new window should open in your RStudio session.\n\n\n\nTask 8: View datasets only in the critstats data package\nWe can also view only the data sets loaded into the critstats package.\n\ndata(package=\"critstats\")\n\nA new window containing only the critstats data should open in your RStudio session.\n\n\n\nTask 9: View documentation for another data package called datasets\n\ndata(package=\"datasets\")\n\n\n\n\nTask 10: View documentation for the entire critstats data package\n\n??critstats\n\n\n\n\nTask 11: View documentation for a specific data set\nWe will select the africa_data_all data set and view its documentation.\n\n??africa_data_all\n\nThis documentation is the codebook for the data. It contains more specific information about the data frame, each of the variables, and any sourcing information.\n\n\n\nTask 12: Assign a dataset to an object for efficiency\nIf we want to call a data set more efficiently, we can assign it to an object.\nThe df object is a short name for data frame.\nWe will assign df1 using our assignment operator to the data set.\n\ndf1 &lt;- africa_data_all\n\n\n\n\nTask 13: Run a simple command\nWe can run simple commands on a data frame using the shorthand object we assigned to it.\nLet’s use the glimpse() function to explore the africa_data_all data frame that we assigned to the object df1.\n\nglimpse(df1)\n\nRows: 116\nColumns: 13\n$ country           &lt;chr&gt; \"Nigeria\", \"Ethiopia\", \"Egypt\", \"DR Congo\", \"South A…\n$ pop               &lt;dbl&gt; 206139589, 114963588, 102334404, 89561403, 59308690,…\n$ pop.yearly.change &lt;dbl&gt; 2.58, 2.57, 1.94, 3.19, 1.28, 2.98, 2.28, 3.32, 1.85…\n$ pop.net.change    &lt;dbl&gt; 5175990, 2884858, 1946331, 2770836, 750420, 1728755,…\n$ density           &lt;dbl&gt; 226, 115, 103, 40, 49, 67, 94, 229, 18, 25, 83, 26, …\n$ area              &lt;dbl&gt; 910770, 1000000, 995450, 2267050, 1213090, 885800, 5…\n$ migrants          &lt;dbl&gt; -60000, 30000, -38033, 23861, 145405, -40076, -10000…\n$ fertility.rate    &lt;dbl&gt; 5.4, 4.3, 3.3, 6.0, 2.4, 4.9, 3.5, 5.0, 3.1, 4.4, 2.…\n$ med.age           &lt;dbl&gt; 18, 19, 25, 17, 28, 18, 20, 17, 29, 20, 30, 17, 22, …\n$ urban.pop         &lt;dbl&gt; 52, 21, 43, 46, 67, 37, 28, 26, 73, 35, 64, 67, 57, …\n$ world.share       &lt;dbl&gt; 2.64, 1.47, 1.31, 1.15, 0.76, 0.77, 0.69, 0.59, 0.56…\n$ pop_in_mill       &lt;dbl&gt; 206.13959, 114.96359, 102.33440, 89.56140, 59.30869,…\n$ year              &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020…\n\n\n\n\n\nTask 14: View your data\nWe can use the view() function to see our entire data frame.\n\nview(df1)\n\nYou may have noticed that a new window opened so that you can view() the entire data set. Sometimes, it is inefficient to use this command. Let’s look at two other options.\n\n\nTask 14-a: View the head of your data\nWe use the head function to get a closer look at the first few observations.\n\n# use the `head` function to view the top of the data\nhead(df1)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Nigeria        2.06e8              2.58        5175990     226 9.11e5   -60000\n2 Ethiopia       1.15e8              2.57        2884858     115 1   e6    30000\n3 Egypt          1.02e8              1.94        1946331     103 9.95e5   -38033\n4 DR Congo       8.96e7              3.19        2770836      40 2.27e6    23861\n5 South Africa   5.93e7              1.28         750420      49 1.21e6   145405\n6 Tanzania       5.97e7              2.98        1728755      67 8.86e5   -40076\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nChange the number of observations that you want to view in the data frame.\n\n# use the `head` function to view ten (10) observations at the top of the data\nhead(df1, n=10)\n\n# A tibble: 10 × 13\n   country          pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Nigeria       2.06e8              2.58        5175990     226 9.11e5   -60000\n 2 Ethiopia      1.15e8              2.57        2884858     115 1   e6    30000\n 3 Egypt         1.02e8              1.94        1946331     103 9.95e5   -38033\n 4 DR Congo      8.96e7              3.19        2770836      40 2.27e6    23861\n 5 South Africa  5.93e7              1.28         750420      49 1.21e6   145405\n 6 Tanzania      5.97e7              2.98        1728755      67 8.86e5   -40076\n 7 Kenya         5.38e7              2.28        1197323      94 5.69e5   -10000\n 8 Uganda        4.57e7              3.32        1471413     229 2.00e5   168694\n 9 Algeria       4.39e7              1.85         797990      18 2.38e6   -10000\n10 Sudan         4.38e7              2.42        1036022      25 1.77e6   -50000\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\n\n\n\nTask 14-b: View the tail of your data\nWe use the tail function to get a closer look at the last few observations.\n\n# use the `tail` function to view the top of the data\ntail(df1)\n\n# A tibble: 6 × 13\n  country           pop pop.yearly.change pop.net.change density   area migrants\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Cabo Verde     598682              0.93           5533     149   4030    -1227\n2 Western Sahara 587259              1.96          11273       2 266000     5600\n3 Mayotte        335995              3.03           9894     896    375        0\n4 Sao Tome & Pr… 231856              1.97           4476     242    960     -600\n5 Seychelles     107660              0.51            542     234    460     -200\n6 Saint Helena     5314             -1.12            -60      14    390        0\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nChange the number of observations that you want to view in the data frame.\n\n# use the `tail` function to view ten (10) observations at the top of the data\ntail(df1, n=10)\n\n# A tibble: 10 × 13\n   country          pop pop.yearly.change pop.net.change density   area migrants\n   &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Eswatini      1.21e6              0.76           9152      70  17200    -5268\n 2 Djibouti      1.14e6              1.39          15606      49  23180      900\n 3 Réunion       9.82e5              0.8            7744     393   2500     -630\n 4 Comoros       8.52e5              1.83          15301     458   1861    -2000\n 5 Cabo Verde    5.99e5              0.93           5533     149   4030    -1227\n 6 Western Saha… 5.87e5              1.96          11273       2 266000     5600\n 7 Mayotte       3.36e5              3.03           9894     896    375        0\n 8 Sao Tome & P… 2.32e5              1.97           4476     242    960     -600\n 9 Seychelles    1.08e5              0.51            542     234    460     -200\n10 Saint Helena  5.31e3             -1.12            -60      14    390        0\n# ℹ 6 more variables: fertility.rate &lt;dbl&gt;, med.age &lt;dbl&gt;, urban.pop &lt;dbl&gt;,\n#   world.share &lt;dbl&gt;, pop_in_mill &lt;dbl&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\nTask 15: Make and save a plot\nThe goal of this last task is to learn how to save a plot.\nOn the bottom left of your RStudio session, you should notice a few tabs.\nThe second tab is the Plots tab.\n\n\n\nFind the Plots tab.\n\n\n\n\nTask 15-a: Make a plot\nWe will make a random plot using the plot command (the plot itself is not important for now).\n\nplot(df1$pop)\n\nWhenever you make a plot, you can save it in your working directory.\nBe sure to check your working directory using the getwd command prior to saving.\n\n\n\nTask 15-b: Save a plot\nWhenever you want to save a plot, you can do so manually in directory portion of your RStudio session.\n\nNavigate to the bottom right portion of your RStudio session.\nGot to the Plots tab.\n\nRun the code for your plot, or navigate to your plot using the arrows.\n\nClick Export\n\nYou can save your file as a .pdf or as an image.\n\n\nLater, we will learn to customize plots and insert them into reports and papers.\n\n\n\nSee the bottom right of the screen.\n\n\n\n\n\nNext up: Week 6"
  },
  {
    "objectID": "week5.html#footnotes",
    "href": "week5.html#footnotes",
    "title": "DATA 202 - Week 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe are in part two of our course, so I have chosen stats-pt2 for this project’s title. We’ll be starting other projects in parts three and four of our course. Note: there will be no project for part one of our course.↩︎"
  },
  {
    "objectID": "week5-slides.html#part-i-context",
    "href": "week5-slides.html#part-i-context",
    "title": "DATA 202 - Week 5",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "week5-slides.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "href": "week5-slides.html#image-from-the-w.e.b.-dubois-collection.-httpscredo.library.umass.edu",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Example 5.4: The Atlanta University Studies\n\n\n\nImage from The W.E.B. DuBois Collection. https://credo.library.umass.edu"
  },
  {
    "objectID": "week5-slides.html#part-ii-content",
    "href": "week5-slides.html#part-ii-content",
    "title": "DATA 202 - Week 5",
    "section": "Part II: Content",
    "text": "Part II: Content"
  },
  {
    "objectID": "week5-slides.html#correlation-and-linear-regression.-image-from-lennys-newsletter.",
    "href": "week5-slides.html#correlation-and-linear-regression.-image-from-lennys-newsletter.",
    "title": "DATA 202 - Week 5",
    "section": "",
    "text": "Chi-square test\nWhen using correlation and regression analyses, one core assumption is that the variables are quantitative in nature.\nWe use a chi-square test to study categorical variables.\n\n\n\nExample research question for a chi-square test. Image from Datatab.net.\n\n\nIn the example above, the null hypothesis is that there is no relationship between gender and highest level of education; the alternative hypothesis is that there is a relationship between gender and highest level of education."
  },
  {
    "objectID": "week5-slides.html#part-iii-code",
    "href": "week5-slides.html#part-iii-code",
    "title": "DATA 202 - Week 5",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will help prepare you to complete Lab 1.\nLab 1 focuses on univariate statistics.\nUnivariate statistics refer to analyses that describe a single variable or attribute."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "DATA 202 - Week 4",
    "section": "",
    "text": "Last week, we considered one cyclical process of theory construction.\n\n\n\nTheory construction. Image from saylordotorg.github.io\n\n\n\nThis week, we return to this idea of theory construction but with more detail.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\n\n\nResearch inquiry\n– Transfer your reading and informal observations into pratical problems\nHypotheses\n– Utilize the research literature to understand your research problem\n– Extend this research literature by developing your own research questions\n– Based on your research question(s), develop a hypothesis (or a set of hypotheses)\nAnalysis\n– Outline your empirical study in specific details to conduct your analysis\n– Conduct your analysis and  consider the limitations of your methods \nEvaluation\n– Evaluate your analytic findings alongside your hypotheses\nRevision\n– Utilize the reseaerch literature to revise or confirm your hypotheses\n\n\n\n\n\nAcross research methodologies, a limitation is any feature of a study that may cause concerns. Limitations vary in both their scope and context. As a result, it is important to consider both the obvious limitations and hidden concerns.\n\nSome examples of limitations in statistics\n\n\n- Lack of reliable data\n\n- Limited sample size\n\n- Deficiencies in measurements of data\n\n\nNot limitations but “bad” statistical practices\n\n\n- Theory does not depict the entire story or phenomenon\n\n- Old data (and research citations)\n\n- Broad conclusions with no supporting data\n\n- Analyzing data for significant results. P-hacking!\n\n\n\n\n\nLogic is an important component in theory construction. Logical reasoning is the use of critical thinking in the applications of statistics. However, logic in relation to theory is based on a set of ideas that help build sound and consistent arguments.\nA few key considerations:\n\n\nPremises and conclusions\n– Clear statements are used; statements are focused and direct\nInternal structure\n– There is consistency in your premises and any conclusions\n– There are no contradictions in your structure\nArguments and inferences\n– Your argument follow your initial premises, internal structure, and conclusions\n\n\n\n\n\n\nOne way to visualize the relationships between your variables is in a path diagram.\n\nPath diagrams are used in path analysis, a subset of statistical methods that help researchers discern and assess the relationship(s) between multiple variables.\nPath analysis is based on a closed system of nested relationships.\n– These nested relationships must have a logical internal structure.\nTogether, a path diagram can represent a series of structured linear regression equations.\nPath models are often used in economics and political science.\n\nAs we move further into our analyses, we will learn more about path analysis.\n\nThis is a simple two-variable path diagram\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\n\n\n\nIdentify two variables for a simple path diagram.\n\\(X\\) (independent variable) and \\(Y\\) (dependent variable)\n\nWhat is the logical relationship between the variables?\n– Write a logical statement.\nWhat is the question that structures the relationship between the variables?\n– Turn your logic statement into a question.\nWhat is your theory on how the two variables relate to one another?\n– Write a theoretical statement based on the research literature.\nWhat is your hypothesis?\n– Develop an educated guess based on the research literature.\n\n\nThree variable path diagram\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n\n\n\n\n\nA path diagram made of multiple variables\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n  D[Variable 3] --&gt; B"
  },
  {
    "objectID": "week4.html#more-on-theory-construction",
    "href": "week4.html#more-on-theory-construction",
    "title": "DATA 202 - Week 4",
    "section": "",
    "text": "Last week, we considered one cyclical process of theory construction.\n\n\n\nTheory construction. Image from saylordotorg.github.io\n\n\n\nThis week, we return to this idea of theory construction but with more detail.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\n\n\nResearch inquiry\n– Transfer your reading and informal observations into pratical problems\nHypotheses\n– Utilize the research literature to understand your research problem\n– Extend this research literature by developing your own research questions\n– Based on your research question(s), develop a hypothesis (or a set of hypotheses)\nAnalysis\n– Outline your empirical study in specific details to conduct your analysis\n– Conduct your analysis and  consider the limitations of your methods \nEvaluation\n– Evaluate your analytic findings alongside your hypotheses\nRevision\n– Utilize the reseaerch literature to revise or confirm your hypotheses\n\n\n\n\n\nAcross research methodologies, a limitation is any feature of a study that may cause concerns. Limitations vary in both their scope and context. As a result, it is important to consider both the obvious limitations and hidden concerns.\n\nSome examples of limitations in statistics\n\n\n- Lack of reliable data\n\n- Limited sample size\n\n- Deficiencies in measurements of data\n\n\nNot limitations but “bad” statistical practices\n\n\n- Theory does not depict the entire story or phenomenon\n\n- Old data (and research citations)\n\n- Broad conclusions with no supporting data\n\n- Analyzing data for significant results. P-hacking!\n\n\n\n\n\nLogic is an important component in theory construction. Logical reasoning is the use of critical thinking in the applications of statistics. However, logic in relation to theory is based on a set of ideas that help build sound and consistent arguments.\nA few key considerations:\n\n\nPremises and conclusions\n– Clear statements are used; statements are focused and direct\nInternal structure\n– There is consistency in your premises and any conclusions\n– There are no contradictions in your structure\nArguments and inferences\n– Your argument follow your initial premises, internal structure, and conclusions\n\n\n\n\n\n\nOne way to visualize the relationships between your variables is in a path diagram.\n\nPath diagrams are used in path analysis, a subset of statistical methods that help researchers discern and assess the relationship(s) between multiple variables.\nPath analysis is based on a closed system of nested relationships.\n– These nested relationships must have a logical internal structure.\nTogether, a path diagram can represent a series of structured linear regression equations.\nPath models are often used in economics and political science.\n\nAs we move further into our analyses, we will learn more about path analysis.\n\nThis is a simple two-variable path diagram\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\n\n\n\nIdentify two variables for a simple path diagram.\n\\(X\\) (independent variable) and \\(Y\\) (dependent variable)\n\nWhat is the logical relationship between the variables?\n– Write a logical statement.\nWhat is the question that structures the relationship between the variables?\n– Turn your logic statement into a question.\nWhat is your theory on how the two variables relate to one another?\n– Write a theoretical statement based on the research literature.\nWhat is your hypothesis?\n– Develop an educated guess based on the research literature.\n\n\nThree variable path diagram\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n\n\n\n\n\nA path diagram made of multiple variables\n\n\n\n\nflowchart LR\n  A[Variable 1] --&gt; B[Y]\n  C[Variable 2] --&gt; B\n  D[Variable 3] --&gt; B"
  },
  {
    "objectID": "week4.html#task-1-open-a-new-rscript-and-write-a-preamble",
    "href": "week4.html#task-1-open-a-new-rscript-and-write-a-preamble",
    "title": "DATA 202 - Week 4",
    "section": "Task 1: Open a new RScript and write a preamble",
    "text": "Task 1: Open a new RScript and write a preamble\nPrior to conducting any analyses in R, we want to include a brief summmary of our work so that we (and others who read our code) know what we are prioritizing. This is an important step to documenting our research. We will write this preamble at the very top of our RScript file.\nA preamble is simply an introduction to your code. The structure of a preamble can vary widely.\nIn an RScript file, we use hashtags (#) to write certain sections in our preamble. The # symbol informs R that it should ignore that line in our code.\n\nTask 1-a: Write your preamble\n\n# project name: &lt;insert an appropriate title for your project here&gt;\n# date created: Sept 12, 2023\n# last edited: &lt;as you update your code, keep a record of your last edit&gt;\n# author: N. Alexander\n# notes: Creating a data frame on college students' time spent studying and exam score"
  },
  {
    "objectID": "week4.html#task-2-create-a-data-frame-of-the-observations-using-vectors",
    "href": "week4.html#task-2-create-a-data-frame-of-the-observations-using-vectors",
    "title": "DATA 202 - Week 4",
    "section": "Task 2: Create a data frame of the observations using vectors",
    "text": "Task 2: Create a data frame of the observations using vectors\nWhen we do not have raw sample data from an excel or .cvs file available, we can insert data using the data.frame function. As a warm up activity, we will recreate the sample data below by concatenating values into a data.frame\n\n\n\ntime\nscore\ngrade\ntextbook\n\n\n\n\n30\n51\nF\nT\n\n\n180\n93\nA\nF\n\n\n45\n65\nD\nF\n\n\n0\n73\nC\nF\n\n\n240\n89\nB\nT\n\n\n15\n84\nB\nT\n\n\n90\n79\nC\nF\n\n\n\nFor this data set, our variables are defined as follows: time = time spent studying (in minutes); score = score on math exam (out of 100); grade = letter grade cooresponding to score; textbook = participant response to the question: “I used my textbook to study”.\nNormally, we would want to create a codebook to store this information. More on this soon.\n\n\nTask 2-a: Here is the sample code to recreate the table above\nAs you look at the code, try to remember the differences between numeric, character and logic values.\n\ntime &lt;- c(30, 180, 45, 0, 240, 15, 90)\nscore &lt;- c(51, 93, 65, 73, 89, 64, 79)\ngrade &lt;- c(\"F\", \"A\",\"D\",\"C\",\"B\",\"D\", \"C\")\ntextbook &lt;- c(T, F, F, F, T, T, F)\ndf &lt;- data.frame(time, score, grade, textbook)\ndf\n\n  time score grade textbook\n1   30    51     F     TRUE\n2  180    93     A    FALSE\n3   45    65     D    FALSE\n4    0    73     C    FALSE\n5  240    89     B     TRUE\n6   15    64     D     TRUE\n7   90    79     C    FALSE\n\n\nThe only values that need quotes ” ” or ’ ’ are character values.\n\n\n\nTask 2-b: Take a look at your data frame\nWe can use View(df) to take a look at the data set we have just created (Notice that the ‘V’ in view is capitalized). This command will open up a new window in RStudio so that you can see how you organized the data.\n\n\n\nTask 2-c: See a summary of the variables in the data set\nWe can use the _summary() command to see a summary of the variables in our data set\n\nsummary(df)\n\n      time            score          grade            textbook      \n Min.   :  0.00   Min.   :51.00   Length:7           Mode :logical  \n 1st Qu.: 22.50   1st Qu.:64.50   Class :character   FALSE:4        \n Median : 45.00   Median :73.00   Mode  :character   TRUE :3        \n Mean   : 85.71   Mean   :73.43                                     \n 3rd Qu.:135.00   3rd Qu.:84.00                                     \n Max.   :240.00   Max.   :93.00                                     \n\n\nWhen you look at the summary of the data, what are some of your initial observations?\n\n\n\nTask 2-d: Use a few simple commands to visualize your data\nWe can use plot(IV, DV), where IV = independent variable and DV = dependent variable to create a scatterplot of our variables of interest\n\nplot(time, score)\n\n\n\n\nWhat are some of your initial ovserations when you look at the visual plot of your data?\nWe can also create a histogram for a single numeric variable by using the hist( ) command to create quick visual summaries of our data\n\nhist(time)\n\n\n\n\n\nhist(score)\n\n\n\n\n\n\n\nNext up: Week 5\nWe will continue with our statistical work and load real-world data into R."
  },
  {
    "objectID": "week4-slides.html#more-on-theory-construction",
    "href": "week4-slides.html#more-on-theory-construction",
    "title": "DATA 202 - Week 4",
    "section": "More on theory construction",
    "text": "More on theory construction\nLast week, we considered one cyclical process of theory construction.\n\nTheory construction. Image from saylordotorg.github.io"
  },
  {
    "objectID": "week4-slides.html#task-1-open-a-new-rscript-and-write-a-preamble",
    "href": "week4-slides.html#task-1-open-a-new-rscript-and-write-a-preamble",
    "title": "DATA 202 - Week 4",
    "section": "Task 1: Open a new RScript and write a preamble",
    "text": "Task 1: Open a new RScript and write a preamble\nPrior to conducting any analyses in R, we want to include a brief summmary of our work so that we (and others who read our code) know what we are prioritizing. This is an important step to documenting our research. We will write this preamble at the very top of our RScript file.\nA preamble is simply an introduction to your code. The structure of a preamble can vary widely.\nIn an RScript file, we use hashtags (#) to write certain sections in our preamble. The # symbol informs R that it should ignore that line in our code.\nTask 1-a: Write your preamble\n\n# project name: &lt;insert an appropriate title for your project here&gt;\n# date created: Sept 12, 2023\n# last edited: &lt;as you update your code, keep a record of your last edit&gt;\n# author: N. Alexander\n# notes: Creating a data frame on college students' time spent studying and exam score"
  },
  {
    "objectID": "week4-slides.html#task-2-create-a-data-frame-of-the-observations-using-vectors",
    "href": "week4-slides.html#task-2-create-a-data-frame-of-the-observations-using-vectors",
    "title": "DATA 202 - Week 4",
    "section": "Task 2: Create a data frame of the observations using vectors",
    "text": "Task 2: Create a data frame of the observations using vectors\nWhen we do not have raw sample data from an excel or .cvs file available, we can insert data using the data.frame function. As a warm up activity, we will recreate the sample data below by concatenating values into a data.frame\n\n\n\ntime\nscore\ngrade\ntextbook\n\n\n\n\n30\n51\nF\nT\n\n\n180\n93\nA\nF\n\n\n45\n65\nD\nF\n\n\n0\n73\nC\nF\n\n\n240\n89\nB\nT\n\n\n15\n84\nB\nT\n\n\n90\n79\nC\nF\n\n\n\nFor this data set, our variables are defined as follows: time = time spent studying (in minutes); score = score on math exam (out of 100); grade = letter grade cooresponding to score; textbook = participant response to the question: “I used my textbook to study”.\nNormally, we would want to create a codebook to store this information. More on this soon."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Racial injustice is a leading historical and contemporary social issue across the globe.\n\n\n\nImage from Prison Policy Initiative\n\n\n\nBuilding new theories around the various types of injustices requires empirical analysis.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFor research, you may seek to test the validity of certain claims."
  },
  {
    "objectID": "week7.html#part-i-context",
    "href": "week7.html#part-i-context",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Racial injustice is a leading historical and contemporary social issue across the globe.\n\n\n\nImage from Prison Policy Initiative\n\n\n\nBuilding new theories around the various types of injustices requires empirical analysis.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFor research, you may seek to test the validity of certain claims."
  },
  {
    "objectID": "week7.html#image-from-bestcolleges.com",
    "href": "week7.html#image-from-bestcolleges.com",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Understanding the social in justice\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\nImage from Northeast Global News\n\n\n\n\nThe Murder of George Floyd\nWhat do we know about George Floyd and the events that occured on May 25, 2020?\n\n\n\nImage from Mass Humanities\n\n\nIn your free time you can read How George Floyd Was Killed in Police Custody. Image from NY Times. to understand a step-by-step reconstruction of the incident.\n\n\nYahoo! News Race and Justice poll results\nPoll results are stored in the yahoo_data data set located in the critstats package."
  },
  {
    "objectID": "week7.html#part-ii-content",
    "href": "week7.html#part-ii-content",
    "title": "DATA 202 - Week 7",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatterplots\nCorrelation coefficients\nSimple linear regression\n\n\n\nSimple linear regression\nA simple linear regression (sometimes referenced as a bivariate regression) is a linear equation describing the relationship between an explanatory variable and an outcome variable.\nThere is an assumption that the explanatory variable influences the outcome variable, and not the other way around.\nTake, for example, a variable \\(y_i\\) which denotes the income of some individual in a sample, and we index this data using \\(i\\) where \\(i \\in \\{1, 2, ..., n\\}\\). We can let some other variable in our data \\(x_i\\) represent the years of education for the same individual. A simple linear regression equation of these variables take the following form: \\[y_i = b_0 + b_{1}x_{i} + e_i\\]\nwhere \\(b_i\\) is the sample estimate of the slope of the regression line with respect to the years of education and \\(b_0\\) is the sample estimate for the vertical intercept of the regression line.\n\n\n\nCorrelation coefficients and scatterplots\nAs a reminder, correlation ranges from -1 to 1. It gives us an indication on two things:\n\nThe direction of the relationship between the two variables\nThe strength of the relationship between the two variables\n\nOutliers matter a lot when considering correlation coefficients."
  },
  {
    "objectID": "week7.html#outliers-in-pearson-correlation.-image-from-stats-and-r.-image-from-datatab.net",
    "href": "week7.html#outliers-in-pearson-correlation.-image-from-stats-and-r.-image-from-datatab.net",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Estimating differences in proportions\nWhen generating cross tabulations, we can make sense of a few bivariate tests:\n\nDifferences in proportions\nStandard errors of the difference in proportions\nConfidence intervals for the differences\nT-test for differences in proportions"
  },
  {
    "objectID": "week7.html#part-iii-code",
    "href": "week7.html#part-iii-code",
    "title": "DATA 202 - Week 7",
    "section": "Part III: Code",
    "text": "Part III: Code\nThe code for this week will prepare you to run analyses for Paper #2.\nPaper #2 is a short exploration of a data set in the forcats package.\nA sample RScript and workflow is provided here.\n\n\nWrite preamble\n\n# ---\n# title: Exploring associations between income and political party in the US\n# subtitle: sample paper 2\n# author: Nathan Alexander\n# course: DATA 202 - fall 2023\n# ---\n\n# research inquiry: does income relate to political party support in the US?\n# data: 2020 sample data from the General Social Survey (GSS)\n# note(s): variables should be mutated and recoded into two levels\n\n\n\n\nstep 0: install packages and load libraries\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nstep 1: view gss_cat data in the package forcats\n\ngetwd() # check working directory\n?gss_cat # view data documentation\ngss_cat # call data frame\nglimpse(gss_cat) # glimpse data\nsummary(gss_cat) # view summary of data\n\n\n\n\nstep 2: clean and manage data\n\ngss_cat_clean &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  select(year, rincome, partyid) %&gt;% \n  rename(income = rincome) %&gt;% \n  rename(party = partyid)\n\ngss_cat_clean # view cleaned data\n\n\n\n\nstep 3: subset data: year == 2000\n\ndf &lt;- gss_cat_clean %&gt;% \n  filter(year==2000)\n\nhead(df) # view top of data\ntail(df) # view bottom of data\nsummary(df) # check data\n\n\n\n\nstep 4: inspect and transform income variable into two levels\n\n## create a frequency table of each level in the income variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Don't know', 'Refused', and 'Not applicable' levels\ndf &lt;- df %&gt;%  \n  filter(income != \"No answer\",\n         income != \"Don't know\",\n         income != \"Refused\",\n         income != \"Not applicable\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## use levels() function to view levels for income variable\nlevels(df$income)\n\n## create two levels: below $20,000 and above $20,000\ndf &lt;- df %&gt;% \n  mutate(income = fct_recode(income, \n                             \"More than 20000\" = \"$25000 or more\",\n                             \"More than 20000\" = \"$20000 - 24999\",\n                             \"Less than 20000\" = \"$15000 - 19999\",\n                             \"Less than 20000\" = \"$10000 - 14999\",\n                             \"Less than 20000\" = \"$8000 to 9999\",\n                             \"Less than 20000\" = \"$7000 to 7999\",\n                             \"Less than 20000\" = \"$6000 to 6999\",\n                             \"Less than 20000\" = \"$5000 to 5999\",\n                             \"Less than 20000\" = \"$4000 to 4999\",\n                             \"Less than 20000\" = \"$3000 to 3999\",\n                             \"Less than 20000\" = \"$1000 to 2999\",\n                             \"Less than 20000\" = \"Lt $1000\"))\n\n## view a summary of your transformed data frame\nsummary(df)\n\n\n\n\nstep 5: inspect and transform party variable into two levels\n\n## create a frequency table of each level in the party variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Independent' and 'Other Party' levels\ndf &lt;- df %&gt;%  \n  filter(party != \"No answer\",\n         party != \"Independent\",\n         party != \"Other party\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## create two levels: 'Republican' and 'Democrat'\ndf &lt;- df %&gt;% \n  mutate(party = fct_recode(party,\n                            \"Republican\" = \"Strong republican\",\n                            \"Republican\" = \"Not str republican\",\n                            \"Republican\" = \"Ind,near rep\",\n                            \"Democrat\" = \"Ind,near dem\",\n                            \"Democrat\" = \"Not str democrat\",\n                            \"Democrat\" = \"Strong democrat\"))\n\n## remove year from data frame\ndf &lt;- df %&gt;% \n  select(-year)\n\n## view a summary of the data to check for any errors\nsummary(df)\n\n\n\n\nstep 6: visualize data\n\n## create a frequency table and bar graph of income \ntable.income = table(df$income)\ntable.income\nbarplot(table.income,\n        main = \"Bar graph of Income\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$income),\n        main = \"Bar graph of Income\",\n        col = \"lightgreen\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\",\n        ylim = c(0,650)) # this y-axis range: (0, 650) works best for my plot\n## create a frequency table and bar graph of party \ntable.party = table(df$party)\ntable.party\nbarplot(table.party,\n        main = \"Bar graph of Party\",\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$party),\n        main = \"Bar graph of Party\",\n        col = c(\"red\",\"blue\"),\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\",\n        ylim = c(0,600)) # this y-axis range: (0, 550) works best for my plot\n## create a stacked bar plot of the proportions\n#### question: which of the two plots do you prefer, why?\nplot(df$income, df$party)\nplot(df$party, df$income)\n## the below code produces similar outputs as above with specifications\nplot(df$party, df$income,\n     main = \"Mosaic Plot of Political Party and Income\",\n     col = c(\"lightyellow\",\"lightgreen\"),\n     xlab = \"Political Party\",\n     ylab = \"Income\")\n\n\n\n\nstep 7: create a basic cross tab for manual calculations\n\n## gather sample size\nn = count(df)\nn\n\n## view a basic cross tabulation\ntable(df$income, df$party)\n\n\n\n\nstep 8: statistical analyses in R\n\n## load required libraries (and packages, where needed)\ninstall.packages(\"descr\", repos = \"http://cran.us.r-project.org\")\nlibrary(descr)\n\ninstall.packages(\"Hmisc\", repos = \"http://cran.us.r-project.org\")\nlibrary(Hmisc)\n\n## create a cross tab (list dependent variable in your hypothesis first)\ncrosstab(df$party, df$income)\n\n## add column percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.c=T) # add column percentages\n## add row percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.r=T) # add row percentages\n\n## get expected frequencies and cell chi-square contributions\ncrosstab(df$party, df$income,\n         expected = T, # get expected values\n         prop.chisq=T) # get chi-square contribution\n\n## get critical value of chi-square, p=.05, df=1\n#### recall: df = (r-1)(c-1)\nqchisq(.05, 1, lower.tail=F)\n\n## get chi-square statistic\nchisq.test(df$party, df$income)\n\n\n\n\nstep 9: describe some initial limitations of analysis\n\n### limitation 1: sampling error\n# data come from a sample and there are likely differences in other samples\n\n### limitation 2: category reductions\n# creating two levels for the variables greatly impacted the diversity of responses\n\n### limitation 3: cases dropped\n# sample was further impacted by the number of values dropped in the analysis\n\n### limitation 4: chi-square test\n# the chi-square test does not tell us about the strength or direction of association\n\n\n\n\nNext up: Week 8"
  },
  {
    "objectID": "week7-slides.html#part-i-context",
    "href": "week7-slides.html#part-i-context",
    "title": "DATA 202 - Week 7",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "week7-slides.html#image-from-bestcolleges.com",
    "href": "week7-slides.html#image-from-bestcolleges.com",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Understanding the social in justice\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\nImage from Northeast Global News"
  },
  {
    "objectID": "week7-slides.html#part-ii-content",
    "href": "week7-slides.html#part-ii-content",
    "title": "DATA 202 - Week 7",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatterplots\nCorrelation coefficients\nSimple linear regression"
  },
  {
    "objectID": "week7-slides.html#outliers-in-pearson-correlation.-image-from-stats-and-r.-image-from-datatab.net",
    "href": "week7-slides.html#outliers-in-pearson-correlation.-image-from-stats-and-r.-image-from-datatab.net",
    "title": "DATA 202 - Week 7",
    "section": "",
    "text": "Estimating differences in proportions\nWhen generating cross tabulations, we can make sense of a few bivariate tests:\n\nDifferences in proportions\nStandard errors of the difference in proportions\nConfidence intervals for the differences\nT-test for differences in proportions"
  },
  {
    "objectID": "week7-slides.html#part-iii-code",
    "href": "week7-slides.html#part-iii-code",
    "title": "DATA 202 - Week 7",
    "section": "Part III: Code",
    "text": "Part III: Code\nThe code for this week will prepare you to run analyses for Paper #2.\nPaper #2 is a short exploration of a data set in the forcats package.\nA sample RScript and workflow is provided here."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain.\n\n\n\n\n\n\n\n\n\n\nThe Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline."
  },
  {
    "objectID": "week2.html#case-study",
    "href": "week2.html#case-study",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain."
  },
  {
    "objectID": "week2.html#institutional-review-board-irb",
    "href": "week2.html#institutional-review-board-irb",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "The Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline."
  },
  {
    "objectID": "week2.html#sets-and-numbers",
    "href": "week2.html#sets-and-numbers",
    "title": "DATA 202 - Week 2",
    "section": "Sets and numbers",
    "text": "Sets and numbers\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers\n\n\n\n– Natural numbers: \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\)\n– Whole numbers: \\(\\mathbb{N_0} = \\{0, 1, 2, 3, ...\\}\\)\n– Integers: \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ...\\}\\)\n– Rational numbers: \\(\\mathbb{Q} = \\Big\\{\\dfrac{p}{q}, p \\in \\mathbb{Z}, q \\in \\mathbb{Z}, q \\neq 0 \\Big\\}\\)\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers, continued…\n\n\n\n\nIrrational numbers\n\nany number that is not a rational number; irrational means not rational (no ratio)\ne.g., you may know some irrational numbers such as \\(\\pi\\), \\(\\sqrt{2}\\), \\(\\sqrt{3}\\), \\(e\\) (Euler’s number)\n\nReal numbers: \\(\\mathbb{R}\\)\n\nThe set of numbers on the real number line\nThis set is constructed by combining the rational and irrational numbers\n\nImaginary numbers: \\(\\mathbb{I}\\)\n\na number that has a negative value when it is squared\n\\(i\\) is the unit imaginary number, \\(\\sqrt{-1} = i\\) by definition\nso \\(i\\) is a complex number since \\(i^2 = -1\\)\n\nComplex numbers: \\(\\mathbb{C}\\)\n\na number in the form \\(a + bi\\) where \\(a \\in \\mathbb{R}\\), \\(b \\in \\mathbb{R}\\), and \\(i\\) is an imaginary number"
  },
  {
    "objectID": "week2.html#what-is-statistics",
    "href": "week2.html#what-is-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What is statistics?",
    "text": "What is statistics?\nStatistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\n\n\n\nDEFINITION: Statistics\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, statistics calls for questions where we explore difference or change. We use variation to understand differences within or between a set (or sets) of measurements.\n\n\nWhat should it mean to be critical in the context of statistics?\nOn Canvas, in the Week 2 folder, there is a document titled “Critical Thinking” by Jennifer Duncan. This document is one example of how we can frame what it could or should mean to be critical in statistics. Please review this document.\n\n\nUsing a higher order of thinking. Duncan emphasizes that critical thinking is a higher order of thinking with different advanced thinking skills, and offers a few suggestions.\n\nWe base our thinking on logic and not on feelings.\nWe should look deeper into inferences for hidden assumptions or values.\nAsk complex questions that help build a critical inquiry.\n\n\n\n\nAsking complex questions. Duncan breaks down the process into a few sub-questions.\n\nWho is the implied audience?\nWhat are the strengths and weaknesses of the argument?\nWhat are the underlying assumptions and values?\n\n\n\n\nUsing a variety of thinking processes. Duncan defines a process around analyzing, synthesizing, interpreting, and evaluating information that helps with our thinking.\n\n\n\nReflecting on how we answer a question. Duncan ends with a set of questions that help us think about different points of view, if we have clarity, and if more details are needed."
  },
  {
    "objectID": "week2.html#getting-started-in-rstudio",
    "href": "week2.html#getting-started-in-rstudio",
    "title": "DATA 202 - Week 2",
    "section": "Getting started in RStudio",
    "text": "Getting started in RStudio\nIn Lab 0, you downloaded and installed base R and RStudio. In this section, we will learn more about R and RStudio.\nLet’s start with a little fun!\n\nFirst, install the ‘praise’ package.\n\n# install the package\ninstall.packages(\"praise\", repos = \"http://cran.us.r-project.org\")\n\n\nNext, load the library for the ‘praise’ package.\n\n# load library\nlibrary(praise)\n\n\nNow, get some praise!\n\n# get some praise\npraise()\n\nYou can keep inserting the code above to get praise when you need it!"
  },
  {
    "objectID": "week2.html#arithmetic-in-r",
    "href": "week2.html#arithmetic-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Arithmetic in R",
    "text": "Arithmetic in R\nWe will learn how to calculate values in R.\n\n\n1 + 2  # the 'plus sign' computes the sum\n\n[1] 3\n\n\n\n\n2 - 3  # the 'minus sign' computes the difference\n\n[1] -1\n\n\n\n\n3 * 4  # the 'asterisk' computes the product\n\n[1] 12\n\n\n\n\n4 / 5 # the 'forward slash' computes the quotient\n\n[1] 0.8\n\n\n\n\n# from hw exercise 0.2, we can compute the sum of the first 100 positive integers\nsum(1:100) \n\n[1] 5050"
  },
  {
    "objectID": "week2.html#variables-in-r",
    "href": "week2.html#variables-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Variables in R",
    "text": "Variables in R\nWe will learn to give a variable (or character) a value.\n\nUse the different assignment operators\n\ny = 2 # the equal sign can be used as an assignment operator\n\ny &lt;-2 # a \"less than\" sign and dash can also be used as an assignment operator\n\ny # R stores all values you assign, so you must \"call\" any variables to see their values\n\n[1] 2\n\n\n\nSet x equal to two added to three\n\nx = 2 + 3\nx\n\n[1] 5\n\n\n\nSet y equal to two minus three\n\ny = 2 - 3\ny\n\n[1] -1\n\n\n\nSet z equal to two times three\n\nz = 2 * 3\nz\n\n[1] 6\n\n\n\nOverwrite the value of y by setting y equal to x divided by z\n\ny = x / z\ny\n\n[1] 0.8333333\n\n\n\n\n\n\n\n\n\nPaper 1 is due on Thu August 31 at 11:59pm ET\n\n\n\nPlease see a sample paper #1 here.\n\n\nNext up: Week 3\nWe will consider the role of evaluation in using data for evidence. Specifically, we continue our explorations in R by learning how to load data sets into our data frame, and perform some basic operations using some additional packages. These packages will allow us to consider how we can construct original data sets to develop unique questions for our analysis."
  },
  {
    "objectID": "week2-slides.html#case-study",
    "href": "week2-slides.html#case-study",
    "title": "DATA 202 - Week 2",
    "section": "Case Study",
    "text": "Case Study\nTuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)"
  },
  {
    "objectID": "week2-slides.html#institutional-review-board-irb",
    "href": "week2-slides.html#institutional-review-board-irb",
    "title": "DATA 202 - Week 2",
    "section": "Institutional Review Board (IRB)",
    "text": "Institutional Review Board (IRB)\nThe Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline."
  },
  {
    "objectID": "week2-slides.html#sets-and-numbers",
    "href": "week2-slides.html#sets-and-numbers",
    "title": "DATA 202 - Week 2",
    "section": "Sets and numbers",
    "text": "Sets and numbers\n\n\n\nDEFINITIONS: Sets of numbers\n\n\n– Natural numbers: \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\)\n– Whole numbers: \\(\\mathbb{N_0} = \\{0, 1, 2, 3, ...\\}\\)\n– Integers: \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ...\\}\\)\n– Rational numbers: \\(\\mathbb{Q} = \\Big\\{\\dfrac{p}{q}, p \\in \\mathbb{Z}, q \\in \\mathbb{Z}, q \\neq 0 \\Big\\}\\)"
  },
  {
    "objectID": "week2-slides.html#what-is-statistics",
    "href": "week2-slides.html#what-is-statistics",
    "title": "DATA 202 - Week 2",
    "section": "What is statistics?",
    "text": "What is statistics?\nStatistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\nDEFINITION: Statistics\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, statistics calls for questions where we explore difference or change. We use variation to understand differences within or between a set (or sets) of measurements."
  },
  {
    "objectID": "week2-slides.html#getting-started-in-rstudio",
    "href": "week2-slides.html#getting-started-in-rstudio",
    "title": "DATA 202 - Week 2",
    "section": "Getting started in RStudio",
    "text": "Getting started in RStudio\nIn Lab 0, you downloaded and installed base R and RStudio. In this section, we will learn more about R and RStudio.\nLet’s start with a little fun!"
  },
  {
    "objectID": "week2-slides.html#arithmetic-in-r",
    "href": "week2-slides.html#arithmetic-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Arithmetic in R",
    "text": "Arithmetic in R\nWe will learn how to calculate values in R."
  },
  {
    "objectID": "week2-slides.html#variables-in-r",
    "href": "week2-slides.html#variables-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Variables in R",
    "text": "Variables in R\nWe will learn to give a variable (or character) a value."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "REMINDER: Paper 1 is due Thursday August 31 at 11:59pm ET\nYou can read more about paper #1 here.\nSee the instructions for paper assignments here.\nAlso, learn more about paper assignments here."
  },
  {
    "objectID": "week1.html#paper-1",
    "href": "week1.html#paper-1",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "REMINDER: Paper 1 is due Thursday August 31 at 11:59pm ET\nYou can read more about paper #1 here.\nSee the instructions for paper assignments here.\nAlso, learn more about paper assignments here."
  },
  {
    "objectID": "week1.html#tuskegee-experiement-of-untreated-syphillis",
    "href": "week1.html#tuskegee-experiement-of-untreated-syphillis",
    "title": "DATA 202 - Week 1",
    "section": "Tuskegee Experiement of Untreated Syphillis",
    "text": "Tuskegee Experiement of Untreated Syphillis\nYou should explore more information about our disucssion.\nOne place to start is here: “The Tuskegee Experiment: Crash Course Black American History #29” in the video below:\n\n\n\n\n\nNext up: Week 2"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Tuesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\nTable 1: DATA 202 course schedule (fall 2023)\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nAug 21 - Sep 15\n\n\nPart II: Data and people\n4 weeks\nSep 18 - Oct 13\n\n\nPart III: Data and policy\n4 weeks\nOct 16 - Nov 10\n\n\nPart IV: Data in practice\n3 weeks\nNov 13 - Dec 1\n\n\nCourse wrap-up\n2 weeks\nDec 4 - Dec 15\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nHomework (hw). Regular practice problems [25 points]\nMini-projects (labs and code). In Quarto Markdown (.qmd) [20 points]\nPapers. Four papers (08/31, 10/13, 11/10, 12/1 12/8) [20 points]\nExams. Midterm exam and a final exam [35 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW-0 and Lab-0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks."
  },
  {
    "objectID": "course-overview.html#a-slightly-more-detailed-overview.",
    "href": "course-overview.html#a-slightly-more-detailed-overview.",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Tuesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\nTable 1: DATA 202 course schedule (fall 2023)\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nAug 21 - Sep 15\n\n\nPart II: Data and people\n4 weeks\nSep 18 - Oct 13\n\n\nPart III: Data and policy\n4 weeks\nOct 16 - Nov 10\n\n\nPart IV: Data in practice\n3 weeks\nNov 13 - Dec 1\n\n\nCourse wrap-up\n2 weeks\nDec 4 - Dec 15\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nHomework (hw). Regular practice problems [25 points]\nMini-projects (labs and code). In Quarto Markdown (.qmd) [20 points]\nPapers. Four papers (08/31, 10/13, 11/10, 12/1 12/8) [20 points]\nExams. Midterm exam and a final exam [35 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW-0 and Lab-0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks."
  },
  {
    "objectID": "course-overview.html#footnotes",
    "href": "course-overview.html#footnotes",
    "title": "Course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that these are rough estimates↩︎"
  },
  {
    "objectID": "week8.html#part-i-context",
    "href": "week8.html#part-i-context",
    "title": "DATA 202 - Week 8",
    "section": "Part I: Context",
    "text": "Part I: Context\n\n\nThe great masses of people are determined to end the exploitation of their races and land. They are awake and moving toward their goal like a tidal wave. You can hear them rumbling in every village street, on the docks, in the houses, among the students, in the churches, and at political meetings. - Martin Luther King, Jr., Nobel Lecture, December 1964\n\n\n\n\nIllustration by Yuko Shimizu from cigionline.org.\n\n\n\nHistory, cause, and effect\nTheory is often understood through an investigation of both the literature and the historical contexts surrounding a given issue. Take, for example, the legacies of colonization. As data and scientific developments expand into new fields, new questions arise as to what we can associate and how we understand these associations. For example, how does one theoretically relate a series of events from 150 years ago to a present set of conditions? What are the possibilities? What are the limitations? What are the cautionary tales?\nStatistics was situated as a vehicle to understand and measure observations. As it becomes an ever popular vehicle to inform social justice discourses, the many real-world associations identified in society bring to mind the well-known and readily available cautionary tale:\n\nCorrelation does not imply causation.\n\nThis popular statement relates to the ability (or inability) to deduce cause-and-effect relationships.\n\n\n\nCorrelation\nIn the traditional setting, correlation is framed as a statistical measure that relays the size and direction of the relationship between two or more variables. From a more critical framework, however, the idea of associations between variables and the attribution of relationships between variables must be both examined and interrogated.\nAssociations should be understood and initiated from the root of a theoretical framework.\nIf two variables are related, in a statistical context, we assume that their values change in some ordered fashion. These measures are often represented on an axis depicting the strength of the association.\n\n\n\nImage from statlect.com\n\n\nAs one value of a first variable increases the other variable’s values may also increase (positive relationship). Alternatively, as the value of one variable increases the other variable’s values may decrease (negative relationship). Key questions to consider focus on how context and other factors shape any seeming bivariate relationship.\n\n\nSpurious correlation\nAt the intersection of mathematics and statistics is the concept of the spurious correlation (Ward, 2013). A few examples:\n\nDivorce rate has been found to correlate with margarine use.\nIce cream sales have been correlated with:\n\nSwimming deaths\nShark attacks\nCrime rates\n\nThe use of the popular site Facebook has been correlated with diminished well-being.\n\nThese association help us see the need to investigate relationships further. What are possible unexplained relationships in these associations? We identify potential spurious variables when developing theoretical ideas about associations.\n\n\n\n\nCausation\nCausality (See Sloman and Acnado (2015)) is a representation and principle of cause and effect. There are many different viewpoints of causation across disciplines and fields of study. Terminology matters in how one frames cause and effect.\nThere are four possible relationships between two variables:\n\nX causes Y\nY causes X\nX and Y are both caused by Z\nX is not related to Y\n\nWhile these relationships may exist in some combination, it is important to frame their differences.\n\n\n\nAssociating X and Y\nOur initial framing might directly link the variables X and Y.\nFirst, we have the association X causes Y.\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\nNext, we have the association Y causes X.\n\n\n\n\ngraph LR\n  A[Y] --&gt; B[X]\n\n\n\n\n\nFinally, we have the association X causes Y and Y causes X.\n\n\n\n\ngraph LR\n  A[X] &lt;--&gt; B[Y]\n  B --&gt; A\n\n\n\n\n\n\n\n\nFraming spurious relationships\nHowever, after further reading, we may find that we need to integrate a third factor Z.\nIn this context, a third factor (the “third-cause fallacy”) represents a spurious relationship.\n\n\n\n\ngraph LR\n  A[X]\n  B[Y]\n  C[Z]\n\n\n\n\n\nSpurious relationships denote an observed or hypothesized association.\nThere are important differences in how we depict and deal with third factor associations.\n\n\n\nConfounding variables\nConfounding is a causal concept that focuses on spurious or distorted associations.\nBelow, Z is considered a confounding variable.\n\n\n\n\ngraph TD\n  A[Z] --&gt; B[X]\n  A --&gt; C[Y]\n\n\n\n\n\n\nZ is related to both the independent, \\(X\\), and dependent, \\(Y\\), variables in the causal sense.\nYour analysis should examine if the relationship between \\(X\\) and \\(Y\\) holds.\nThese interactions are described as spurious associations.\n\n\n\n\nMediating variables\nA mediating variable explains the process by which two variables are related.\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  B --&gt; C[Y]\n\n\n\n\n\n\nZ provides the mechanism to relate the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\n\n\n\nModerating variables\nA moderating variable explains the strength by which two variables are related.\n\n\n\n\ngraph LR\n  A[X] --&gt; B[Z]\n  A --&gt; C[Y]\n  B --&gt; C\n\n\n\n\n\n\nZ explains the strength of association between the independent, \\(X\\), and dependent, \\(Y\\), variables.\n\n\n\n\nReal-world associations\n\n\n\nImage from ACLU.org\n\n\nLet us look at a more concrete example. Imagine you read or hear the statement:\n\n“In the U.S., Black drivers are more likely to be pulled over by police than white drivers.”\n\nThis statement represents a complex relationship that requires history and context to inform analysis.\n\nRacial profiling has been an issue in the U.S. for centuries.\nU.S. police are deployed at higher rates in some poor and Black neighborhoods.\n\nWhat data and evidence do you think is needed to confirm if the statement is accurate? Moreover, what measures and statistical tests would allow us to integrate history and context? What variable associations might be used to assess these relationship?"
  },
  {
    "objectID": "week8.html#part-ii-content",
    "href": "week8.html#part-ii-content",
    "title": "DATA 202 - Week 8",
    "section": "Part II: Content",
    "text": "Part II: Content\n\n\nA guiding example\nLet us return to an example mentioned last week: the yahoo_data in the critstats package. This data comes from a Yahoo! News Race and Justice poll.\n\n\n\nImage from Mass Humanities\n\n\nA complete description of the data set is as follows:\n\nResults from a Yahoo! News poll conducted by YouGov on May 29-31, 2020. In total 1060 U.S. adults were asked a series of questions regarding race and justice in the wake of the killing of George Floyd by a police officer. Results in this data set are percentages for the question, “Do you think Blacks and Whites receive equal treatment from the police?” For this particular question there were 1059 respondents.\n\n\n\n\nA guiding example\nLet us begin with a summary of responses to a survey sparked by the killing of George Floyd.\n\nSummary of survey response data1\n\n\nResponse\nTotal\n\n\n\n\nNo\n700\n\n\nYes\n236\n\n\n\nThe study sample \\(n = 936\\). Please take note of the table’s footnote.\nAs of now, we know very little about the survey (i.e., the context has been stripped away).\n\nAn initial question\nIs there a difference in the survey responses?\n\n\n\nFraming proportions\nAs you learn about hypothesis testing, you want to understand the nature of variable associations. There are many approaches to how we measure associations. One we’ll explore extensively is differences in proportions of two dichotomous variables.\nFirst, we add a third column to our table by computing the proportion in each row:\n\\[ P(No) = \\frac{\\text{Number of respondents who replied No}}{n} \\]\n\\[ P(Yes) = \\frac{\\text{Number of respondents who replied Yes}}{n} \\]\n\n\n\nUnderstanding proportions\n\nSummary of survey response data with proportion column\n\n\nResponse\nTotal\nProportion\n\n\n\n\nNo\n700\n\\(0.7478\\)\n\n\nYes\n236\n\\(0.2522\\)\n\n\n\nSo we have \\(P(No) = 0.7478\\) and \\(P(Yes) = 0.2522\\).\nThis is a sample. Be mindful of the notes about sampling and sampling error.\nWhat can these proportions tell us about the data?\n\n\n\n\nA closer look\nLet’s look at a preview panel of the full data set.\n\nPreview of full survey response data set\n\n\nID\nRace\nResponse\n\n\n\n\n1\nWhite\nYes\n\n\n2\nWhite\nYes\n\n\n.\n.\n.\n\n\n773\nBlack\nYes\n\n\n774\nBlack\nYes\n\n\n..\n..\n..\n\n\n874\nHispanic\nYes\n\n\n875\nHispanic\nYes\n\n\n…\n…\n…\n\n\n\\(1059\\)\nOther\nNot sure\n\n\n\n\nWhen we format the data into tables, there are multiple categories to consider.\nIn the table below, the race variable is listed in rows.\n\n\nTable 1: Race in rows\n\n\nRace\nNo\nYes\n\n\n\n\nBlack\nBlack \\(\\cap\\) No\nBlack \\(\\cap\\) Yes\n\n\nHispanic\nHispanic \\(\\cap\\) No\nHispanic \\(\\cap\\) Yes\n\n\nOther\nOther \\(\\cap\\) No\nOther \\(\\cap\\) Yes\n\n\nWhite\nWhite \\(\\cap\\) No\nWhite \\(\\cap\\) Yes\n\n\n\n\nRecall that the \\(\\cap\\) represents “and” while the \\(\\cup\\) represents “or” in logic.\n\nIn the table below, the response variable is listed in rows.\n\n\nTable 2: Response in rows\n\n\n\n\n\n\n\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\n\n\n\n\nNo\nNo \\(\\cap\\) Black\nNo \\(\\cap\\) Hispanic\nNo \\(\\cap\\) Other\nNo \\(\\cap\\) White\n\n\nYes\nYes \\(\\cap\\) Black\nYes \\(\\cap\\) Hispanic\nYes \\(\\cap\\) Other\nYes \\(\\cap\\) White\n\n\n\n\n\nWe use our understanding of probability to generate a reconstruction for the response ‘No’.\n\\[ P(No) = \\dfrac{\\text{No} \\cap \\text{Black}}{n} + \\dfrac{\\text{No} \\cap \\text{Hispanic}}{n} + \\dfrac{\\text{No} \\cap \\text{Other}}{n} + \\dfrac{\\text{No} \\cap \\text{White}}{n} \\]\nWe generate a parallel construction for the response ‘Yes’.\n\\[ P(Yes) = \\dfrac{\\text{Yes} \\cap \\text{Black}}{n} + \\dfrac{\\text{Yes} \\cap \\text{Hispanic}}{n} + \\dfrac{\\text{Yes} \\cap \\text{Other}}{n} + \\dfrac{\\text{Yes} \\cap \\text{White}}{n} \\]\n\n\nSummary values\nAdding values to the table will give us a more complete picture of the data.\n\nResponses to a survey question by race\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\n\n\n\n\nNo\n92\n75\n47\n486\n\n\nYes\n6\n15\n14\n201\n\n\n\n\nUse the table to calculate Response probabilities:\n\nResponses to a survey question by race\n\n\nResponse\nBlack\nHispanic\nOther\nWhite\nTotal\n\n\n\n\nNo\n92\n75\n47\n486\n700\n\n\nYes\n6\n15\n14\n201\n236\n\n\n\nGather response probabilities for No by dividing each cell by the sample size.\n\\[ P(No) = \\dfrac{92}{936} + \\dfrac{75}{936} + \\dfrac{47}{936} + \\dfrac{486}{936} = \\dfrac{700}{936} = 0.7478 \\] Gather response probabilities for Yes by dividing each cell by the sample size.\n\\[ P(Yes) = \\dfrac{6}{936} + \\dfrac{15}{936} + \\dfrac{14}{936} + \\dfrac{201}{936} = \\dfrac{236}{936} = 0.2522 \\] We will use this framing to compute proportions for each cell but in R.\n\nGiven that we want to explore the data in more detail and understand any base associations, we need to revisit our framing and consider some additional tasks.\n\n\n\nReturning to the framework\nA basic analysis goes a long way in determining our next steps.\nWith this base analytic exercises, you may want to ask: What is the research question?\nWhat are the main or primary theoretical constructions?\n\nFirst, examine the literature and annotate specific citations.\nNext, construct a framework for the various interpretations.\n\nWhat are the similarities across theoretical constructions?\nWhat are the main differences across theoretical constructions?\nHow does a nuanced view of the history of the issue improve our understanding?\n\n\nBased on your background analyses, what is the hypothesis?"
  },
  {
    "objectID": "week8.html#part-iii-code",
    "href": "week8.html#part-iii-code",
    "title": "DATA 202 - Week 8",
    "section": "Part III: Code",
    "text": "Part III: Code\n\n\nSetting things up\nAs a general rule, always load the appropriate libraries at the start of your analyses.\n\n# load our libraries\nlibrary(critstats)\nlibrary(tidyverse)\nlibrary(descr)\nlibrary(Hmisc)\n\nIn this case:\n\ncritstats contains the yahoo_data that we’d like to analyze.\ntidyverse contains the set of functions to help us work with our data\ndescr and Hmisc contain useful functions to analyze social science data.\n\nIf you receive an error, you may need to use the install.packages() commands first.\n\n\n\nLoad the data\nInspect the data documentation and contents of the data.\n\n# access the yahoo_data\n??yahoo_data\nyahoo_data # \"Do you think Blacks and Whites receive equal treatment from the police?\"\n\nWhile the specific wording of the question may not fit our theoretical framing (e.g. Blacks and Whites), it is useful to consider what information and insights can be gathered from the data.\nHere, theory and the literature become important components to outlining your analytic plan.\n\n\n\nClean your data\nThe data in the yahoo_data set is already cleaned and prepped. However, when you conduct your own analyses, you will need to follow the series of steps outlined in previous assignments. For example, Lab 1 is a great resource on cleaning.\n\n\n\nUnderstand your data\nGiven that we are analyzing categorical data, we will create a series of tables.\nThese tables will allow us to gather a sense of the data.\nOur first table will be of the variable race_eth.\n\n# create a table of the race_eth\ntable(yahoo_data$race_eth)\n\n\n   Black Hispanic    Other    White \n     101      104       82      772 \n\n\nWhat do you notice? What do you wonder?\n\n\n\nUnderstand your data\nOur second table will be of the variable response.\n\n# create a table of the race_eth\ntable(yahoo_data$response)\n\n\n      No Not sure      Yes \n     700      123      236 \n\n\nWhat do you notice? What do you wonder?\n\n\n\nModify your data\nMake the necessary modifications to your data.\nHere I set the yahoo_data to df and rename the race_eth variable.\n\ndf &lt;- yahoo_data # set the yahoo_data to df\ndf &lt;- df %&gt;% \n  rename(\"race\" = \"race_eth\") # rename the race_eth variable to race\n\n\n\n\nGet the sample size\nWhen looking at the table, it may be useful to get a sense of the proportions.\nWe’ll begin with getting the sample size.\n\n# get a quick count of the sample size\ncount(df) # the count function can collect the sample size, n, of a tibble\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1059\n\n\nThe tables give us the frequency of each category.\nThe sample size will help us get the relative frequency of each category.\n\n\n\nRelative frequencies by race\nGet the relative frequencies for the race variable.\n\ndf %&gt;% \n  count(race) %&gt;% \n  mutate(prop = prop.table(n))\n\n# A tibble: 4 × 3\n  race         n   prop\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 Black      101 0.0954\n2 Hispanic   104 0.0982\n3 Other       82 0.0774\n4 White      772 0.729 \n\n\n\n\n\nRelative frequencies by response\nGet the relative frequencies for the response variable.\n\ndf %&gt;% \n  count(response) %&gt;% \n  mutate(prop = prop.table(n))\n\n# A tibble: 3 × 3\n  response     n  prop\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 No         700 0.661\n2 Not sure   123 0.116\n3 Yes        236 0.223\n\n\n\n\n\nGenerate a crosstab of race and response\n\ncrosstab(df$response, df$race, plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n-------------------------------------------------------\nNot sure           3         14      21      85     123\n-------------------------------------------------------\nYes                6         15      14     201     236\n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\nWhat do you notice? What do you wonder?\nSet plot=T if you want to see a visual plot of the data.\n\n\n\nStandardize the frequencies\nRaw frequencies can be hard to read in a crosstab, especially since column totals are not equal.\nAdd prop.c = T to get column percentages\n\ncrosstab(df$response, df$race, plot=F, prop.c=T)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|          Column Percent | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo               92         75      47     486     700 \n               91.1%      72.1%   57.3%   63.0%        \n-------------------------------------------------------\nNot sure          3         14      21      85     123 \n                3.0%      13.5%   25.6%   11.0%        \n-------------------------------------------------------\nYes               6         15      14     201     236 \n                5.9%      14.4%   17.1%   26.0%        \n-------------------------------------------------------\nTotal           101        104      82     772    1059 \n                9.5%       9.8%    7.7%   72.9%        \n=======================================================\n\n\nWe also refer to crosstabs as contingency tables; the percentages provide conditional probabilities.\nWhat do you notice? What do you wonder?\n\n\n\nHypothesis testing with crosstab\nDevelop (or refine) your research question(s).\nExamining the relationship between two variables.\n\n# research inquiry: is there a relationship between the race and response variables?\n# data: survey results from the Yahoo! News race and justice poll\n# note(s): response contains three levels: Yes, No, and Not sure\n\nAdd your research inquiry/question to your preamble.\nNotes help you remember import information for writing code. You can also put them in-line.\n\n\n\nNull and alternative hypothesis for a \\(\\chi^2\\) test are:\n\n\\(H_0\\): No relationship. The two variables are statistically independent.\n\\(H_1\\): There is a relationship. The two variables are not statistically independent.\n\nNotice that \\(H_1\\) does not give us information about the direction or strength of the relationships. To test the null hypothesis, we will calculate the \\(\\chi^2\\) statistic:\n\\[\\chi^2 = \\sum \\dfrac{(O-E)^2}{E}\\]\nWhere\n\n\\(O\\) is the observed frequency for each cell\n\\(E\\) is the expected frequency for each cell\n\nThe expected frequency is what we’d expected if there is no relationship (\\(H_0\\) is true)\n\n\n\n\nCreate a crosstab with raw frequencies\n\ncrosstab(df$response, df$race, plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n-------------------------------------------------------\nNot sure           3         14      21      85     123\n-------------------------------------------------------\nYes                6         15      14     201     236\n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\n\n\n\nSampling error\nAn important note about sampling error\nThere appears to be a relationship between the variable response and race.\nHowever, the data are from a sample. We do not and should not infer that there is a relationship at the population level, but only for the sample data we are analyzing.\nEarlier, we saw the difference in proportions by the column differences for “Yes” and “No”. Since we found that the overall population said “No” to the question of fair treatment, we want to take this into consideration when analyzing our data.\n\n\n\nGather the expected frequencies\n\ncrosstab(df$response, df$race,\n         expected=T, #Add expected frequency to each cell\n         plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n|-------------------------|\n\n=======================================================\n               df$race\ndf$response    Black   Hispanic   Other   White   Total\n-------------------------------------------------------\nNo                92         75      47     486     700\n                66.8       68.7    54.2   510.3        \n-------------------------------------------------------\nNot sure           3         14      21      85     123\n                11.7       12.1     9.5    89.7        \n-------------------------------------------------------\nYes                6         15      14     201     236\n                22.5       23.2    18.3   172.0        \n-------------------------------------------------------\nTotal            101        104      82     772    1059\n=======================================================\n\n\n\n\n\nAdd the chi-square contributions\n\ncrosstab(df$response, df$race,\n         expected=T, #Add expected frequency to each cell\n         prop.chisq = T, #Total contribution of each cell \n         plot=F)\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n| Chi-square contribution | \n|-------------------------|\n\n=========================================================\n               df$race\ndf$response     Black   Hispanic    Other   White   Total\n---------------------------------------------------------\nNo                 92         75       47     486     700\n                 66.8       68.7     54.2   510.3        \n                9.542      0.569    0.957   1.156        \n---------------------------------------------------------\nNot sure            3         14       21      85     123\n                 11.7       12.1      9.5    89.7        \n                6.498      0.305   13.828   0.243        \n---------------------------------------------------------\nYes                 6         15       14     201     236\n                 22.5       23.2     18.3   172.0        \n               12.107      2.885    1.000   4.874        \n---------------------------------------------------------\nTotal             101        104       82     772    1059\n=========================================================\n\n\n\n\n\nCalculate degrees of freedom\nWe have \\[df_{\\chi^2} = (r-1)(c-1)\\]\nwhere r = number of rows in the table and c = number of columns in the table\n\nChi-square critical values\n\n\n\nChi-square critical values\n\n\n\n\n\nGather critical value of chi-square\n\nqchisq(.05, 6, lower.tail=F)\n\n[1] 12.59159\n\n\n\n\n\nGet the chi-square statistic from R\n\n## get chi-square statistic\nchisq.test(df$response, df$race)\n\n\n    Pearson's Chi-squared test\n\ndata:  df$response and df$race\nX-squared = 53.964, df = 6, p-value = 7.5e-10\n\n\n\nOur focus should be on interpreting the output values.\nIn the output, you want to examine the following:\n\nConfirm \\(\\chi^2\\) value and degrees of freedom.\nNotice more precise information about the p-value\n\nUse this value to make sense of your research inquiry\n\nDoes this p-value make sense statistically and substantively?\n\n\n\n\ndescribe some initial limitations of analysis\nAlways close out your analyses with a write up of limitations.\n\n### limitation 1: sampling error\n\n### limitation 2: category reductions\n\n### limitation 3: cases dropped\n\n### limitation 4: chi-square test\n\n\n\n\n\nNext up: Week 9"
  },
  {
    "objectID": "week8.html#footnotes",
    "href": "week8.html#footnotes",
    "title": "DATA 202 - Week 8",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRespondents who selected ‘Not sure’ on the survey were moved from this analysis.↩︎"
  },
  {
    "objectID": "week8-slides.html#part-i-context",
    "href": "week8-slides.html#part-i-context",
    "title": "DATA 202 - Week 8",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "week8-slides.html#part-ii-content",
    "href": "week8-slides.html#part-ii-content",
    "title": "DATA 202 - Week 8",
    "section": "Part II: Content",
    "text": "Part II: Content"
  },
  {
    "objectID": "week8-slides.html#part-iii-code",
    "href": "week8-slides.html#part-iii-code",
    "title": "DATA 202 - Week 8",
    "section": "Part III: Code",
    "text": "Part III: Code"
  },
  {
    "objectID": "papers/paper2.html",
    "href": "papers/paper2.html",
    "title": "Paper 2",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:"
  },
  {
    "objectID": "papers/paper2.html#path-diagram",
    "href": "papers/paper2.html#path-diagram",
    "title": "Paper 2",
    "section": "Path Diagram",
    "text": "Path Diagram\n\n\n\n\nflowchart LR\n  A[Income] --&gt; B[Strength of support for a political party]"
  },
  {
    "objectID": "papers/paper2.html#prepare-files",
    "href": "papers/paper2.html#prepare-files",
    "title": "Paper 2",
    "section": "Prepare files",
    "text": "Prepare files\nTo investigate the relationship outlined in the path diagram, we will use data from the General Social Survey (GSS). To begin your exploratory analysis, start a new RScript in your stats-pt2 RStudio project directory and give your RScript a proper preamble."
  },
  {
    "objectID": "papers/paper2.html#data-set",
    "href": "papers/paper2.html#data-set",
    "title": "Paper 2",
    "section": "Data set",
    "text": "Data set\nFor paper #2, you will use the gss_cat data located in the forcats package.\nTo test our hypothesis, two survey variables/measures are selected from the gss_cat data:\n\nrincome (respondent’s reported income)\npartyid (respondent’s levels of support for one of three major US political parties)\n\n\nLoad the libraries\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidycensus)\n\n\n\nView data in your current R session\n\ndata()\n\nLocate the gss_cat data in the forcats pacakge.\nThis is a data set for us to examine the use of categorical data and factor variable types.\n\n\nView documentation for your data\n\n?gss_cat\n\nView your data\n\ngss_cat\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Ind,near … Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Not str r… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Ind,near … Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Not str d… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Strong de… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Not str r… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Ind,near … Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Not str d… Prot… Other       0\n10  2000 Married          47 White $25000 or more Strong re… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nBe sure to check the categories and codes for your variables.\n\nCategories for rincome\n\nsummary(gss_cat$rincome)\n\n     No answer     Don't know        Refused $25000 or more $20000 - 24999 \n           183            267            975           7363           1283 \n$15000 - 19999 $10000 - 14999  $8000 to 9999  $7000 to 7999  $6000 to 6999 \n          1048           1168            340            188            215 \n $5000 to 5999  $4000 to 4999  $3000 to 3999  $1000 to 2999       Lt $1000 \n           227            226            276            395            286 \nNot applicable \n          7043 \n\n\n\n\nCategories for partyid\n\nsummary(gss_cat$partyid)\n\n         No answer         Don't know        Other party  Strong republican \n               154                  1                393               2314 \nNot str republican       Ind,near rep        Independent       Ind,near dem \n              3032               1791               4119               2499 \n  Not str democrat    Strong democrat \n              3690               3490 \n\n\nNotice how each variable is measured and take notes for the measurement section of your paper.\nYou will need to re-code these values prior to any statistical analysis."
  },
  {
    "objectID": "papers/paper2.html#prepare-data-for-analysis",
    "href": "papers/paper2.html#prepare-data-for-analysis",
    "title": "Paper 2",
    "section": "Prepare data for analysis",
    "text": "Prepare data for analysis\nBegin exploratory analyses on each variable.\nWhile doing exploratory analyses, you should prepare your data for statistical analysis.\n\nInspect your data using str()\nCheck the distribution of your variables using count()\nDecide how you will work with missing data, such as na.omit()\n\n\nRemove missing values from your analysis\nFor this paper, it is fine to drop all missing observations.\nWe will also explore the data from the year 2000.\n\ndf &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  filter(year == 2000) %&gt;% \n  select(year, rincome, partyid)\n\nExamine the reduced data frame.\n\ndf\n\n# A tibble: 1,824 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 Not applicable Independent       \n 3  2000 Not applicable Ind,near rep      \n 4  2000 Not applicable Not str democrat  \n 5  2000 $25000 or more Not str republican\n 6  2000 $25000 or more Not str democrat  \n 7  2000 $25000 or more Strong republican \n 8  2000 $25000 or more Not str democrat  \n 9  2000 $25000 or more Strong democrat   \n10  2000 $25000 or more Ind,near dem      \n# ℹ 1,814 more rows\n\n\nView the top and bottom of your data using head and tail, respectively.\n\nhead(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid           \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n1  2000 $8000 to 9999  Ind,near rep      \n2  2000 Not applicable Independent       \n3  2000 Not applicable Ind,near rep      \n4  2000 Not applicable Not str democrat  \n5  2000 $25000 or more Not str republican\n6  2000 $25000 or more Not str democrat  \n\ntail(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid          \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;            \n1  2000 $25000 or more Not str democrat \n2  2000 $25000 or more Strong republican\n3  2000 $10000 - 14999 Independent      \n4  2000 $25000 or more Strong republican\n5  2000 $25000 or more Ind,near rep     \n6  2000 $7000 to 7999  Strong republican\n\n\nIn paper 2, you will conduct original statistical analyses.\n\n\nGet frequency tables for your data\nUsing the count() function, we will get frequency tables for each variable.\n\ndf %&gt;% \n  count(rincome)\n\n# A tibble: 16 × 2\n   rincome            n\n   &lt;fct&gt;          &lt;int&gt;\n 1 No answer         15\n 2 Don't know        22\n 3 Refused           92\n 4 $25000 or more   590\n 5 $20000 - 24999   126\n 6 $15000 - 19999   115\n 7 $10000 - 14999   139\n 8 $8000 to 9999     45\n 9 $7000 to 7999     18\n10 $6000 to 6999     24\n11 $5000 to 5999     17\n12 $4000 to 4999     20\n13 $3000 to 3999     22\n14 $1000 to 2999     36\n15 Lt $1000          24\n16 Not applicable   519\n\n\n\ndf %&gt;% \n  count(partyid)\n\n# A tibble: 9 × 2\n  partyid                n\n  &lt;fct&gt;              &lt;int&gt;\n1 No answer              3\n2 Other party           22\n3 Strong republican    180\n4 Not str republican   244\n5 Ind,near rep         168\n6 Independent          392\n7 Ind,near dem         213\n8 Not str democrat     331\n9 Strong democrat      271\n\n\nFrom our frequency tables, it is clear that we will need to transform our data."
  },
  {
    "objectID": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "href": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "title": "Paper 2",
    "section": "Recode categories into two-levels (dichotomous)",
    "text": "Recode categories into two-levels (dichotomous)\nFor this analysis, we will transform our data into two dichotomous variables. Let’s examine the outputs before overwriting our data frame.\nWe will use the logic operator != to imply we do not want to keep these values (i.e., we will filter the values that are not equal to the right hand side).\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\")\n\n# A tibble: 1,198 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 $25000 or more Not str republican\n 3  2000 $25000 or more Not str democrat  \n 4  2000 $25000 or more Strong republican \n 5  2000 $25000 or more Not str democrat  \n 6  2000 $25000 or more Strong democrat   \n 7  2000 $25000 or more Ind,near dem      \n 8  2000 $25000 or more Strong democrat   \n 9  2000 $25000 or more Independent       \n10  2000 $10000 - 14999 Not str democrat  \n# ℹ 1,188 more rows\n\n\nWe now recode the categories using mutuate() and recode().\n\ndf %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 10000\" = \"$25000 or more\",\n          \"More than 10000\" = \"$20000 to 24999\",\n          \"More than 10000\" = \"$15000 to 19999\",\n          \"More than 10000\" = \"$10000 to 14999\",\n          \"Less than 10000\" = \"$8000 to 9999\",\n          \"Less than 10000\" = \"$7000 to 7999\",\n          \"Less than 10000\" = \"$6000 to 6999\",\n          \"Less than 10000\" = \"$5000 to 5999\",\n          \"Less than 10000\" = \"$4000 to 4999\",\n          \"Less than 10000\" = \"$3000 to 3999\",\n          \"Less than 10000\" = \"$1000 to 2999\",\n          \"Less than 10000\" = \"$Lt $1000\"))\n\n# A tibble: 1,824 × 3\n    year rincome         partyid           \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;             \n 1  2000 Less than 10000 Ind,near rep      \n 2  2000 Not applicable  Independent       \n 3  2000 Not applicable  Ind,near rep      \n 4  2000 Not applicable  Not str democrat  \n 5  2000 More than 10000 Not str republican\n 6  2000 More than 10000 Not str democrat  \n 7  2000 More than 10000 Strong republican \n 8  2000 More than 10000 Not str democrat  \n 9  2000 More than 10000 Strong democrat   \n10  2000 More than 10000 Ind,near dem      \n# ℹ 1,814 more rows\n\n\n\ndf %&gt;% \n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,824 × 3\n    year rincome        partyid    \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;      \n 1  2000 $8000 to 9999  Republican \n 2  2000 Not applicable Independent\n 3  2000 Not applicable Republican \n 4  2000 Not applicable Democrat   \n 5  2000 $25000 or more Republican \n 6  2000 $25000 or more Democrat   \n 7  2000 $25000 or more Republican \n 8  2000 $25000 or more Democrat   \n 9  2000 $25000 or more Democrat   \n10  2000 $25000 or more Democrat   \n# ℹ 1,814 more rows\n\n\nWe can stack our variable transformations together into one chunk of code.\nTake note of the way we are creating our new data set for analysis.\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\") %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 20000\" = \"$25000 or more\",\n          \"More than 20000\" = \"$20000 - 24999\",\n          \"Less than 20000\" = \"$15000 - 19999\",\n          \"Less than 20000\" = \"$10000 - 14999\",\n          \"Less than 20000\" = \"$8000 to 9999\",\n          \"Less than 20000\" = \"$7000 to 7999\",\n          \"Less than 20000\" = \"$6000 to 6999\",\n          \"Less than 20000\" = \"$5000 to 5999\",\n          \"Less than 20000\" = \"$4000 to 4999\",\n          \"Less than 20000\" = \"$3000 to 3999\",\n          \"Less than 20000\" = \"$1000 to 2999\",\n          \"Less than 20000\" = \"Lt $1000\")) %&gt;%\n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,198 × 3\n    year rincome         partyid    \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;      \n 1  2000 Less than 20000 Republican \n 2  2000 More than 20000 Republican \n 3  2000 More than 20000 Democrat   \n 4  2000 More than 20000 Republican \n 5  2000 More than 20000 Democrat   \n 6  2000 More than 20000 Democrat   \n 7  2000 More than 20000 Democrat   \n 8  2000 More than 20000 Democrat   \n 9  2000 More than 20000 Independent\n10  2000 Less than 20000 Democrat   \n# ℹ 1,188 more rows\n\n\nAs you examine the code more closely, you will notice that I created two categories:\n\nPeople making less than $20,000\nPeople making more than $20,000"
  },
  {
    "objectID": "papers/paper4.html",
    "href": "papers/paper4.html",
    "title": "Paper 4",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:\n\nInstructions for papers\nMore information for course papers\n\n\nStep 1: Theory and logic diagram\nFor paper #4 (similar to paper 3), we are interested in exploring theories pertaining to the relationships and interactions between multiple variables. For this paper, you should pay particular attention to the theory underlying your identified relationships and conduct a focused analysis as it relates to each variable. For example, given three variables: X, Y, and Z, your paper should be sure to take care in discussing and analyzing the relationships between X and Y, X and Z, and Y and Z. Please note: you should not use the same variables from a previous assignment but you may use the same data set.\nNote: For your paper, please remember to address the theoretical relationship that the literature has identified between the conceptual variables selected. That is, before describing the measures and the survey you will be using, explain your theory and why you think that your independent variable is having an effect on your dependent variable.\n\n\nStep 2: Variables, Measurement, Hypothesis\nAll variables and analyses should be described in detail.\n\n\nStep 3: Statistical Analysis\nPlease be sure to utilize analyses on the interactions between multiple variables.\n\n\nStep 4: Conclusion\nFor your final paper, please be sure to focus on a solid conclusion based on your statistical analysis."
  },
  {
    "objectID": "hw/hw4.html",
    "href": "hw/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Please submit Homework 4 responses as a .pdf file on Canvas here.\n\nExercise 1.1"
  },
  {
    "objectID": "hw/hw2.html",
    "href": "hw/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please submit Homework 2 responses as a .pdf file on Canvas here.\n\nExercise 1.1\nIn the following questions, read each scenario and (a) describe the sampling method used and (b) determine whether the sampling method appears to be sound or flawed. Explain your reasoning in complete sentences.\n\n(Study hours): A group of students decides to collect data on the number of hours students in a university spend in the library per week. The researchers collect data by setting up a table outside of the library entrance.\n(Clinical trials): Researchers at a lab conduct a wide variety of clinical trials by using subjects who volunteer after reading advertisements hung on boards and light poles soliciting paid volunteers to participate in the study.\n(Covid-19): In an online survey with a sample of 957 subjects, the following question was posed on both Instagram and Twitter: “In your view, is the Covid-19 vaccine safe?” The survey respondents were internet users who chose to respond to the question posted on the social media accounts over the course of 24 hours.\n(Community data): A group of researchers decides to partner with a major company to examine environmental hazards at the neighborhood level. They code each region by zip code and randomly select zip codes in the metropolitan region. The researchers then plan to randomly select households within each of the selected zip codes.\n(Debt): In a survey of hospital workers, a total of 2,087 respondents were randomly selected and asked how much credit card debt they pay off each month. Survey results were used to generate population parameters.\n\n\n\nExercise 1.2\nIn the following exercises, explain the issue with the study and sampling method. Use complete sentences.\n\n(Political party): In a research study conducted by a political party at their annual rally, a convenience sample of 1000 adults were asked to select their favorite political party, the favorite choice was the political party in question, which was selected by 92% of respondents.\n(Marijuana): Proponents of the legalization of marijuana in their state collected data using an electronic poll in various CBD and vape shops across the city, showing that 65% of those surveyed said that they “strongly agree” and 15% said that they “agree” with the legalization of marijuana.\n(Police Training Facility): A group study citizen beliefs about the “Cop City” facility in Atlanta collected data from 367 individuals at four city council meetings. In a report on their findings, in which they describe the use of advanced statistical methods, they state: “a majority of Atlanta citizens support the development of the training facility.”\n\n\n\nExercise 1.3\n(Discrete vs. continuous data): Identify which of the following is discrete vs continuous.\n\nThe number of people surveyed in a national election poll\nThe exact height of a random sample of students in a statistics course\nThe exact times that drives spend texting while driving over a 7 day period.\nThe number of animals observed in a reserve on a given day.\nThe temperature recorded by the National Weather Service.\n\n\n\nExercise 1.4\n(Levels of measurement): Identify the level of measurement for the variables below.\n\nCollege rankings in the U.S. News and World Report.\nExit poll results for a presidential election where respondents were asked to identify political affiliation.\nThe colors of shirts (e.g., red, green, blue, etc.) worn by a group of students listed in a data set.\nThe amount of a virus in a sample of blood collected in a medical study.\n\n\n\nExercise 1.5\nIn your own words, define the term “social justice” and describe how statistics can be used to support and advocate for social movement building around issues of injustice.\n\nFor Exercises 1.6 through 1.10, you should complete all calculations in R.\nLet P be a sample of payments (in thousands of dollars) for residents of a small rural community. These payments represent a random sample of 140 payments being made through the local city council. Payments are restitution for an environmental hazard. The hazard was the result of a major corporation’s new factory construction, which forced many of the town’s residents to relocate.\n\\[ P = \\{25.4, 27.6, 19.7, 18.1, 18.7, 65.6, 20.0, 21.7, 39.6, 17.2, 34.5, 32.7, 92.7, 12.3\\} \\]\n\n\nExercise 1.6\nCalculate the measures of center for \\(P\\).\n\n\nExercise 1.7\nCalculate the measures of variation for \\(P\\).\n\n\nExercise 1.8\nCalculate the IQR for \\(P\\) and the z-scores for the minimum and maximum values.\n\n\nExercise 1.9\nWhat do the descriptive statistics (measures of center, measures of variation, and measures of relative standing) on the sample data in \\(P\\) tell us about the payments?\n\n\nExercise 1.10\nGenerate a descriptive and appropriate plot for the data in the set \\(P\\), include labels."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "This page contains three sections:\n\nBrief overview of all course assignments.\nInformation about homework 0 (ungraded).\nInformation about lab 0 (ungraded).\n\n\nOverview of course assignments\nOur course assignments will be split between reading and typing (code and analytic results).\n\n\n\n\n\n\n\n\nCourse component\nAssignment\nDate(s)\n\n\n\n\nPart I: Statistics and society\n\n\n\n\n\nHomework 0\nDue: Thu Aug 24\n\n\n\nLab 0\nDue: Tue Aug 29\n\n\n\nPaper #1\nDue: Thu Aug 31\n\n\nPart II: Data and people\n\n\n\n\n\nHomework 1\nDue: Thu Sept 21  Sun Sept 24 \n\n\n\nLab 1\nDue: Thu Sept 21  Sun Oct 1 \n\n\n\nAnnotated Bibliography (update)\nDue: Fri Sept 29  Fri Oct 6 \n\n\n\nPaper #2\nDue: Fri Oct 13\n\n\nPart III: Data and policy\n\n\n\n\n\nHomework 2\nDue: Thu Oct 19\n\n\n\nLab 2\nDue: Thu Oct 19\n\n\n\nMidterm examination\nDue: Fri Oct 27\n\n\n\nHomework 3\nDue: Thu Nov 2\n\n\n\nLab 3\nDue: Thu Nov 2\n\n\n\nPaper #3\nDue: Fri Nov 10\n\n\nPart IV: Data in practice\n\n\n\n\n\nHomework 4\nDue: Thu Nov 16\n\n\n\nLab 4\nDue: Thu Nov 16\n\n\n\nHomework 5\nDue: Thu Nov 30\n\n\n\nLab 5\nDue: Thu Nov 30\n\n\n\nPaper #4\nDue:  Fri Dec 1  Fri Dec 8 \n\n\nCourse wrap-up\n\n\n\n\n\nFinal examination\nDue: Tues Dec 12\n\n\n\n\n\nHomework 0\nFor your first homework, you will complete a few computational exercises. This homework is ungraded, and while there is no prerequisite for this course, homework 0 (hw-0) should serve as a review and a preview of content that we’ll explore for the course. Please do not hesitate to reach out with questions.\nExercise 0.1. If an individual’s projected income is to be converted to a z-score, which of these z-scores would a “rational agent” prefer: -2.00, -1.00, 0, 1.00, or 2.00? What is a “rational agent”? Explain why the “rational agent” would prefer the z-score you selected? Cite any sources.\nExercise 0.2. What is an integer? What is the sum of the first 100 positive integers? Explain your solution. Cite any sources. Bonus points (but definitely not required for our course): write a proof of the solution.\nExercise 0.3. Have you ever heard of p-hacking? P-hacking is where a researcher conducts multiple tests and only reports the significant results from those tests. What are some main issues here?\nI will not collect hw-0 but I am happy to review it if you’d like; upload it to Canvas here.\n\n\nLab 0\nFor your first lab, you will install two software programs on your local machine: the first is base R and the second is the RStudio Integrated Development Environment, or IDE. Both programs are required.\n\n\n\n\n\n\nYou must download both R and RStudio!\n\n\n\n\n\nIn order to complete assignments, you must download base R and RStudio to your computer.\n\n\n\nRStudio is now called Posit. When you are searching online you will begin to see language differences.\nIt can all get very confusing. In order to reduce confusion, I am requesting that you watch this short tutorial so that we’re all on the same page. I will discuss this more when we meet.\n\n\nNext up: Computing\nOn the next page, I will walk you through downloading the appropriate software for our work this term."
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "For Lab 5, you may submit your solutions to Canvas as a .pdf or an R Script or even as a .txt file.\nPlease see the Preparing Lab Reports at the bottom of the lab 1 assignment."
  },
  {
    "objectID": "labs/lab5.html#learning-objectives",
    "href": "labs/lab5.html#learning-objectives",
    "title": "Lab 5",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 5 assignment focuses on you exploring multiple variables located in gssr.\nYour analyses in this lab should focus on a complex analysis of multiple variables and their interactions. Specifically, your path model should indicate the associations between the multiple variables in your model and make use of a complex theoretical relationship to develop your path model and subsequent theories.\nPlease note: you may use the same variables from a previous assignment but please use different questions.\nYour analysis in lab 5 may inform the (optional) paper 5 assignment. Ideally, you will use similar constructs to those described in paper 1."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "For Lab 3, you may submit your solutions to Canvas as a .pdf or an R Script or even as a .txt file.\nPlease see the Preparing Lab Reports at the bottom of the lab 1 assignment."
  },
  {
    "objectID": "labs/lab3.html#learning-objectives",
    "href": "labs/lab3.html#learning-objectives",
    "title": "Lab 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 3 assignment focuses on you exploring any two variables located in one of the data frames that we have explored, with a preference for the gssr package over gss_cat. You will again return to some of the functions we have used before to clean variables and manipulate data frames.\nYour analyses in this lab should focus on a regression analysis between two variables, one or both variables must be a numeric.\nPlease note: you should not use the same variables from a previous assignment but you may use the same data set.\nImportantly, your analysis in lab 3 should inform the paper 3 assignment."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "For Lab 2, you may submit your solutions to Canvas as a .pdf or an R Script or even as a .txt file.\nPlease see the Preparing Lab Reports at the bottom of the lab 1 assignment."
  },
  {
    "objectID": "labs/lab2.html#learning-objectives",
    "href": "labs/lab2.html#learning-objectives",
    "title": "Lab 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 2 assignment focuses on you exploring any two variables located in one of the data frames in the critstats package or in gss_cat. You will also return to some of the functions we have used before to clean variables and manipulate data frames. Your analyses in this lab should focus on a bivariate analysis between two categorical variables, two numeric variables, or one numeric variable and one categorical variable.\nPlease note: you should not use the same variables from a previous assignment but you may use the same data set."
  },
  {
    "objectID": "labs/lab2.html#learning-activities",
    "href": "labs/lab2.html#learning-activities",
    "title": "Lab 2",
    "section": "Learning Activities",
    "text": "Learning Activities\nBy the end of this lab you will be able to:\n\nLocate data sets in the critstats package, or use gss_cat\nDevelop an original research question\nClean and manipulate data for analysis\nExamine the relationship between two variables\n\nYou should submit your final output on Canvas here."
  },
  {
    "objectID": "labs/lab2.html#task-0.1-create-a-new-projec-called-stats-pt3",
    "href": "labs/lab2.html#task-0.1-create-a-new-projec-called-stats-pt3",
    "title": "Lab 2",
    "section": "Task 0.1: Create a new projec called stats-pt3",
    "text": "Task 0.1: Create a new projec called stats-pt3\nIn your R session, navigate to: File &gt; New Project. Create a new project called stats-pt3."
  },
  {
    "objectID": "labs/lab2.html#task-0.2-check-your-working-directory",
    "href": "labs/lab2.html#task-0.2-check-your-working-directory",
    "title": "Lab 2",
    "section": "Task 0.2: Check your working directory",
    "text": "Task 0.2: Check your working directory\nCheck your working directory by typing getwd().\nIf you are not in the desired directory, you can change your directory using the associated path. This path should be the same as the project folder that you plan to work out of for the next several weeks.\nFor example, stats-pt3/lab2.\n\n# insert your desired path in the parenthesis and remove the #\n# setwd(\"/your/working/directory/goes/here\") \n\nYou can add a new sub-folder manually or under the Files tab in the RStudio IDE."
  },
  {
    "objectID": "labs/lab2.html#task-0.3-start-a-new-file-in-your-session",
    "href": "labs/lab2.html#task-0.3-start-a-new-file-in-your-session",
    "title": "Lab 2",
    "section": "Task 0.3: Start a new file in your session",
    "text": "Task 0.3: Start a new file in your session\nOnce you have confirmed that you are in the correct directory, start a new R Script."
  },
  {
    "objectID": "labs/lab2.html#task-0.4-write-a-preamble",
    "href": "labs/lab2.html#task-0.4-write-a-preamble",
    "title": "Lab 2",
    "section": "Task 0.4: Write a preamble",
    "text": "Task 0.4: Write a preamble\n\n## Name: &lt;include your full name&gt;\n## Assignment: Lab 2\n## Date: &lt;here you may want to add a date&gt;\n## Purpose: &lt;insert the goals or purpose of the RScript&gt;"
  },
  {
    "objectID": "labs/lab2.html#task-0.5-packages-and-libraries",
    "href": "labs/lab2.html#task-0.5-packages-and-libraries",
    "title": "Lab 2",
    "section": "Task 0.5: Packages and libraries",
    "text": "Task 0.5: Packages and libraries\n\n# install the tidyverse package\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\n# load the libraries needed for today's analyses\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(critstats)\n\n## update packages if needed; remove # to run code\n# update.packages(\"package-name\")"
  },
  {
    "objectID": "labs/lab2.html#report-1.1",
    "href": "labs/lab2.html#report-1.1",
    "title": "Lab 2",
    "section": "Report 1.1",
    "text": "Report 1.1\nWhat data set have you decided to use?"
  },
  {
    "objectID": "labs/lab2.html#report-1.2",
    "href": "labs/lab2.html#report-1.2",
    "title": "Lab 2",
    "section": "Report 1.2",
    "text": "Report 1.2\nWhich two variables from your data set will be analyzed?"
  },
  {
    "objectID": "labs/lab2.html#report-1.3",
    "href": "labs/lab2.html#report-1.3",
    "title": "Lab 2",
    "section": "Report 1.3",
    "text": "Report 1.3\nWhat is your research question?"
  },
  {
    "objectID": "labs/lab2.html#report-1.4",
    "href": "labs/lab2.html#report-1.4",
    "title": "Lab 2",
    "section": "Report 1.4",
    "text": "Report 1.4\nWhat is your data analysis plan? Please be descriptive."
  },
  {
    "objectID": "labs/lab2.html#report-1.5",
    "href": "labs/lab2.html#report-1.5",
    "title": "Lab 2",
    "section": "Report 1.5",
    "text": "Report 1.5\nWhat are some potential limitations for your analysis?"
  },
  {
    "objectID": "labs/lab2.html#report-1.6",
    "href": "labs/lab2.html#report-1.6",
    "title": "Lab 2",
    "section": "Report 1.6",
    "text": "Report 1.6\nDoes your data contain missing values? If so, how have you dealt with these values?"
  },
  {
    "objectID": "labs/lab2.html#report-1.7",
    "href": "labs/lab2.html#report-1.7",
    "title": "Lab 2",
    "section": "Report 1.7",
    "text": "Report 1.7\nPlease include all code used to clean and manipulate the variables."
  },
  {
    "objectID": "labs/lab2.html#report-1.8",
    "href": "labs/lab2.html#report-1.8",
    "title": "Lab 2",
    "section": "Report 1.8",
    "text": "Report 1.8\nWhat relationship, if any, exists between the two variables?"
  },
  {
    "objectID": "labs/lab2.html#report-1.9",
    "href": "labs/lab2.html#report-1.9",
    "title": "Lab 2",
    "section": "Report 1.9",
    "text": "Report 1.9\nHow do these findings relate to your research question and theory?"
  },
  {
    "objectID": "labs/lab2.html#report-1.10",
    "href": "labs/lab2.html#report-1.10",
    "title": "Lab 2",
    "section": "Report 1.10",
    "text": "Report 1.10\nWhat limitations exist as a result of the data analysis?"
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "For Lab 4, you may submit your solutions to Canvas as a .pdf or an R Script or even as a .txt file.\nPlease see the Preparing Lab Reports at the bottom of the lab 1 assignment."
  },
  {
    "objectID": "labs/lab4.html#learning-objectives",
    "href": "labs/lab4.html#learning-objectives",
    "title": "Lab 4",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe Lab 4 assignment focuses on you exploring multiple variables located in gssr.\nYour inquiry in this lab should focus on a multivariate regression analysis.\nPlease note: you may use the same variables from a previous assignment but please use different questions.\nImportantly, your analysis in lab 4 should inform the paper 4 assignment."
  },
  {
    "objectID": "hw/hw1.html",
    "href": "hw/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Please submit Homework 1 responses as a .pdf file on Canvas here.\n\nExercise 1.1\nThe USPHS Syphilis Study at Tuskegee was one of many historical cases of scientific racism and unethical practices enacted by the U.S. government. Was the USPHS Syphilis Study at Tuskegee an experimental or observational study? Explain your reasoning.\n\n\nExercise 1.2\nConduct a web search and literature review to identify two (2) other cases where data and/or information was collected using unethical practices. Provide a brief explanation of each case and its significance to understanding ethics. Include a full citation of all sources.\n\n\nExercise 1.3\nThere are many definitions of statistics. What is the definition of statistics that has been used in our course lectures? Conduct a web search and find two (2) alternative definitions of statistics. Include a full citation of all sources.\n\n\nExercise 1.4\nBased on your current understanding of statistics, what should it mean to be critical in the context of statistics? Explain your thinking. Include a full citation of all sources.\n\n\nExercise 1.5\nYou have been tasked with identifying data for a new study on homelessness and housing insecurity in your local area. Identify and describe two (2) of each variable type that could be collected for this study: nominal, ordinal, discrete, continuous. That is, identify and fully describe two nominal, two ordinal, two discrete, and two continuous variables that could be used to gather insights about homelessness and housing insecurity. Your descriptions should be detailed.\n\n\nExercise 1.6\nYour local city council plans to conduct a 2024 census of the local homeless population. You have been hired as a data analyst to identify priorities for the project. This census is part of a broader effort to understand the issues experienced during homelessness and to find ways to mitigate issues given the recent increase in the homeless population. What are some potential ethical issues that could arise with the councils’ plans to conduct a local homeless census? In the city’s plan to collect data on those experiencing homelessness, should informed consent be obtained per the IRB? If so, why and how?\n\n\nExercise 1.7\nDescribe the contents of the data set below and what the values most likely represent.\n\n\n# A tibble: 6 × 62\n  country  year_1960 year_1961 year_1962 year_1963 year_1964 year_1965 year_1966\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghani…   8996967   9169406   9351442   9543200   9744772   9956318  10174840\n2 Albania    1608800   1659800   1711319   1762621   1814135   1864791   1914573\n3 Algeria   11057864  11336336  11619828  11912800  12221675  12550880  12902626\n4 America…     20127     20605     21246     22029     22850     23675     24473\n5 Andorra      13410     14378     15379     16407     17466     18542     19646\n6 Angola     5454938   5531451   5608499   5679409   5734995   5770573   5781305\n# ℹ 54 more variables: year_1967 &lt;dbl&gt;, year_1968 &lt;dbl&gt;, year_1969 &lt;dbl&gt;,\n#   year_1970 &lt;dbl&gt;, year_1971 &lt;dbl&gt;, year_1972 &lt;dbl&gt;, year_1973 &lt;dbl&gt;,\n#   year_1974 &lt;dbl&gt;, year_1975 &lt;dbl&gt;, year_1976 &lt;dbl&gt;, year_1977 &lt;dbl&gt;,\n#   year_1978 &lt;dbl&gt;, year_1979 &lt;dbl&gt;, year_1980 &lt;dbl&gt;, year_1981 &lt;dbl&gt;,\n#   year_1982 &lt;dbl&gt;, year_1983 &lt;dbl&gt;, year_1984 &lt;dbl&gt;, year_1985 &lt;dbl&gt;,\n#   year_1986 &lt;dbl&gt;, year_1987 &lt;dbl&gt;, year_1988 &lt;dbl&gt;, year_1989 &lt;dbl&gt;,\n#   year_1990 &lt;dbl&gt;, year_1991 &lt;dbl&gt;, year_1992 &lt;dbl&gt;, year_1993 &lt;dbl&gt;, …\n\n\n# A tibble: 6 × 62\n  country  year_1960 year_1961 year_1962 year_1963 year_1964 year_1965 year_1966\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Vietnam   32670048  33666111  34683410  35721213  36780001  37858947  38958046\n2 Virgin …     32500     34300     35000     39800     40800     43500     46200\n3 West Ba…        NA        NA        NA        NA        NA        NA        NA\n4 Yemen, …   5315351   5393034   5473671   5556767   5641598   5727745   5816241\n5 Zambia     3070780   3164330   3260645   3360099   3463211   3570466   3681953\n6 Zimbabwe   3776679   3905038   4039209   4178726   4322854   4471178   4623340\n# ℹ 54 more variables: year_1967 &lt;dbl&gt;, year_1968 &lt;dbl&gt;, year_1969 &lt;dbl&gt;,\n#   year_1970 &lt;dbl&gt;, year_1971 &lt;dbl&gt;, year_1972 &lt;dbl&gt;, year_1973 &lt;dbl&gt;,\n#   year_1974 &lt;dbl&gt;, year_1975 &lt;dbl&gt;, year_1976 &lt;dbl&gt;, year_1977 &lt;dbl&gt;,\n#   year_1978 &lt;dbl&gt;, year_1979 &lt;dbl&gt;, year_1980 &lt;dbl&gt;, year_1981 &lt;dbl&gt;,\n#   year_1982 &lt;dbl&gt;, year_1983 &lt;dbl&gt;, year_1984 &lt;dbl&gt;, year_1985 &lt;dbl&gt;,\n#   year_1986 &lt;dbl&gt;, year_1987 &lt;dbl&gt;, year_1988 &lt;dbl&gt;, year_1989 &lt;dbl&gt;,\n#   year_1990 &lt;dbl&gt;, year_1991 &lt;dbl&gt;, year_1992 &lt;dbl&gt;, year_1993 &lt;dbl&gt;, …\n\n\n\n\nExercise 1.8\n\nsum(1:51)\n\n\nWhat is the meaning of the code chunk sum(1:51)?\nWhat is the numerical output?\n\n\n\nExercise 1.9\nThe population of five countries is listed in a data set using computational scientific notation.\nNumerically expand the population for each country.\n\nCountry 1: 2.06139E8\nCountry 2: 8.9561E7\nCountry 3: 2.77E7\nCountry 4: 2.72815E5\nCountry 5: 6.077E3\n\n\n\nExercise 1.10\nDescribe the error in the following attempt to construct a data frame.\n\nvec1 &lt;- c(1, 2, 3, 4)\nvec2 &lt;- c(\"a\",\"b\",\"c\",\"d\")\nvec3 &lt;- data.frame(T, F, F, T)\ndf &lt;- data.frame(vec1, vec2, vec3)"
  },
  {
    "objectID": "hw/hw3.html",
    "href": "hw/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please submit Homework 3 responses as a .pdf file on Canvas here.\n\nExercise 1.1\nIs the relationship between the \\(x\\) and \\(y\\) variables in the below model significant?\nIf so, explain. If not, explain why.\n\nmodel &lt;- lm(y ~ x)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.19086 -0.70179 -0.07264  0.79898  2.37303 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.1441     0.1231   74.28   &lt;2e-16 ***\nx            -5.9740     0.1277  -46.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 73 degrees of freedom\nMultiple R-squared:  0.9677,    Adjusted R-squared:  0.9673 \nF-statistic:  2187 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nExercise 1.2\nExamine the plot below. Estimate the correlation coefficient for the plot.\n\nplot(x, y)\n\n\n\n\n\n\nExercise 1.3\nExamine the plot below. Estimate the correlation coefficient for the plot.\nBased on your estimate, should we move forward with our analysis? If so, why? If no, why not?\n\nplot(age, income)\n\n\n\n\n\n\nExercise 1.4\nIn a few sentences, summarize the relationship between the variables based on the output.\nIs there a significant relationship?\n\nmodel2 &lt;- lm(funding ~ capacity)\nsummary(model2)\n\n\nCall:\nlm(formula = funding ~ capacity)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28999 -12361   1602  10632  35789 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 238752.68  124897.76   1.912   0.0589 .\ncapacity       -19.92      16.64  -1.197   0.2342  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14590 on 98 degrees of freedom\nMultiple R-squared:  0.01441,   Adjusted R-squared:  0.004349 \nF-statistic: 1.432 on 1 and 98 DF,  p-value: 0.2342\n\nplot(capacity, funding)\nabline(model2, col=\"blue\")\n\n\n\n\n\n\nExercise 1.5\nUsing the model outlined above and the plot shown below, explain the function of a residual plot.\nDoes the residual plot represent a “healthy” or “problematic” pattern?\n\nresids2 &lt;- residuals(model2)\nplot(funding, resids2)"
  },
  {
    "objectID": "hw/hw5.html",
    "href": "hw/hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Please submit Homework 5 responses as a .pdf file on Canvas here.\n\nExercise 1.1"
  },
  {
    "objectID": "papers/paper3.html",
    "href": "papers/paper3.html",
    "title": "Paper 3",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:\n\nInstructions for papers\nMore information for course papers\n\n\nStep 1: Theory and logic diagram\nFor paper #3, we are interested in exploring theories pertaining to the relationship between two aggregate variables. Specifically, we are interested in understanding the association of characteristics.\nNote: For your paper, please remember to address the theoretical relationship that the literature has identified between the conceptual variables selected. That is, before describing the measures and the survey you will be using, explain your theory and why you think that your independent variable is having an effect on your dependent variable.\n\n\nStep 2: Variables, Measurement, Hypothesis\nAll variables and analyses should be described in detail.\n\n\nStep 3: Statistical Analysis\nPlease be sure to utilize analyses on the interactions between multiple variables.\n\n\nStep 4: Conclusion\nFor this paper, please be sure to focus on a solid conclusion based on your statistical analysis."
  },
  {
    "objectID": "papers/annotated.html",
    "href": "papers/annotated.html",
    "title": "Annotated Bibliography and Paper 1 (updates)",
    "section": "",
    "text": "This page will support you with the re-submission of Paper #1 with edits and highlighting citations.\nThe highlighted citations will be used to inform the contents of your annotated bibligography."
  },
  {
    "objectID": "papers/annotated.html#some-guidance-on-doing-an-annotated-bibliography",
    "href": "papers/annotated.html#some-guidance-on-doing-an-annotated-bibliography",
    "title": "Annotated Bibliography and Paper 1 (updates)",
    "section": "Some guidance on doing an  annotated bibliography ",
    "text": "Some guidance on doing an  annotated bibliography \n\nPurpose\nEvery research paper should include a relevant literature review. Literature reviews can help your readers understand the need for your study, outline a paper’s central thesis, and support readers in making sense of closely related results and findings. As a result, literature reviews can come in many different lengths and formats.\nDoing a literature review is similar to doing any kind of research. A literature review should identify a central question, methodology, and also report findings. One effective way to start a literature review is to create an annotated bibliography.\nPrior to starting an annotated bibliography, it is useful to develop a question that will help you explore and better understand theoretical and methodological connections.\n\n\nFraming a question\nLiterature reviews, like most research projects, can begin from a basic question.\n\nDoes income relate to the availability of resources for [a population]?\nAre years of experience for [a sample] related to their perceptions of power?\n\nOnce you have identified a suitable question, you can use keywords to find sources for an annotated bibliography. The library website can be used to search periodicals or sources like the ERIC system, and you can also utilize popular search engines that house similar works, such as Research Rabbit and Google Scholar.\n\n\nAnnotated bibliographies\nAn annotated bibliography is a list of sources about a specific research question or topic. Each source contains a short statement about the contents of the source (e.g., research paper, online article, or other scholarly and/or published works). Annotated bibliographies are not abstracts, although their length and structure may be similar.\nEach source included in an annotated bibliography should begin with an appropriate citation of the source (in APA, MLA, or Chicago style) and a brief description of the purpose and contents of the source, as well as an evaluation or reflection. The inclusion of an evaluation or reflection when summarizing a published source is one effective way to tell the difference between the abstract and the annotation.\nView samples of annotated bibliographies."
  },
  {
    "objectID": "week9-slides.html#part-i-context",
    "href": "week9-slides.html#part-i-context",
    "title": "DATA 202 - Week 9",
    "section": "Part I: Context",
    "text": "Part I: Context\nThis week launches part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to formulate a focused definition of social justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains. We will begin with an example."
  },
  {
    "objectID": "week9-slides.html#part-ii-content",
    "href": "week9-slides.html#part-ii-content",
    "title": "DATA 202 - Week 9",
    "section": "Part II: Content",
    "text": "Part II: Content\nThe methods used to collect sample data is extremely important.\nIf sample data are not collected in an appropriately, any resulting statistical analyses will be futile. As a result, planning a study by identifying research questions, the population and sample of interest, and considering the types of methods that will be used to analyze data that is collected are all essential parts in the statistical data analysis process."
  },
  {
    "objectID": "week9-slides.html#part-iii-code",
    "href": "week9-slides.html#part-iii-code",
    "title": "DATA 202 - Week 9",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will focus on a new data set: the General Social Survey, or GSS.\nWe will begin by installing a few new packages.\n\n## install new packages\ninstall.packages(\"srvyr\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"statsr\", repos = \"http://cran.us.r-project.org\")\n\nThen we will load the libraries.\n\n## load libraries\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(descr)\nlibrary(foreign)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(statsr)\n\nTo access the GSS data, we will need to use a remote install command.\nThe data come from the GSSR package.\n\n# install GSSR data\nremotes::install_github(\"kjhealy/gssr\")\nlibrary(gssr)\n\nWe will now load the GSS documentation.\n\ndata(gss_doc)\n\nView the GSS documentation.\n\ndata(gss_doc)\n\nWe will pick up from here with live coding in class.\nNext up: Week 10\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "DATA 202 - Week 9",
    "section": "",
    "text": "This week launches part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to formulate a focused definition of social justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains. We will begin with an example.\n\n\n\nData from 1973 on violent crime rates by US State in the USArrests data set.\nValues are per 100,000 residents for assault, murder, and rape.\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\ntail(USArrests)\n\n              Murder Assault UrbanPop Rape\nVermont          2.2      48       32 11.2\nVirginia         8.5     156       63 20.7\nWashington       4.0     145       73 26.2\nWest Virginia    5.7      81       39  9.3\nWisconsin        2.6      53       66 10.8\nWyoming          6.8     161       60 15.6\n\n\n\ncor(USArrests$UrbanPop, USArrests$Murder)\n\n[1] 0.06957262\n\nplot(USArrests$UrbanPop, USArrests$Murder,\n     xlab = \"Percent living in urban areas\",\n     ylab = \"Murders per 100,000 residents\")\n\n\n\n\nWhat are some initial takeaways from this plot?\nWithout a guiding theory, where might our analyses take us?\n\n\n\n\nIn your groups, consider the values in the USArrests data and generate some questions."
  },
  {
    "objectID": "week9.html#part-i-context",
    "href": "week9.html#part-i-context",
    "title": "DATA 202 - Week 9",
    "section": "",
    "text": "This week launches part 3 of our course: data and policy, as in public policies.\nOur goals this week will be to formulate a focused definition of social justice for your particular area(s) of interest and study, and to cite references that allow us to understand the meaning of social justice in your specific intellectual domains. We will begin with an example.\n\n\n\nData from 1973 on violent crime rates by US State in the USArrests data set.\nValues are per 100,000 residents for assault, murder, and rape.\n\nhead(USArrests)\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\ntail(USArrests)\n\n              Murder Assault UrbanPop Rape\nVermont          2.2      48       32 11.2\nVirginia         8.5     156       63 20.7\nWashington       4.0     145       73 26.2\nWest Virginia    5.7      81       39  9.3\nWisconsin        2.6      53       66 10.8\nWyoming          6.8     161       60 15.6\n\n\n\ncor(USArrests$UrbanPop, USArrests$Murder)\n\n[1] 0.06957262\n\nplot(USArrests$UrbanPop, USArrests$Murder,\n     xlab = \"Percent living in urban areas\",\n     ylab = \"Murders per 100,000 residents\")\n\n\n\n\nWhat are some initial takeaways from this plot?\nWithout a guiding theory, where might our analyses take us?\n\n\n\n\nIn your groups, consider the values in the USArrests data and generate some questions."
  },
  {
    "objectID": "week9.html#part-ii-content",
    "href": "week9.html#part-ii-content",
    "title": "DATA 202 - Week 9",
    "section": "Part II: Content",
    "text": "Part II: Content\nThe methods used to collect sample data is extremely important.\nIf sample data are not collected in an appropriately, any resulting statistical analyses will be futile. As a result, planning a study by identifying research questions, the population and sample of interest, and considering the types of methods that will be used to analyze data that is collected are all essential parts in the statistical data analysis process.\n\n\nMore on sampling methods\nFirst, let’s revisit and describe the different sampling methods:\nThere are two broad categories of selecting members of a population to generate sample data: probability sampling and non-probability sampling.\nWithin these two broad categories are other methods based on the needs of the study. Each of these methods is used to support statistical data analysis with some methods providing stronger evidence than others.\n\n\nProbability sampling\n\ninvolves the random selection of subjects in such a way that every member of a sample has the sample probability of being selected.\n\nNon-probability sampling\n\ninvolves the use of criteria to select data that is not based on an equal likelihood of selection.\n\n\n\n\n\nProbability sampling methods\n\nSimple random sample of \\(n\\) subjects ensures that every possible sample of the same size \\(n\\) has the same chance or likelihood of being chosen.\nSystematic sample of \\(n\\) subjects involves selecting every \\(k\\)th subject on some regular interval.\nStratified sample involves dividing the population up into strata (groups) with the same characteristics and then randomly sampling within those strata.\nCluster sample involves partitioning the population into clusters (groups), randomly selecting some clusters, and then selecting all members of the selected clusters.\n\n\n\n\nNon-prbability sampling methods\n\nConvenience sample is data gathered from the most accessible or convenient source. Although this is easy and efficient, the data cannot produce generalizable results.\nPurposive sample is data gathered based on the purposes of the research or the specific research question. This strategy includes clear criteria and rationale for inclusion.\nSnowball sample is data gathered via recruitment by the other participants of a study. The number of subjects included “snowballs” as more contacts are generated.\n\n\n\nMultistage sampling\nIn larger studies, sometimes multistage sampling procedures are used to generate data. In this design, different samples are selected in different stages, and each stage might use a different sampling method. The end result may represent a complicated sampling design but it is often simpler and faster than some designs, such as a simple random sample.\n\nMissing Data\nData can be missing from a set, and the total number of elements can differ between two or more sets.\nThe differences in the number of elements can be due to a host of reasons, but often it is missing data. A data value can be missing at random or not missing at random. The amount of information missing from a data set can have a minimal impact or a major impact on a statistical analysis.\n\n\n\n\n\n\nNote – Missing data\n\n\n\nA data value is missing completely at random if the likelihood of it being missing is independent of its value or any other values in the data set is just as likely to be missing.\nA data value is not missing at random if the missing value is related to the reason it is missing."
  },
  {
    "objectID": "week9.html#part-iii-code",
    "href": "week9.html#part-iii-code",
    "title": "DATA 202 - Week 9",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will focus on a new data set: the General Social Survey, or GSS.\nWe will begin by installing a few new packages.\n\n## install new packages\ninstall.packages(\"srvyr\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"statsr\", repos = \"http://cran.us.r-project.org\")\n\nThen we will load the libraries.\n\n## load libraries\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(descr)\nlibrary(foreign)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(statsr)\n\nTo access the GSS data, we will need to use a remote install command.\nThe data come from the GSSR package.\n\n# install GSSR data\nremotes::install_github(\"kjhealy/gssr\")\nlibrary(gssr)\n\nWe will now load the GSS documentation.\n\ndata(gss_doc)\n\nView the GSS documentation.\n\ndata(gss_doc)\n\nWe will pick up from here with live coding in class.\n\nNext up: Week 10"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DATA 202",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nHWs\nLabs\nPapers\nOther\n\n\n\n\n\nI\n1\nTues, Aug 22\nIntroduction to the course\nHW 0\nLab 0\n\n\n\n\n\n\n2\nTues, Aug 29\nFoundations\n\n\nPaper 1\n\n\n\n\n\n3\nTues, Sept 5\nTheory construction\n\n\n(Paper 1 edits)\n\n\n\n\n\n4\nTues, Sept 12\nMore on theory construction\n\n\n(Paper 1 edits)\n\n\n\n\nII\n5\nTues, Sept 19\nUnivariate analysis\n\n\n(Paper 1 edits)\n\n\n\n\n\n6\nTues, Sept 26\nExploratory data analysis\nHW 1\nLab 1\n\n\n\n\n\n\n7\nTues, Oct 3\nBivariate analysis\n\n\n\nAnnotated bibliography\n\n\n\n\n8\nTues, Oct 10\nNotes on causal theories\n\n\nPaper 2\n\n\n\n\nIII\n9\nTues, Oct 17\nModeling social in/justice\nHW 2\nLab 2\n\n\n\n\n\n\n10\nTues, Oct 24\nMidterm examination week\n\n\n\nMidterm examination\n\n\n\n\n11\nTues, Oct 31\nBivariate regression analysis\nHW 3\nLab 3\n\n\n\n\n\n\n12\nTues, Nov 7\nMultivariate regression (part a)\n\n\nPaper 3\n\n\n\n\nIV\n13\nTues, Nov 14\nMultivariate regression (part b)\nHW 4\nLab 4\n\n\n\n\n\n\n14\nTues, Nov 21\nOverview of additional models\n\n\n\n\n\n\n\n\n15\nTues, Nov 28\nWrap-up\nHW 5\nLab 5\n\n\n\n\n\nWrap-up\n16\nTues, Dec 5\nOpen office hours\n\n\nPaper 4\n\n\n\n\n\n17\nTues, Dec 12\nNo class meeting\n\n\n\nFinal examination"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Welcome to DATA 202",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nHWs\nLabs\nPapers\nOther\n\n\n\n\n\nI\n1\nTues, Aug 22\nIntroduction to the course\nHW 0\nLab 0\n\n\n\n\n\n\n2\nTues, Aug 29\nFoundations\n\n\nPaper 1\n\n\n\n\n\n3\nTues, Sept 5\nTheory construction\n\n\n(Paper 1 edits)\n\n\n\n\n\n4\nTues, Sept 12\nMore on theory construction\n\n\n(Paper 1 edits)\n\n\n\n\nII\n5\nTues, Sept 19\nUnivariate analysis\n\n\n(Paper 1 edits)\n\n\n\n\n\n6\nTues, Sept 26\nExploratory data analysis\nHW 1\nLab 1\n\n\n\n\n\n\n7\nTues, Oct 3\nBivariate analysis\n\n\n\nAnnotated bibliography\n\n\n\n\n8\nTues, Oct 10\nNotes on causal theories\n\n\nPaper 2\n\n\n\n\nIII\n9\nTues, Oct 17\nModeling social in/justice\nHW 2\nLab 2\n\n\n\n\n\n\n10\nTues, Oct 24\nMidterm examination week\n\n\n\nMidterm examination\n\n\n\n\n11\nTues, Oct 31\nBivariate regression analysis\nHW 3\nLab 3\n\n\n\n\n\n\n12\nTues, Nov 7\nMultivariate regression (part a)\n\n\nPaper 3\n\n\n\n\nIV\n13\nTues, Nov 14\nMultivariate regression (part b)\nHW 4\nLab 4\n\n\n\n\n\n\n14\nTues, Nov 21\nOverview of additional models\n\n\n\n\n\n\n\n\n15\nTues, Nov 28\nWrap-up\nHW 5\nLab 5\n\n\n\n\n\nWrap-up\n16\nTues, Dec 5\nOpen office hours\n\n\nPaper 4\n\n\n\n\n\n17\nTues, Dec 12\nNo class meeting\n\n\n\nFinal examination"
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Welcome to DATA 202",
    "section": "Start here!",
    "text": "Start here!\n\nFirst, welcome.\nI am excited to learn and explore together in our course.\nI’ll start with the image (Battey, C. M., ca. 1919) to the top-left of the screen.\nAs you may know, it is a photo of W. E. B. Du Bois1, who is one of the many researchers, activists, and community organizers that will help guide our statistical practices this term. Using their examples, we make sense of theories around the phenomenon of “social justice,” and we use statistics to theorize.\nThe pages on this site outline the coding components of the DATA 202: Statistically Measuring and Modeling Social Justice course at Howard University, taught by me: Professor Nathan Alexander.\nThe information on this page will serve as a technical companion to our Canvas site.\nThis page will support you in our course in the following ways:\n\nThis page will help you work with data sets and complete small tasks over time.\nThis page will walk you through the measurement aspect of research questions relevant to modeling issues of social justice.\nThis page will help you integrate mathematical ideas and technical formulas into your work.\nThis page will support your use of theory in your statistical practice, and increase your ability to document reproducible code around a set of research questions.\n\nOur canvas site (click here) contains readings and is where you will submit all assignments.\nThis site should be paired with Canvas to help you complete course assignments.\nAnd just for good measure: all assignments should be turned in via Canvas.\nWhen turning in assignments on Canvas:\n\nGenerally, PDF submissions are the way to go.\nI will not download any files not uploaded to Canvas. I will only click on a clearly visible Github repository (i.e., no shortened or bit.ly links). Article citations that include DOIs are preferred.\nWe will discuss Quarto Markdown files which integrate code, formulas, and text. Quarto is actually how I created this site for our course, and it is a great way to communicate your technical ideas.\nThis is a Canvas companion site/page (I’ll use these terms interchangeably) and it will serve as your guide to completing all technical assignments.\n\n\n\n It is alive!\nThis is a living document, meaning that more materials will be added as topics are introduced. It is important that you check these pages regularly. You should also use this course organization as an archive of any technical code that is covered in class. Canvas contains readings and this page will integrate code.\n\n\nJust a reminder.\nCanvas contains readings. This site integrates code. All submissions go to Canvas: here.\n\n\nNext up: Overview\nThe overview in the next section will walk you through the technical flow of the course.\nLike this section, a preview of forthcoming topics will be included at the bottom of each page; this means that you can treat this site like a textbook that you would read in order from start to finish. Though I encourage you to explore what else is out there and make use of other resources. That said, submissions should follow the formal guidelines outlined on our syllabus, Canvas, and this site.\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nComing up next are the four parts of the course and an overview of the assignments for the term. Course topics are outlined in more detail and integrated into the assignment submission schedule on the next page; see you over there!"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome to DATA 202",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’ll look for further details about Du Bois’ work when we consider two things in particular: first, the historical context and, second, how we develop frameworks to make sense of our hypotheses.↩︎"
  },
  {
    "objectID": "computing.html#lab-0-downloading-r-and-rstudio.",
    "href": "computing.html#lab-0-downloading-r-and-rstudio.",
    "title": "Computing",
    "section": "Lab 0: Downloading R and RStudio.",
    "text": "Lab 0: Downloading R and RStudio.\nFor lab-0 you will download two software programs to your computer.\nPlease watch the below video to help you download R and RStudio.\n\n\n\n\nWatch this video to complete the steps below:\n\n- Step 1: Navigate to https://posit.co\n\n\n- Step 2: Click on the Download RStudio link\n\n\n- Step 3: Install base R (Install R)\n\n\n- Step 4: Install the RStudio IDE (Install RStudio)\nOnce you have completed your downloads, check to see that you can open RStudio (not R).\nWe’ll pick up here in class next week.\n\n\nNext up: Schedule and Week 1\nPlease see the full course schedule on the next page.\nAs a thought exercise, you will write a short paper based on a social justice issue you are familiar with or wish to learn more about. In this paper, you should write about a theory concerning the relationships between and among some dependent variable and particular independent variables (you may have one or more than one independent variables of interest at the outset). After introducing your theory and assuming you had unlimited research funding, propose a research design and measurement plan for a study that would use real-world data and statistical analysis to examine the theory."
  },
  {
    "objectID": "week3-slides.html#theory",
    "href": "week3-slides.html#theory",
    "title": "DATA 202 - Week 3",
    "section": "Theory",
    "text": "Theory\nSome definitions:\n\nPer the Oxford Languages dictionary, a theory is a supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.\nPer Britannica, a theory is an idea or set of ideas that is intended to explain facts or events.\n\nWe will need to think more acutely about theory development as we progress through the term.\n\nFor now, how might you define a theory?"
  },
  {
    "objectID": "week3-slides.html#theory-construction",
    "href": "week3-slides.html#theory-construction",
    "title": "DATA 202 - Week 3",
    "section": "Theory construction",
    "text": "Theory construction\nPer Markovsky & Webster (2015), theory construction is the process of formulating components of a theory into a logical whole.\nWe may consider some of the following elements as we prepare theoretical statements.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision"
  },
  {
    "objectID": "week3-slides.html#part-ii-content",
    "href": "week3-slides.html#part-ii-content",
    "title": "DATA 202 - Week 3",
    "section": "Part II: Content",
    "text": "Part II: Content\nFraming data and information\nThere are many different ways to conceptualize data and information.\n\n\nDepending on our specific context, statistical needs, or research purposes, we can frame information as data, or data as information, or even place an equivalence statement between the two terms such that we have: \\[\\text{information} = \\text{data}\\]\nAs we continue to explore what it should mean to be critical in the context of statistics, we will need some common language and base definitions to understand the processes involved in a statistical study.\nWe defined statistics as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information. “Collecting” and “organizing” – the first two steps of this process – requires that we define what we mean by data and information."
  },
  {
    "objectID": "week3-slides.html#data-and-information",
    "href": "week3-slides.html#data-and-information",
    "title": "DATA 202 - Week 3",
    "section": "Data and information",
    "text": "Data and information\n\n\n\nDEFINITION: Data\n\n\nData are collections of information or observations.\nThe term data is plural, so we should say “data are…” not “data is…”.\nA single data value is referred to as a datum but this term is very rarely used.\n\n\n\n\n\nThe term “information” is a universal concept that is highly useful but it also lacks precision.\nIn every day terms, information is defined rather loosely.\nIn statistics, however, we might contend that information becomes data when it is collected and organized in some form or fashion.\nThus, it takes some structure to turn information into usable data."
  },
  {
    "objectID": "week3-slides.html#population-and-sample",
    "href": "week3-slides.html#population-and-sample",
    "title": "DATA 202 - Week 3",
    "section": "Population and sample",
    "text": "Population and sample\n\n\n\nDEFINITIONS: Population and sample\n\n\nA population is representative of every member of a group of interest or collection of objects.\nA sample is a sub-collection of members or objects from a selected population.\n\n\n\n\nPopulation and sample. Image from SimplyPsychology.org"
  },
  {
    "objectID": "week3-slides.html#what-is-a-variable",
    "href": "week3-slides.html#what-is-a-variable",
    "title": "DATA 202 - Week 3",
    "section": "What is a variable?",
    "text": "What is a variable?\n\n\n\nDEFINITIONS: Variable\n\n\nA variable is any characteristic or quantity that can be measured or counted.\n\n\n\n\nThere are many types of data and variables used in statistics.\n\nThe type of variable helps determine the appropriate methods for analysis.\n– Categorical or Quantitative\nThe level of measurement helps determine how we measure variables.\n– Nominal, Ordinal, Interval, Ratio"
  },
  {
    "objectID": "week3-slides.html#task-0-understanding-the-rstudio-ide",
    "href": "week3-slides.html#task-0-understanding-the-rstudio-ide",
    "title": "DATA 202 - Week 3",
    "section": "Task 0: Understanding the RStudio IDE",
    "text": "Task 0: Understanding the RStudio IDE\n\nRStudio IDE"
  },
  {
    "objectID": "week3-slides.html#task-1-create-an-r-script",
    "href": "week3-slides.html#task-1-create-an-r-script",
    "title": "DATA 202 - Week 3",
    "section": "Task 1: Create an R Script",
    "text": "Task 1: Create an R Script\nWe will conduct most of our work using what is called an RScript.\nThe RScript allows us to save and annotate our code for future use.\nWhen you run code from an RScript, it will show up in the Console (bottom left pane in RStudio).\nIn the RStudio IDE, open an RScript by using the following navigation:\n\nFile &gt; New File &gt; R Script"
  },
  {
    "objectID": "week3-slides.html#rstudio-ide-with-an-rscript",
    "href": "week3-slides.html#rstudio-ide-with-an-rscript",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Inserting code into an RScript\n\ny &lt;- 2 + 2\n\n\n# You can use the `#` symbol to leave notes above your code.\ny &lt;- 2 + 2 # You can use the `#` symbol to leave notes in-line with your code.\n\nWe just defined an object y. We can see its value by running this syntax and typing y into the Console.\n\ny\n\n[1] 4\n\n\nThe output on your screen should match the last line above - with the hashtags.\n\n[ 1 ] indicates a single line of results.\nThe output tells us that y has a value of 4, so we say that y is numeric or that it has a numeric value.\n\nYou can run code by clicking ‘Run’ at the top of the Source window, or by typing CMD+Enter"
  },
  {
    "objectID": "week3-slides.html#task-2-explore-different-object-types",
    "href": "week3-slides.html#task-2-explore-different-object-types",
    "title": "DATA 202 - Week 3",
    "section": "Task 2: Explore different object types",
    "text": "Task 2: Explore different object types\nFor this task, we will explore three object types: numeric, character, and logic values.\nTask 2-a: Compute a mathematical statement and create a numeric variable\n\n1 + 2\n\n[1] 3\n\n\nWe can assign a variable to this statement by using an assignment operator: &lt;-\n\na &lt;- 1 + 2\n\nWe can also use an equal sign to assign values: \\(=\\)\n\na = 1 + 2\n\nType “a” to show the value of the variable\n\na\n\n[1] 3"
  },
  {
    "objectID": "week3-slides.html#task-3-creating-vectors",
    "href": "week3-slides.html#task-3-creating-vectors",
    "title": "DATA 202 - Week 3",
    "section": "Task 3: Creating vectors",
    "text": "Task 3: Creating vectors\nWhen we want to list multiple objects or values, we use R’s available data types, such as vectors and factors.\n\nWe concatenate values in these data types using the operator c( ) to place our values in the order we desire.\nConcatenate means to place things together one after the other.\n\nVectors\nVectors are a data type we use to order values (i.e., numeric, character, logic) or mix different values.\nFor example, we can store all of the numbers from 1 to 9 in a vector by using a colon.\n\nmy.vector &lt;- c(1:9)\nmy.vector\n\n[1] 1 2 3 4 5 6 7 8 9"
  },
  {
    "objectID": "week3-slides.html#task-4-creating-data-frames",
    "href": "week3-slides.html#task-4-creating-data-frames",
    "title": "DATA 202 - Week 3",
    "section": "Task 4: Creating data frames",
    "text": "Task 4: Creating data frames\nFor this final task, we will create a list. Lists can contain anything: functions, vectors, other lists, and data frames.\nTo start, let’s create a series of vectors.\n\nvec1 &lt;- c(\"Ida B. Wells\", \"W.E.B Du Bois\",\"Mary G. Ross\", \"Jaime Escalante\",\"Etta Z. Falconer\", \"Bob Moses\", \"Ruth Gonzales\")\nvec2 &lt;- c(1862, 1868, 1908, 1930, 1933, 1935, 1970)\nvec3 &lt;- c(\"MS\", \"MA\", \"OK\", \"Bolivia\", \"MS\", \"NYC\", \"NJ\")\nvec4 &lt;- c(TRUE, TRUE, TRUE, FALSE, T, T, T)\nvec5 &lt;- c(\"African American\", \"Ghanaian American\", \"Native American\", \"Bolivian American\", \"African American\", \"African American\", \"Mexican American\")\nvec6 &lt;- c(\"Journalist\", \"Sociologist\", \"Engineer\", \"Educator\", \"Mathematician\", \"Educator\", \"Engineer\")"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "One main goal this week will be to understand theory and theory construction."
  },
  {
    "objectID": "week3.html#theory",
    "href": "week3.html#theory",
    "title": "DATA 202 - Week 3",
    "section": "Theory",
    "text": "Theory\nSome definitions:\n\nPer the Oxford Languages dictionary, a theory is a supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.\nPer Britannica, a theory is an idea or set of ideas that is intended to explain facts or events.\n\nWe will need to think more acutely about theory development as we progress through the term.\n\nFor now, how might you define a theory?"
  },
  {
    "objectID": "week3.html#theory-construction",
    "href": "week3.html#theory-construction",
    "title": "DATA 202 - Week 3",
    "section": "Theory construction",
    "text": "Theory construction\nPer Markovsky & Webster (2015), theory construction is the process of formulating components of a theory into a logical whole.\nWe may consider some of the following elements as we prepare theoretical statements.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision"
  },
  {
    "objectID": "week3.html#part-ii-content",
    "href": "week3.html#part-ii-content",
    "title": "DATA 202 - Week 3",
    "section": "Part II: Content",
    "text": "Part II: Content\n\nFraming data and information\nThere are many different ways to conceptualize data and information.\n\n\nDepending on our specific context, statistical needs, or research purposes, we can frame information as data, or data as information, or even place an equivalence statement between the two terms such that we have: \\[\\text{information} = \\text{data}\\]\nAs we continue to explore what it should mean to be critical in the context of statistics, we will need some common language and base definitions to understand the processes involved in a statistical study.\nWe defined statistics as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information. “Collecting” and “organizing” – the first two steps of this process – requires that we define what we mean by data and information."
  },
  {
    "objectID": "week3.html#data-and-information",
    "href": "week3.html#data-and-information",
    "title": "DATA 202 - Week 3",
    "section": "Data and information",
    "text": "Data and information\n\n\n\n\n\n\nDEFINITION: Data\n\n\n\nData are collections of information or observations.\nThe term data is plural, so we should say “data are…” not “data is…”.\nA single data value is referred to as a datum but this term is very rarely used.\n\n\n\n\nThe term “information” is a universal concept that is highly useful but it also lacks precision.\nIn every day terms, information is defined rather loosely.\nIn statistics, however, we might contend that information becomes data when it is collected and organized in some form or fashion.\nThus, it takes some structure to turn information into usable data.\n\n\n\nOne method of organizing collections of data or information is in a Microsoft Excel sheet.\n\n\n\nExcel spreadsheet\n\n\n\nA spreadsheet in a traditional format contains important features that will support data analysis.\n\n\nColumns are vertical arrangements of sets.\n– Columns are represented by capital letters: \\(A\\), \\(B\\), \\(C\\), and so on.\n– We relate columns to sets, which we defined as collections of elements or items.\nRows are horizontal arrangements of elements.\n– Rows are represented by numbers and relate to the index of a set.\n– The subscript \\(i \\in \\mathbb{N}\\) such that \\(i = 1, 2, 3, ...\\), indicates an element’s position.\n\n\n\n\n\n\nExcel spreadsheet with data on US states\n\n\n\n\n\n\n\n\n\nDEFINITION: Filename extension\n\n\n\nA file extension (or file name extension) is a suffix at the end of a digital file name:\n\n\n.doc\n.docx\n.txt\n\nThe extension indicates the file’s format and the organization of information.\n\n\n\n\nThere are many different file extensions.\n\nFor example, the file extension for an MS Excel document can be .xlxs, .csv, or .htm.\nCSV (.csv) files remove all formatting.\n– The removal of formatting helps to reduce errors when transferring data between computers or software programs.\n“CSV” stands for comma-separated values.\n\n\n\n\n\n\nCSV file with data on US states\n\n\n\n\nFormatting data into a data frame\n\nAlthough the CSV format removes formatting, the file is not yet in a structure that we can use to conduct efficient statistical analyses.\nThe CSV format supports a user with collecting and organizing information into a usable data structure.\nHowever, the data needs to be sent to a computer program to undergo the next step in our statistical process: analyzing data. Specifically, we re-format the CSV file to a data frame to conduct analyses.\n\n\n\n\n\n\n\n\nDEFINITION: Data frame\n\n\n\nA data frame (or dataframe) is a two-dimensional table of rows and columns.\n\n\nData frames are a common and popular way to structure, store, and share data.\n\nData frames allow analysts to store sets of observations that vary in size and content.\nEach row describes a single observation.\nEach column stores information for one set, or variable.\n\n\nRecall that a set is a collection of elements.\n\nWe extend this definition to say that a set is a collection of \\(n \\in \\mathbb{N}\\) elements, where \\(n\\) refers to the number of elements in the set.\n\n\n\n\n\n\n\nNote – Sets with \\(n\\) elements\n\n\n\nIf \\(X\\) is a set with \\(n\\) elements, then \\(X\\) can be represented as \\(X = \\{x_1, x_2, x_3, ..., x_n \\}\\).\nIf \\(Y\\) is a set with \\(n\\) elements, then \\(Y\\) can be represented as \\(Y = \\{y_1, y_2, y_3, ..., y_n \\}\\).\n\n\nThe value of \\(n\\) can be used to represent the “size” of a set.\n\nA set with no elements is referred to as the empty set and can be represented as \\(\\{ \\emptyset \\}\\).\n\nWe will need to make sense of the various objects that a set can contain.\n\nFirst, we think more mathematically about sets and data frames.\n\n\n\n\n\n\nDEFINITION: Matrix\n\n\n\nIn mathematics, a matrix is a rectangular table of entries arranged in rows and columns.\n\n\n\nA data frame containing only numbers is an \\(n \\times m\\) matrix:\n\n\\(n\\) refers to the number of rows (or observations)\n\\(m\\) refers to the number of columns (or variables)\n\nBy formatting a series of sets into a data frame, we get\n\n\\(n\\) rows (each row containing data on a single observation)\n\\(m\\) columns (which contain elements over a variable’s values)\n\n\n\nData frame containing \\(n\\) observations for \\(2\\) variables (\\(X\\) and \\(Y\\))\n\n\n\n\n\n\nEXAMPLE – A data frame containing two variables (or sets)\n\n\n\n\n\n\nX\nY\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(x_3\\)\n\\(y_3\\)\n\n\n.\n.\n\n\n.\n.\n\n\n.\n.\n\n\n\\(x_n\\)\n\\(y_n\\)\n\n\n\n\n\n\nTake, for example, two sets with the following ordered values:\n\n\nLet \\(X\\) contain the odd values 1, 3, 5, and 7.\n\nWe have \\(X = \\{1, 3, 5, 7 \\}\\)\n\nLet \\(Y\\) contain the even values 2, 4, 6, and 8.\n\nWe have \\(Y = \\{2, 4, 6, 8 \\}\\)\n\n\n\n\nWe set the following to be true:\n\n\\(x_1 = 1\\), \\(x_2 = 3\\), \\(x_3 = 5\\), \\(x_4 = 7\\)\n\\(y_1 = 2\\), \\(y_2 = 4\\), \\(y_3 = 6\\), \\(y_4 = 8\\)\n\nBy combining the sets \\(X\\) and \\(Y\\), we create the following data frame:\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n2\n\n\n3\n4\n\n\n5\n6\n\n\n7\n8\n\n\n\n\n\n\nWe list the index of each element of each set using a new column – ID (for index).\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nID\nX\nY\n\n\n\n\n1\n1\n2\n\n\n2\n3\n4\n\n\n3\n5\n6\n\n\n4\n7\n8\n\n\n\n\n\nThis \\(n \\times m\\) data frame contains\n\n\n\\(n = 4\\) observations\n\\(3\\) variables\n\nID\n\\(X\\)\n\\(Y\\)\n\n\n\n\nWe can consider this structure more generally as noted below.\n\n\n\nStructure of a data set from R for Data Science by Wickham & Grolemund (2022)"
  },
  {
    "objectID": "week3.html#population-and-sample",
    "href": "week3.html#population-and-sample",
    "title": "DATA 202 - Week 3",
    "section": "Population and sample",
    "text": "Population and sample\n\n\n\n\n\n\nDEFINITIONS: Population and sample\n\n\n\nA population is representative of every member of a group of interest or collection of objects.\nA sample is a sub-collection of members or objects from a selected population.\n\n\n\n\n\nPopulation and sample. Image from SimplyPsychology.org\n\n\n\n\nThe difference between big \\(N\\) (a population) and little \\(n\\) (a sample)\n\n\n\n\n\n\nNote\n\n\n\n\nWhen referring to the size of a population, use a capital \\(N\\).\nWhen referring to the size of a sample, use a lowercase \\(n\\).\n\n\n\n\n\n\nPopulation parameter vs. Sample statistic\n\n\n\n\n\n\nDEFINITIONS: Parameter versus statistic\n\n\n\nA parameter is a numerical measurement describing some characteristic of a population.\n\nThink of a “population parameter” to remember this relationship.\n\nA statistic is a numerical measure describing some characteristic of a sample.\n\nThink of a “sample statistic” to remember this relationship.\n\n\n\n\nStatistics vs. A statistic\n\n\n\n\n\n\nNote – Statistics (plural) versus a statistic (singular)\n\n\n\nThere is a difference between the term statistics (plural) and a statistic (singular).\n\nWe previously defined statistics (with an ‘s’ at the end) as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\nA statistic (no ‘s’ at the end) refers to a measurement or value from a test on a sample.\n\n\n\n\n\n\nCensus\n\n\n\n\n\n\nDEFINITION: Census\n\n\n\nA census is the collection of data from every member of a population.\n\n\n\n\n\nPopulation census. Image from CasaNC.org\n\n\n\nThere are many sites with free and publicly available data that can help us make better sense of how others have collected data on populations and samples.\n– Sampling of websites with freely accessible data (no downloads required)\n\nColored Conventions Project\nThe DataHub\nData.gov\nKaggle\n\nIn our course, we will focus on integrating how we structure and analyze data by using various mathematical concepts to frame the process of conducting a statistical study."
  },
  {
    "objectID": "week3.html#what-is-a-variable",
    "href": "week3.html#what-is-a-variable",
    "title": "DATA 202 - Week 3",
    "section": "What is a variable?",
    "text": "What is a variable?\n\n\n\n\n\n\nDEFINITIONS: Variable\n\n\n\nA variable is any characteristic or quantity that can be measured or counted.\n\n\n\nThere are many types of data and variables used in statistics.\n\nThe type of variable helps determine the appropriate methods for analysis.\n– Categorical or Quantitative\nThe level of measurement helps determine how we measure variables.\n– Nominal, Ordinal, Interval, Ratio\n\n\n\n\nVariable types\nThere are two main variable types: categorical and quantitative variables.\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Categorical)\n  A[Variable type] --&gt; C(Quantitative)\n\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Variable types\n\n\n\nA categorical variable consists of qualitative values such as names or labels.\nA quantitative variable consists of numbers representing counts or measurements.\n\n\n\nData that are categorical in nature are non-numeric.\n– Categorical values are labels used to represent categories or data values.\nData that are quantitative in nature are numeric.\n– Quantitative values make use of the sets of numbers to represent counts or measures.\n\n\nWe can further distinguish between these categories using the following definitions:\n\n\n\n\n\n\nDEFINITIONS: Nominal, Ordinal, Discrete, Continuous\n\n\n\nNominal data are categorical data that cannot be arranged in some order (such as low to high). Some examples are eye color and pet names.\nOrdinal data can be arranged in some order but differences between the values are meaningless. Consider letter grades: A is higher than B but A minus B does not make sense.\nDiscrete data are data where the number of values is finite or “countable.” Some examples include number of students or days spent in the library.\nContinuous data can take on infinitely many values where the collection of values is not countable. Some examples include height or weight.\n\n\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Qualitative)\n  A[Variable type] --&gt; C(Quantitative)\n  C --&gt; D(Discrete)\n  C --&gt; E(Continuous)\n  B --&gt; F(Nominal)\n  B --&gt; G(Ordinal)\n\n\n\n\n\n\n\n\nLevels of measurement\nLevels of measurement are used to describe variable types.\n\n\n\n\n\n\n\n\nLevel of measurement\nBrief description\nExamples\n\n\n\n\nNominal\nData cannot be arranged in some order. Only categories are used.\nEye color, city\n\n\nOrdinal\nData can be arranged in some order but differences cannot be found or are meaningless.\nRankings, likert scale\n\n\nInterval\nThere is not a natural zero starting point and rations are meaningless.\nTemperatures, years\n\n\nRatio\nThere is a natural zero starting point and ratios are meaningful.\nHeights, distances\n\n\n\n\n\n\n\nThe four levels of measurement. Image from Scribbr"
  },
  {
    "objectID": "week3.html#task-0-understanding-the-rstudio-ide",
    "href": "week3.html#task-0-understanding-the-rstudio-ide",
    "title": "DATA 202 - Week 3",
    "section": "Task 0: Understanding the RStudio IDE",
    "text": "Task 0: Understanding the RStudio IDE\n\n\n\nRStudio IDE"
  },
  {
    "objectID": "week3.html#task-1-create-an-r-script",
    "href": "week3.html#task-1-create-an-r-script",
    "title": "DATA 202 - Week 3",
    "section": "Task 1: Create an R Script",
    "text": "Task 1: Create an R Script\nWe will conduct most of our work using what is called an RScript.\nThe RScript allows us to save and annotate our code for future use.\nWhen you run code from an RScript, it will show up in the Console (bottom left pane in RStudio).\nIn the RStudio IDE, open an RScript by using the following navigation:\n\nFile &gt; New File &gt; R Script\n\n\n\nUsing the RScript"
  },
  {
    "objectID": "week3.html#rstudio-ide-with-an-rscript",
    "href": "week3.html#rstudio-ide-with-an-rscript",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Inserting code into an RScript\n\ny &lt;- 2 + 2\n\n\n# You can use the `#` symbol to leave notes above your code.\ny &lt;- 2 + 2 # You can use the `#` symbol to leave notes in-line with your code.\n\nWe just defined an object y. We can see its value by running this syntax and typing y into the Console.\n\ny\n\n[1] 4\n\n\nThe output on your screen should match the last line above - with the hashtags.\n\n[ 1 ] indicates a single line of results.\nThe output tells us that y has a value of 4, so we say that y is numeric or that it has a numeric value.\n\nYou can run code by clicking ‘Run’ at the top of the Source window, or by typing CMD+Enter"
  },
  {
    "objectID": "week3.html#task-2-explore-different-object-types",
    "href": "week3.html#task-2-explore-different-object-types",
    "title": "DATA 202 - Week 3",
    "section": "Task 2: Explore different object types",
    "text": "Task 2: Explore different object types\nFor this task, we will explore three object types: numeric, character, and logic values.\n\nTask 2-a: Compute a mathematical statement and create a numeric variable\n\n1 + 2\n\n[1] 3\n\n\nWe can assign a variable to this statement by using an assignment operator: &lt;-\n\na &lt;- 1 + 2\n\nWe can also use an equal sign to assign values: \\(=\\)\n\na = 1 + 2\n\nType “a” to show the value of the variable\n\na\n\n[1] 3\n\n\n\nCreate a numeric variable “b” that is the product of “a” and “y”\n\nb = a*y\n\nType “b” in your console to show the product of the two variables\n\nb\n\n[1] 12\n\n\nDivide b by 4\n\nb / 4\n\n[1] 3\n\n\nTake the square root of b\n\nsqrt(b)\n\n[1] 3.464102\n\n\nCompute the natural log of b\n\nlog(b)\n\n[1] 2.484907\n\n\nCompute the common log of b\n\nlog10(b)\n\n[1] 1.079181\n\n\nFind 1 minus the square root of b\n\n1-sqrt(b)\n\n[1] -2.464102\n\n\n\nAttempt to find the square root of “1 minus the square root of b” - which is a negative value\n\nsqrt(1-b)\n\nWarning in sqrt(1 - b): NaNs produced\n\n\n[1] NaN\n\n\nNaN stands for “Not a number”. This occurs because there is currently no defined value to recognize the square root of negative numbers in R. But we can compute the square root on the absolute value of this difference, if needed.\n\nsqrt(abs(1-b))\n\n[1] 3.316625\n\n\n\nWe can insert longer or more complex mathematical statements too. For example, we can find the absolute value of the sum of -1 and the square root of b cubed and then subtract from that the value of 3 times the square root of b.\nNotice the use of parentheses.\n\nabs(-1+sqrt(b^3)) - 3*(sqrt(b))\n\n[1] 30.17691\n\n\nWe can override the original value of y to match the mathematical statement we generated above.\n\ny &lt;- abs(-1+sqrt(b^3)) - 3*(sqrt(b))\ny\n\n[1] 30.17691\n\n\nWe consider all of the previous objects to be numeric.\n\n\n\nTask 2-b: Create a non-numeric value\nWe can also create objects to hold non-numeric values.\nThere are two types of non-numeric values: character values and logic values.\n\nCharacter values\n\ncharacter &lt;- 'some label'\ncharacter\n\n[1] \"some label\"\n\n\nWe can create a character value using the ‘,’ or “,” quotes.\n\ncharacter &lt;- \"some label\"\ncharacter\n\n[1] \"some label\"\n\n\n\n\n\nLogic values\nLogic values can either be TRUE or FALSE\n\nlogic_true &lt;- TRUE\nlogic_false &lt;- FALSE\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE\n\n\nWe can also use T for TRUE and F for FALSE.\n\nlogic_true &lt;- T\nlogic_false &lt;- F\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE"
  },
  {
    "objectID": "week3.html#task-3-creating-vectors",
    "href": "week3.html#task-3-creating-vectors",
    "title": "DATA 202 - Week 3",
    "section": "Task 3: Creating vectors",
    "text": "Task 3: Creating vectors\nWhen we want to list multiple objects or values, we use R’s available data types, such as vectors and factors.\n\nWe concatenate values in these data types using the operator c( ) to place our values in the order we desire.\nConcatenate means to place things together one after the other.\n\n\nVectors\nVectors are a data type we use to order values (i.e., numeric, character, logic) or mix different values.\nFor example, we can store all of the numbers from 1 to 9 in a vector by using a colon.\n\nmy.vector &lt;- c(1:9)\nmy.vector\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\n\nWhen creating vectors, we use the assignment operator and the concatenate option to generate our object.\n\nvec1 &lt;- c(\"WEB Du Bois\", 1868, \"17th\", \"MA\", \"civil rights activist\")\nvec1\n\n[1] \"WEB Du Bois\"           \"1868\"                  \"17th\"                 \n[4] \"MA\"                    \"civil rights activist\"\n\n\nNotice that my numeric and logic values do not use quotations, but a character value uses quotations ““.\n\n\n\nFactors\nFactors are a data type we use to store categorical variables for anlayses and data plots. We will explore these later."
  },
  {
    "objectID": "week3.html#task-4-creating-data-frames",
    "href": "week3.html#task-4-creating-data-frames",
    "title": "DATA 202 - Week 3",
    "section": "Task 4: Creating data frames",
    "text": "Task 4: Creating data frames\nFor this final task, we will create a list. Lists can contain anything: functions, vectors, other lists, and data frames.\nTo start, let’s create a series of vectors.\n\nvec1 &lt;- c(\"Ida B. Wells\", \"W.E.B Du Bois\",\"Mary G. Ross\", \"Jaime Escalante\",\"Etta Z. Falconer\", \"Bob Moses\", \"Ruth Gonzales\")\nvec2 &lt;- c(1862, 1868, 1908, 1930, 1933, 1935, 1970)\nvec3 &lt;- c(\"MS\", \"MA\", \"OK\", \"Bolivia\", \"MS\", \"NYC\", \"NJ\")\nvec4 &lt;- c(TRUE, TRUE, TRUE, FALSE, T, T, T)\nvec5 &lt;- c(\"African American\", \"Ghanaian American\", \"Native American\", \"Bolivian American\", \"African American\", \"African American\", \"Mexican American\")\nvec6 &lt;- c(\"Journalist\", \"Sociologist\", \"Engineer\", \"Educator\", \"Mathematician\", \"Educator\", \"Engineer\")\n\n\nOur objects vec1, vec3, and vec5 are made of character values\n\nvec1\n\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\nvec3\n\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\nvec5\n\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\nvec6\n\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nOur object vec2 is made of numeric values\n\nvec2\n\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n\nOur object vec4 is a made of logic values\n\nvec4\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\nLists\nLists allow us to do what the name implies - create a list of items.\n\nmy.list &lt;- list(vec1, vec2, vec3, vec4, vec5, vec6)\nmy.list\n\n[[1]]\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\n[[2]]\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n[[3]]\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\n[[4]]\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n[[5]]\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\n[[6]]\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nIn this form, lists can be hard to read.\n\n\n\nData frames\nTo solve the issue with the list, we will use our vectors to generate a specific type of list known as a data.frame.\nData frames are a subtype of lists made of vectors of equal lenght.\n\ndata.frame(vec1, vec2, vec3, vec4, vec5, vec6)\n\n              vec1 vec2    vec3  vec4              vec5          vec6\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice the difference in how the information is arranged when data.frame.\n\nLet’s label this data frame and put labels at the top of the list.\n\ndf &lt;- data.frame(a=vec1, b=vec2, c=vec3, d=vec4, e=vec5, f=vec6)\ndf\n\n                 a    b       c     d                 e             f\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice that when I add labels using the equal sign operator, the vec labels dissapear and are replaced by the categorical labels that we insert. Let’s create more appropriate labels for the data we have generated.\n\ndf &lt;- data.frame(name=vec1, \n                 birthyear=vec2, \n                 birthplace=vec3, \n                 USborn=vec4, \n                 nationality=vec5, \n                 occupation=vec6)\n\n\nFor future use, we can view our data frame by just typing its name into our console or typing View(df).\n\ndf\n\n              name birthyear birthplace USborn       nationality    occupation\n1     Ida B. Wells      1862         MS   TRUE  African American    Journalist\n2    W.E.B Du Bois      1868         MA   TRUE Ghanaian American   Sociologist\n3     Mary G. Ross      1908         OK   TRUE   Native American      Engineer\n4  Jaime Escalante      1930    Bolivia  FALSE Bolivian American      Educator\n5 Etta Z. Falconer      1933         MS   TRUE  African American Mathematician\n6        Bob Moses      1935        NYC   TRUE  African American      Educator\n7    Ruth Gonzales      1970         NJ   TRUE  Mexican American      Engineer\n\n\n\n\n\nNext up: Week 4\nWe will explore databases, details of statistical procedures, and begin to explore univariate analyses."
  },
  {
    "objectID": "week6-slides.html#part-i-context",
    "href": "week6-slides.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "Part I: Context",
    "text": "Part I: Context\nThis week’s context will focus on the social politics around maps.\n\nThe world’s continents. Image from https://www.visualcapitalist.com/map-true-size-of-africa/"
  },
  {
    "objectID": "week6-slides.html#part-ii-content",
    "href": "week6-slides.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nExploratory data analysis using the dplyr package."
  },
  {
    "objectID": "week6-slides.html#part-iii-code",
    "href": "week6-slides.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will focus on Lab 1.\nLab 1 focuses on supporting you with data exploration and univariate statistics.\nNext up: Week 7\n\n\n\nCourse Data GitHub"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "This week’s context will focus on the social politics around maps.\n\n\n\nThe world’s continents. Image from https://www.visualcapitalist.com/map-true-size-of-africa/\n\n\n\n\n\n\nWhat is a map? This National Geographic education resource presents a clear overview of maps, geography, and Geographic Information Systems (GIS).\n\n\n\n\n\n\nWhat’s the real size of Africa? is a CNN Africa Marketplace article that examines the Western foundations of maps and representations of the African continent.\n\n\n\n\n\n\nVaughan (2018) is an open access publication on the spatial dimensions of social cartography. The text contains valuable information about how maps have been used to understand health and human development issues, such as poverty, disease, housing, and the like. The text also contains notes on race and nationality, crime and disorder, and a host of references for further reading.\n\n\n\n\n\n\nManson & Matson (2017) present an overview of society and mapping with new technological tools. While doing so, the authors provide a history of maps and examine the basic social elements of maps, the technical elements of maps, and how maps have been integrated into liberal arts education.\n\n\n\n\n\n\nCrampton (2015) writes on Maps and the Social Construction of Race in a larger volume on maps produced by the University of Chicago Press.\n\n\n\n\n\n\nAlderman & Inwood (2021) describe how Black cartographers use maps to examine issues of racial inequality. The authors provide a more focused discussion on the social politics of maps, as opposed to a more general overview of their functions.\n\n\n\n\n\n\nCan maps be racist? Palmer (2014) provides some context to understand the technical aspects of maps as they relate to our social construction of the global world. In this review, the author situates the common functions of maps onto the social dimensions while attending to the particular periods of the development and construction of global maps; thus integrating the political dimension of knowledge creation via map making.\n– Britton (2021) in a blog post on the “non-racism of maps” offers a very different perspective on the Mercator projection. The author focuses on ideology and science in modern society. He argues that the original purpose of maps does not make them racist.\n\n\n\n\n\n\nHow maps distort our perception of the world is a short and focused resource written by Lee (2023) on the Anti-Racism Daily site. The author focuses on the social politics of perception."
  },
  {
    "objectID": "week6.html#part-i-context",
    "href": "week6.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "This week’s context will focus on the social politics around maps.\n\n\n\nThe world’s continents. Image from https://www.visualcapitalist.com/map-true-size-of-africa/\n\n\n\n\n\n\nWhat is a map? This National Geographic education resource presents a clear overview of maps, geography, and Geographic Information Systems (GIS).\n\n\n\n\n\n\nWhat’s the real size of Africa? is a CNN Africa Marketplace article that examines the Western foundations of maps and representations of the African continent.\n\n\n\n\n\n\nVaughan (2018) is an open access publication on the spatial dimensions of social cartography. The text contains valuable information about how maps have been used to understand health and human development issues, such as poverty, disease, housing, and the like. The text also contains notes on race and nationality, crime and disorder, and a host of references for further reading.\n\n\n\n\n\n\nManson & Matson (2017) present an overview of society and mapping with new technological tools. While doing so, the authors provide a history of maps and examine the basic social elements of maps, the technical elements of maps, and how maps have been integrated into liberal arts education.\n\n\n\n\n\n\nCrampton (2015) writes on Maps and the Social Construction of Race in a larger volume on maps produced by the University of Chicago Press.\n\n\n\n\n\n\nAlderman & Inwood (2021) describe how Black cartographers use maps to examine issues of racial inequality. The authors provide a more focused discussion on the social politics of maps, as opposed to a more general overview of their functions.\n\n\n\n\n\n\nCan maps be racist? Palmer (2014) provides some context to understand the technical aspects of maps as they relate to our social construction of the global world. In this review, the author situates the common functions of maps onto the social dimensions while attending to the particular periods of the development and construction of global maps; thus integrating the political dimension of knowledge creation via map making.\n– Britton (2021) in a blog post on the “non-racism of maps” offers a very different perspective on the Mercator projection. The author focuses on ideology and science in modern society. He argues that the original purpose of maps does not make them racist.\n\n\n\n\n\n\nHow maps distort our perception of the world is a short and focused resource written by Lee (2023) on the Anti-Racism Daily site. The author focuses on the social politics of perception."
  },
  {
    "objectID": "week6.html#part-ii-content",
    "href": "week6.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nExploratory data analysis using the dplyr package.\n\n\nMeasures of center\n\nmean() returns the mean of a single numeric variable\nmedian() returns the middle value of a single numeric variable\nmode() returns the variable type for the mode of a single variable\ntable() returns a frequency table with counts of each level for a single variable\n\n\n\n\nMeasures of variation\n\nrange() returns the min() and max() values of a single numeric variable\nvar() returns the variance of a variable.\nsd() returns the standard deviation for a single numeric variable\n\n\n\n\nMeasures of relative standing\n\nIQR() returns the interquartile range values for a single numeric variable\nz-scores provide us a way to find the number of standard deviations for a specific variable. We compute these manually.\n\n\n\n\nOther measures\n\nmax() returns the maximum value of a single numeric variable\nmin() returns the minimum value of a single numeric variable\n\n\n\n\nBasic plots\n\nboxplot() returns a boxplot of a numeric variable or variables\nhist() returns a histogram of a single numeric variable\nstem() provides a stem-and-leaf plot when a single numeric variable is input.\nplot() provides a scatter plot of data values by its index, \\(i\\).\nplot(density()) provides a density plot of a single numeric variable"
  },
  {
    "objectID": "week6.html#part-iii-code",
    "href": "week6.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\nOur coding tasks this week will focus on Lab 1.\nLab 1 focuses on supporting you with data exploration and univariate statistics.\n\nNext up: Week 7"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "This page contains solutions to selected homework problems.\n\nHomework 1 Solutions\nExercise 1.1. Observational.\nExercise 1.3. From our lecture notes, we defined statistics as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\nExercise 1.5. Answers will vary.\nExercise 1.7. Given the title and contents of the data set, it includes population data for the world’s countries for the given years.\nExercise 1.9.\n\nCountry 1: 2.06139E8 = 206139000\nCountry 2: 8.9561E7 = 89561000\nCountry 3: 2.77E7 = 27700000\nCountry 4: 2.72815E5 = 272815\nCountry 5: 6.077E3 = 6077\n\n\n\nHomework 0 Solutions\nExercise 0.1. If an individual’s projected income is to be converted to a z-score, which of these z-scores would a “rational agent” prefer: -2.00, -1.00, 0, 1.00, or 2.00? What is a “rational agent”? Explain why the “rational agent” would prefer the z-score you selected? Cite any sources. Solution not provided.\nExercise 0.2. What is an integer? What is the sum of the first 100 positive integers? Explain your solution. Cite any sources. Bonus points (but definitely not required for our course): write a proof of the solution. Solution outlined in the bullets below.\n\nAn integer value \\(x\\) is any number in the set of numbers known as the integers. This set includes: zero, \\(0\\); the natural numbers, \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\); and the additive inverses (opposites) of the values in \\(\\mathbb{N}\\), or the set of values \\(\\{..., -3, -2, -1 \\}\\).\nTogether these three sets make up the integers, \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ... \\}\\).\nThe sum of the first 100 positive integers is equivalent to asking for the sum of the first 100 natural numbers.\nA short explanation of the solution is provided below:\n\nAssume that we have a list of numbers from 1 to 100. Let’s add them up and call the sum \\(S\\):\n\\[ S = 1 + 2 + 3 + ... + 99 + 100 \\]\nNow, assume we reverse the order of the sum, \\(S\\), to get the following:\n\\[ S = 100 + 99 + 98 + ... + 2 + 1 \\]\nIf we add these two sums up, we should get something similar to the line below:\n\\[ 2S = (1 + 100) + (2 + 99) + (3 + 98) + ... + (99 + 2) + (100 + 1) \\]\nIt is important that you see what was done in the line above; try writing it out yourself!\n\\[ 2S = 101 + 101 + 101 + ... + 101 + 101 \\]\nSolving for S in the above equation, we get the following:\n\\[ S = \\dfrac{100(101)}{2} = 5050\\]\nThe sum, \\(S\\), is equal to 5050, which is the solution; try generalizing the sum for any case.\n\n\nExercise 0.3. Have you ever heard of p-hacking? P-hacking is where a researcher conducts multiple tests and only reports the significant results from those tests. What are some main issues here? Answers will vary."
  },
  {
    "objectID": "week12-slides.html#part-i-context",
    "href": "week12-slides.html#part-i-context",
    "title": "DATA 202 - Week 12",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "week12-slides.html#part-ii-content",
    "href": "week12-slides.html#part-ii-content",
    "title": "DATA 202 - Week 12",
    "section": "Part II: Content",
    "text": "Part II: Content\nLet us begin with a set of three variables: \\(x\\), \\(y\\), and \\(z\\).\nWe will assume that there is a hypothesized association between all three variables."
  },
  {
    "objectID": "week12-slides.html#part-iii-code",
    "href": "week12-slides.html#part-iii-code",
    "title": "DATA 202 - Week 12",
    "section": "Part III: Code",
    "text": "Part III: Code"
  },
  {
    "objectID": "week12.html#part-ii-content",
    "href": "week12.html#part-ii-content",
    "title": "DATA 202 - Week 12",
    "section": "Part II: Content",
    "text": "Part II: Content\nLet us begin with a set of three variables: \\(x\\), \\(y\\), and \\(z\\).\nWe will assume that there is a hypothesized association between all three variables.\n\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(x, y)\n\n\n\ncor(x, y)\n\n[1] -0.09107752\n\n\nTake note of the value of the correlation coefficient.\n\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(x, z)\n\n\n\ncor(x, z)\n\n[1] 0.9284953\n\n\nTake note of the value of the correlation coefficient.\n\nLet us take a look at the scatterplots between each pair of variables.\n\nplot(y, z)\n\n\n\ncor(y, z)\n\n[1] -0.04660941\n\n\nTake note of the value of the correlation coefficient.\n\n\nRegression assumptions\nIn our previous lectures, we have discussed the function of assumptions and principles in regression analysis. In progressing towards different types of tests, it is important to consider the specific assumptions for any given test.\nAnalyzing relationships among social science variables has an assumptions of linearity. However, this assumptions is not always correct. The adoption of this assumption is based on a host of factors. Most notably, that many relationships have been found to be linear when considered in the empirical sense.\nSome additional assumptions are as follows:\n\nThe sample is representative of the population\nThe variables of interest are normally distributed\nThere are no outliers in the data\nIndependence\nThere is a linear relationship between the independent variable(s) and dependent variable(s)"
  },
  {
    "objectID": "week12.html#part-iii-code",
    "href": "week12.html#part-iii-code",
    "title": "DATA 202 - Week 12",
    "section": "Part III: Code",
    "text": "Part III: Code\n\nWe will run through a sample analysis using the gssr package.\n\n# install packages\ninstall.packages(\"remotes\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"tidyr\", repos = \"http://cran.us.r-project.org\")\n\n# load gssr package\nremotes::install_github(\"kjhealy/gssr\")\n\n# load libraries\nlibrary(gssr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\nExplore variables in the GSSR documentation file\n\n# load the master documentation files\ndata(gss_all) # note that this is a large file of all GSS data\ndata(gss_doc) # this is the documentation for the GSS data\n\n\n\n\nUse the GSSR dictionary documentation\n\n# use the dictionary to get information in a different format\ndata(gss_dict)\ngss_dict\n\n# A tibble: 2,469 × 6\n     pos variable label                           col_type value_labels years   \n   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                           &lt;chr&gt;    &lt;chr&gt;        &lt;list&gt;  \n 1     1 wrkstat  labor force status              dbl+lbl  [1] working… &lt;tibble&gt;\n 2     2 hrs1     number of hours worked last we… dbl+lbl  [89] 80+ ho… &lt;tibble&gt;\n 3     3 hrs2     number of hours usually work a… dbl+lbl  [89] 80+ ho… &lt;tibble&gt;\n 4     4 evwork   ever work as long as one year   dbl+lbl  [1] yes; [2… &lt;tibble&gt;\n 5     5 wrkslf   r self-emp or works for somebo… dbl+lbl  [1] self-em… &lt;tibble&gt;\n 6     6 wrkgovt  govt or private employee        dbl+lbl  [1] governm… &lt;tibble&gt;\n 7     7 indus80  r's industry code (1980)        dbl+lbl  [1] strongl… &lt;tibble&gt;\n 8     8 occ10    r's census occupation code (20… dbl+lbl  [10] chief … &lt;tibble&gt;\n 9     9 indus10  r's industry code (naics 2007)  dbl+lbl  [170] crop … &lt;tibble&gt;\n10    10 marital  marital status                  dbl+lbl  [1] married… &lt;tibble&gt;\n# ℹ 2,459 more rows\n\ngss_dict %&gt;% \n  filter(variable == \"race\")\n\n# A tibble: 1 × 6\n    pos variable label              col_type value_labels               years   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;                      &lt;list&gt;  \n1    71 race     race of respondent dbl+lbl  [1] white; [2] black; [3]… &lt;tibble&gt;\n\ngss_dict %&gt;% \n  filter(variable == \"sex\")\n\n# A tibble: 1 × 6\n    pos variable label           col_type value_labels         years            \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;chr&gt;                &lt;list&gt;           \n1    70 sex      respondents sex dbl+lbl  [1] male; [2] female &lt;tibble [33 × 2]&gt;\n\ngss_dict %&gt;% \n  filter(variable == \"hrs2\")\n\n# A tibble: 1 × 6\n    pos variable label                            col_type value_labels years   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                            &lt;chr&gt;    &lt;chr&gt;        &lt;list&gt;  \n1     3 hrs2     number of hours usually work a … dbl+lbl  [89] 80+ ho… &lt;tibble&gt;\n\ngss_dict %&gt;% \n  filter(variable == \"educ\")\n\n# A tibble: 1 × 6\n    pos variable label                            col_type value_labels years   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                            &lt;chr&gt;    &lt;chr&gt;        &lt;list&gt;  \n1    39 educ     highest year of school completed dbl+lbl  [0] no form… &lt;tibble&gt;\n\n\n\n\n\nView information on a specific variable\n\n# view information on a specific variable\ngss_doc %&gt;% filter(id == \"race\") %&gt;% \n  select(id, description, text)\n\n# A tibble: 1 × 3\n  id    description        text                                   \n  &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;                                  \n1 race  Race of respondent 24. What race do you consider yourself?\n\ngss_doc %&gt;% filter(id == \"sex\") %&gt;% \n  select(id, description, text)\n\n# A tibble: 1 × 3\n  id    description     text                     \n  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    \n1 sex   Respondents sex 23. Code respondent's sex\n\ngss_doc %&gt;% filter(id == \"hrs2\") %&gt;% \n  select(id, description, text)\n\n# A tibble: 1 × 3\n  id    description                         text                                \n  &lt;chr&gt; &lt;chr&gt;                               &lt;chr&gt;                               \n1 hrs2  Number of hours usually work a week 1b. If with a job, but not at work:…\n\ngss_doc %&gt;% filter(id == \"educ\") %&gt;% \n  select(id, description, text)\n\n# A tibble: 1 × 3\n  id    description                      text                                   \n  &lt;chr&gt; &lt;chr&gt;                            &lt;chr&gt;                                  \n1 educ  Highest year of school completed 15. What is the highest grade in eleme…\n\n\n\n\n\nExamine the years available for any given variable\n\n# check which years your variables are available\ngss_which_years(gss_all, race)\n\n# A tibble: 33 × 2\n    year race \n   &lt;dbl&gt; &lt;lgl&gt;\n 1  1972 TRUE \n 2  1973 TRUE \n 3  1974 TRUE \n 4  1975 TRUE \n 5  1976 TRUE \n 6  1977 TRUE \n 7  1978 TRUE \n 8  1980 TRUE \n 9  1982 TRUE \n10  1983 TRUE \n# ℹ 23 more rows\n\ngss_which_years(gss_all, sex)\n\n# A tibble: 33 × 2\n    year sex  \n   &lt;dbl&gt; &lt;lgl&gt;\n 1  1972 TRUE \n 2  1973 TRUE \n 3  1974 TRUE \n 4  1975 TRUE \n 5  1976 TRUE \n 6  1977 TRUE \n 7  1978 TRUE \n 8  1980 TRUE \n 9  1982 TRUE \n10  1983 TRUE \n# ℹ 23 more rows\n\ngss_which_years(gss_all, hrs2)\n\n# A tibble: 33 × 2\n    year hrs2 \n   &lt;dbl&gt; &lt;lgl&gt;\n 1  1972 FALSE\n 2  1973 TRUE \n 3  1974 TRUE \n 4  1975 TRUE \n 5  1976 TRUE \n 6  1977 TRUE \n 7  1978 TRUE \n 8  1980 TRUE \n 9  1982 TRUE \n10  1983 TRUE \n# ℹ 23 more rows\n\ngss_which_years(gss_all, educ)\n\n# A tibble: 33 × 2\n    year educ \n   &lt;dbl&gt; &lt;lgl&gt;\n 1  1972 TRUE \n 2  1973 TRUE \n 3  1974 TRUE \n 4  1975 TRUE \n 5  1976 TRUE \n 6  1977 TRUE \n 7  1978 TRUE \n 8  1980 TRUE \n 9  1982 TRUE \n10  1983 TRUE \n# ℹ 23 more rows\n\n# when you want to return information for multiple variables\ngss_all %&gt;%\n  gss_which_years(c(race, sex, hrs2, educ)) %&gt;%\n  print(n = Inf)\n\n# A tibble: 33 × 5\n    year race  sex   hrs2  educ \n   &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1  1972 TRUE  TRUE  FALSE TRUE \n 2  1973 TRUE  TRUE  TRUE  TRUE \n 3  1974 TRUE  TRUE  TRUE  TRUE \n 4  1975 TRUE  TRUE  TRUE  TRUE \n 5  1976 TRUE  TRUE  TRUE  TRUE \n 6  1977 TRUE  TRUE  TRUE  TRUE \n 7  1978 TRUE  TRUE  TRUE  TRUE \n 8  1980 TRUE  TRUE  TRUE  TRUE \n 9  1982 TRUE  TRUE  TRUE  TRUE \n10  1983 TRUE  TRUE  TRUE  TRUE \n11  1984 TRUE  TRUE  TRUE  TRUE \n12  1985 TRUE  TRUE  TRUE  TRUE \n13  1986 TRUE  TRUE  TRUE  TRUE \n14  1987 TRUE  TRUE  TRUE  TRUE \n15  1988 TRUE  TRUE  TRUE  TRUE \n16  1989 TRUE  TRUE  TRUE  TRUE \n17  1990 TRUE  TRUE  TRUE  TRUE \n18  1991 TRUE  TRUE  TRUE  TRUE \n19  1993 TRUE  TRUE  TRUE  TRUE \n20  1994 TRUE  TRUE  TRUE  TRUE \n21  1996 TRUE  TRUE  TRUE  TRUE \n22  1998 TRUE  TRUE  TRUE  TRUE \n23  2000 TRUE  TRUE  TRUE  TRUE \n24  2002 TRUE  TRUE  TRUE  TRUE \n25  2004 TRUE  TRUE  TRUE  TRUE \n26  2006 TRUE  TRUE  TRUE  TRUE \n27  2008 TRUE  TRUE  TRUE  TRUE \n28  2010 TRUE  TRUE  TRUE  TRUE \n29  2012 TRUE  TRUE  TRUE  TRUE \n30  2014 TRUE  TRUE  TRUE  TRUE \n31  2016 TRUE  TRUE  TRUE  TRUE \n32  2018 TRUE  TRUE  TRUE  TRUE \n33  2021 FALSE TRUE  FALSE TRUE \n\n\n\n\n# get information on the properties of the data\ngss_get_props(varnames = c(\"race\", \"sex\", \"hrs1\", \"educ\"))\n\n# A tibble: 12 × 4\n   variable property           value     id   \n   &lt;chr&gt;    &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;\n 1 sex      Data type          numeric   SEX  \n 2 sex      Missing-data code  0         SEX  \n 3 sex      Record/column      1/297     SEX  \n 4 race     Data type          numeric   RACE \n 5 race     Missing-data code  0         RACE \n 6 race     Record/column      1/298     RACE \n 7 educ     Data type          numeric   EDUC \n 8 educ     Missing-data codes -1,97-99  EDUC \n 9 educ     Record/columns     1/237-238 EDUC \n10 hrs1     Data type          numeric   HRS1 \n11 hrs1     Missing-data codes -1,98,99  HRS1 \n12 hrs1     Record/columns     1/10-11   HRS1 \n\n\n\n\n# loading single year data\ngss12 &lt;- gss_get_yr(2012)\n\nFetching: https://gss.norc.org/documents/stata/2012_stata.zip\n\n\n\n\n# loading panel data by year (not necessary for our course)\ndata(gss_panel08_long) # 2008 three wave panel data (2008, 2010, 2012)\ndata(gss_panel_doc) # loading panel documentation\n\n\n\n# select your variables for a single year using cross sectional file\ngss12 %&gt;% \n  select(race, sex, hrs2, educ, wtssall)\n\n# A tibble: 1,974 × 5\n   race      sex        hrs2        educ      wtssall  \n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;\n 1 1 [white] 1 [male]   NA(i) [iap] 16        2.62     \n 2 1 [white] 1 [male]   NA(i) [iap] 12        3.50     \n 3 3 [other] 1 [male]   NA(i) [iap] 12        1.75     \n 4 1 [white] 2 [female] NA(i) [iap] 13        1.24     \n 5 2 [black] 2 [female] NA(i) [iap] 16        0.874    \n 6 1 [white] 2 [female] NA(i) [iap] 19        0.824    \n 7 1 [white] 2 [female] NA(i) [iap] 15        0.824    \n 8 3 [other] 2 [female] NA(i) [iap] 11        0.412    \n 9 2 [black] 2 [female] NA(i) [iap]  9        0.412    \n10 1 [white] 2 [female] NA(i) [iap] 17        0.412    \n# ℹ 1,964 more rows\n\ngss12 %&gt;% \n  select(race, sex, hrs2, educ, wtssall) %&gt;% \n  count(race, sex, wtssall)\n\n# A tibble: 66 × 4\n   race      sex       wtssall       n\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;int&gt;\n 1 1 [white] 1 [male]  0.412       167\n 2 1 [white] 1 [male]  0.824       280\n 3 1 [white] 1 [male]  0.874        34\n 4 1 [white] 1 [male]  1.00          1\n 5 1 [white] 1 [male]  1.19          1\n 6 1 [white] 1 [male]  1.24         46\n 7 1 [white] 1 [male]  1.65         22\n 8 1 [white] 1 [male]  1.75         95\n 9 1 [white] 1 [male]  2.06          7\n10 1 [white] 1 [male]  2.62         11\n# ℹ 56 more rows"
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "DATA 202 - Week 10",
    "section": "",
    "text": "The midterm examination can be found on our course canvas site here."
  },
  {
    "objectID": "week12.html",
    "href": "week12.html",
    "title": "DATA 202 - Week 12",
    "section": "",
    "text": "This week we focus on the General Social Survey data, or GSS.\n\n\n\nGeneral Social Survey\n\n\nThis data is located in the gssr package in R (see part III). You should become very familiar with the GSS data. Please explore the website as we will be prioritizing the use of the GSS data for the remainder of our course.\nThe use of the GSS data will allow us to consider the meaning of social justice in the context of attitudes and beliefs around social issues."
  },
  {
    "objectID": "week12.html#part-i-context",
    "href": "week12.html#part-i-context",
    "title": "DATA 202 - Week 12",
    "section": "",
    "text": "This week we focus on the General Social Survey data, or GSS.\n\n\n\nGeneral Social Survey\n\n\nThis data is located in the gssr package in R (see part III). You should become very familiar with the GSS data. Please explore the website as we will be prioritizing the use of the GSS data for the remainder of our course.\nThe use of the GSS data will allow us to consider the meaning of social justice in the context of attitudes and beliefs around social issues."
  }
]